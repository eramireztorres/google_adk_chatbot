@skill-creator
My goal is to create a skill to set up an evolutionary optimization experiment using the `openevolve` framework. This framework uses an LLM to iteratively improve a Python code block to maximize a specific score.

Generate the required files (Example `config.yaml`, `evaluator.py`, `initial_program.py`) to solve the Problem Description given by user

The agent with the skill should study

1-
/home/erick.ramirez/repo/google_adk_chatbot/rag/docs/openevolve/README.md

to understand the framework

2-
/home/erick.ramirez/repo/google_adk_chatbot/rag/docs/openevolve/openevolve_config_hyperparameters_cheat_sheet.txt

To understand config parameters

3- Example

/home/erick.ramirez/repo/google_adk_chatbot/rag/docs/openevolve/examples/signal_processing/config.yaml
/home/erick.ramirez/repo/google_adk_chatbot/rag/docs/openevolve/examples/signal_processing/evaluator.py
/home/erick.ramirez/repo/google_adk_chatbot/rag/docs/openevolve/examples/signal_processing/initial_program.py

If the case needs

diff_based_evolution: true

(The evolution LLM only changes selected parts of initial_program)

4- Example

/home/erick.ramirez/repo/google_adk_chatbot/rag/docs/openevolve/examples/llm_prompt_optimization/config.yaml
/home/erick.ramirez/repo/google_adk_chatbot/rag/docs/openevolve/examples/llm_prompt_optimization/evaluator.py
/home/erick.ramirez/repo/google_adk_chatbot/rag/docs/openevolve/examples/llm_prompt_optimization/initial_prompt.txt

If the case needs

diff_based_evolution: false

(The evolution LLM changes the whole text)

Your tasks are:

1- Study above docs
2- Create the new skill in folder

/home/erick.ramirez/repo/google_adk_chatbot/.agent/skills


___


@openevolve-experiment

My goal si to create an openevolve experiment that optimizes a RAG pipeline to retrieve info from markdown doc files in

/home/erick.ramirez/repo/google_adk_chatbot/rag/docs/adk_docs

____


The file

/home/erick/repo/google_adk_chatbot/.agent/skills/openevolve-experiment/SKILL.md

contains the skill for antigravity to create openevolve experiments

But it has some problems:

1- The snippet:

## 1. Analyze the Problem
Determine the nature of the optimization task:
*   **Code Optimization** (e.g., speeding up a function, improving an algorithm):
    *   Use `diff_based_evolution: true`.
    *   The LLM will edit parts of the code.
*   **Prompt/Text Optimization** (e.g., improving an LLM prompt):
    *   Use `diff_based_evolution: false`.
    *   The LLM will rewrite the entire text.

Suggests that diff_based_evolution should be true for code optimization and false otherwise, but actually is better to set it true if the LLM is to modify only small parts of the initial text (code or not), and it should be false if the LLM is to rewrite the initial text completely (code or not)

2-

        # EVOLVE-BLOCK-START

        # EVOLVE-BLOCK-END

Block is only needed when diff_based_evolution is false


3- evaluator.py's evaluate function should return combined_score instead of composite_score. Is combined_score what openevolve uses by default to compare iterations. It may return other values in dictionary which are going to be informed to LLM in next iterations. Fix code example accordingly

4- Change

    llm:
      api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
      model: "gemini-2.5-flash"
      temperature: 0.7

to

    llm:
      api_base: "https://api.openai.com/v1"
      model: "gpt-4.1-mini"
      temperature: 0.7

And in general prefer gpt-4.1-mini as default LLM model

5- There is no suggestion on when to use

include_artifacts (that could be useful when we want the LLM to receive errors from previous iterations, and it is not a big problem using more tokens for that)


exploitation_ratio (small if we want diversity)

6- Some other config parameters that could be useful

Your tasks are:

1- Study the above observations
2- Study openevolve docs in

/home/erick/repo/google_adk_chatbot/rag/docs/openevolve/openevolve_config_hyperparameters_cheat_sheet.txt
/home/erick/repo/google_adk_chatbot/rag/docs/openevolve/README.md

3- Improve

/home/erick/repo/google_adk_chatbot/.agent/skills/openevolve-experiment/SKILL.md


____


Add two more improvements:

1- One to pick the adequate for top level:

log_level: Logging verbosity (DEBUG, INFO, WARNING, ERROR, CRITICAL)


2- Add the indication in skills so antigravity, besides

(`config.yaml`, `evaluator.py`, `initial_program.py`)  it creates a python script that tries evaluator in initial program (or text)
and run it to check if it raises no errors


____

@open-experiment

Help me creating a new openevolve experiment in

/home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0

In order to optimize

/home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/initial_program.py

To obtain the best RAG pipeline to retrieve info from docs in

/home/erick/repo/google_adk_chatbot/rag/docs/adk_docs

To maximize Evidently metrics computed from ground truth

/home/erick/repo/google_adk_chatbot/rag/docs/ground_truth/adk_docs_ground_truth_8.json

The metrics used should be something similar to:

from evidently.model_evaluation import (
    RAGEvaluation,
    CorrectnessLLMEval,
    BERTScore,
    SemanticSimilarity,
    ContextRelevance,
    FaithfulnessLLMEval,
)

eval = RAGEvaluation(
    metrics=[
        CorrectnessLLMEval(),
        BERTScore(),
        SemanticSimilarity(),
        ContextRelevance(method="llm"),
        FaithfulnessLLMEval(),
    ]
)

results = eval.evaluate(
    dataset=my_ground_truth_json,
    llm=my_llm_client  # optional if metric needs LLM
)


But you need to adapt correctly the loading from json to evidently docs requirement as in example:

synthetic_data = [

    ["Why do flowers bloom in spring?",
     "Plants require extra care during cold months. You should keep them indoors.",
     "because of the rising temperatures"],

    ["Why do we yawn when we see someone else yawn?",
     "Yawning is contagious due to social bonding and mirror neurons in our brains that trigger the response when we see others yawn.",
     "because it's a glitch in the matrix"],

    ["How far is Saturn from Earth?",
     "The distance between Earth and Saturn varies, but on average, Saturn is about 1.4 billion kilometers (886 million miles) away from Earth.",
     "about 1.4 billion kilometers"],

    ["Where do penguins live?",
     "Penguins primarily live in the Southern Hemisphere, with most species found in Antarctica, as well as on islands and coastlines of South America, Africa, Australia, and New Zealand.",
     "mostly in Antarctica and southern regions"],
]

columns = ["Question", "Context", "Response"]
synthetic_df = pd.DataFrame(synthetic_data, columns=columns)


LLM should try diverse:


Hyperparameters like chunk-size/overlap, top-k
chunking, including ad hoc laws to separate code chunks (learn from example in /home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/run_ingestion.py)
splitters
Retrieval (hybrid instead of similarity)

____

I've prompted
@open-experiment

Help me creating a new openevolve experiment in

/home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0

In order to optimize

/home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/initial_program.py

To obtain the best RAG pipeline to retrieve info from docs in

/home/erick/repo/google_adk_chatbot/rag/docs/adk_docs

To maximize Evidently metrics computed from ground truth

/home/erick/repo/google_adk_chatbot/rag/docs/ground_truth/adk_docs_ground_truth_8.json

The metrics used should be something similar to:

from evidently.model_evaluation import (
    RAGEvaluation,
    CorrectnessLLMEval,
    BERTScore,
    SemanticSimilarity,
    ContextRelevance,
    FaithfulnessLLMEval,
)

eval = RAGEvaluation(
    metrics=[
        CorrectnessLLMEval(),
        BERTScore(),
        SemanticSimilarity(),
        ContextRelevance(method="llm"),
        FaithfulnessLLMEval(),
    ]
)

results = eval.evaluate(
    dataset=my_ground_truth_json,
    llm=my_llm_client  # optional if metric needs LLM
)


But you need to adapt correctly the loading from json to evidently docs requirement as in example:

synthetic_data = [

    ["Why do flowers bloom in spring?",
     "Plants require extra care during cold months. You should keep them indoors.",
     "because of the rising temperatures"],

    ["Why do we yawn when we see someone else yawn?",
     "Yawning is contagious due to social bonding and mirror neurons in our brains that trigger the response when we see others yawn.",
     "because it's a glitch in the matrix"],

    ["How far is Saturn from Earth?",
     "The distance between Earth and Saturn varies, but on average, Saturn is about 1.4 billion kilometers (886 million miles) away from Earth.",
     "about 1.4 billion kilometers"],

    ["Where do penguins live?",
     "Penguins primarily live in the Southern Hemisphere, with most species found in Antarctica, as well as on islands and coastlines of South America, Africa, Australia, and New Zealand.",
     "mostly in Antarctica and southern regions"],
]

columns = ["Question", "Context", "Response"]
synthetic_df = pd.DataFrame(synthetic_data, columns=columns)


LLM should try diverse:


Hyperparameters like chunk-size/overlap, top-k
chunking, including ad hoc laws to separate code chunks (learn from example in /home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/run_ingestion.py)
splitters
Retrieval (hybrid instead of similarity)


And got very bad files, as you can see here:

/home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/config.yaml
/home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/evaluator.py

(config does not have even system message, evaluator does not return a dictionary with combined_score, etc)

Either the skills are not well described (or missing some good examples)

Your tasks are:

1- Learn from examples like

/home/erick/repo/openevolve/examples/circle_packing_with_artifacts/config_phase_1.yaml
/home/erick/repo/openevolve/examples/circle_packing_with_artifacts/config_phase_2.yaml
/home/erick/repo/openevolve/examples/circle_packing_with_artifacts/evaluator.py


/home/erick/repo/openevolve/examples/signal_processing/config.yaml
/home/erick/repo/openevolve/examples/signal_processing/evaluator.py

/home/erick/repo/openevolve/examples/llm_prompt_optimization/config.yaml
/home/erick/repo/openevolve/examples/llm_prompt_optimization/evaluator.py

2- Improve

/home/erick/repo/google_adk_chatbot/.agent/skills/openevolve-experiment/SKILL.md

With examples or templates to get better responses when calling the skill

_____



openevolve-run /home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/initial_program.py \
/home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/evaluator.py \
--config /home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/config.yaml \
--iterations 1

__


I think there are some errors in evidently signature in

/home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/evaluator.py

Your tasks are:

1- Study carefully docs in

/home/erick/repo/google_adk_chatbot/rag/docs/evidently/RAG evals - Evidently AI - Documentation.pdf

2- Fix code to compute correclty metrics in


/home/erick/repo/google_adk_chatbot/rag/openevolve_experiments/0/evaluator.py

____
Try to evaluate initial program with evaluator using python in this environment:

/home/erick/repo/google_adk_chatbot/venv


____

I've copied a more comprehensive evidently Documentation in

/home/erick/repo/google_adk_chatbot/rag/docs/evidently

please, study

/home/erick/repo/google_adk_chatbot/rag/docs/evidently/metrics-preset_text_evals.md

Before continuing the fixing
