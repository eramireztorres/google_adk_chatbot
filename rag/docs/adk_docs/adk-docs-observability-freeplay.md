---
url: https://google.github.io/adk-docs/observability/freeplay/
source: Google ADK Documentation
---

[adk-python](https://github.com/google/adk-python "adk-python")

[adk-go](https://github.com/google/adk-go "adk-go")

[adk-java](https://github.com/google/adk-java "adk-java")

* [Home](../..)

  Home
* Build Agents




  Build Agents
  + [Get Started](../../get-started/)

    Get Started
    - [Python](../../get-started/python/)
    - [Go](../../get-started/go/)
    - [Java](../../get-started/java/)
  + [Build your Agent](../../tutorials/)

    Build your Agent
    - [Multi-tool agent](../../get-started/quickstart/)
    - [Agent team](../../tutorials/agent-team/)
    - [Streaming agent](../../get-started/streaming/)

      Streaming agent
      * [Python](../../get-started/streaming/quickstart-streaming/)
      * [Java](../../get-started/streaming/quickstart-streaming-java/)
    - [Visual Builder](../../visual-builder/)
    - [Advanced setup](../../get-started/installation/)
  + [Agents](../../agents/)

    Agents
    - [LLM agents](../../agents/llm-agents/)
    - [Workflow agents](../../agents/workflow-agents/)

      Workflow agents
      * [Sequential agents](../../agents/workflow-agents/sequential-agents/)
      * [Loop agents](../../agents/workflow-agents/loop-agents/)
      * [Parallel agents](../../agents/workflow-agents/parallel-agents/)
    - [Custom agents](../../agents/custom-agents/)
    - [Multi-agent systems](../../agents/multi-agents/)
    - [Agent Config](../../agents/config/)
    - [Models & Authentication](../../agents/models/)
  + [Tools for Agents](../../tools/)

    Tools for Agents
    - [Built-in tools](../../tools/built-in-tools/)
    - Gemini API tools




      Gemini API tools
      * [Computer use](../../tools/gemini-api/computer-use/)
    - Google Cloud tools




      Google Cloud tools
      * [Overview](../../tools/google-cloud-tools/)
      * [MCP Toolbox for Databases](../../tools/google-cloud/mcp-toolbox-for-databases/)
      * [BigQuery Agent Analytics](../../tools/google-cloud/bigquery-agent-analytics/)
      * [Code Execution with Agent Engine](../../tools/google-cloud/code-exec-agent-engine/)
    - [Third-party tools](../../tools/third-party/)

      Third-party tools
      * [AgentQL](../../tools/third-party/agentql/)
      * [Bright Data](../../tools/third-party/bright-data/)
      * [Browserbase](../../tools/third-party/browserbase/)
      * [Exa](../../tools/third-party/exa/)
      * [Firecrawl](../../tools/third-party/firecrawl/)
      * [GitHub](../../tools/third-party/github/)
      * [Hugging Face](../../tools/third-party/hugging-face/)
      * [Notion](../../tools/third-party/notion/)
      * [Tavily](../../tools/third-party/tavily/)
      * [Agentic UI (AG-UI)](../../tools/third-party/ag-ui/)
  + [Custom Tools](../../tools-custom/)

    Custom Tools
    - Function tools




      Function tools
      * [Overview](../../tools-custom/function-tools/)
      * [Tool performance](../../tools-custom/performance/)
      * [Action confirmations](../../tools-custom/confirmation/)
    - [MCP tools](../../tools-custom/mcp-tools/)
    - [OpenAPI tools](../../tools-custom/openapi-tools/)
    - [Authentication](../../tools-custom/authentication/)
* Run Agents




  Run Agents
  + [Agent Runtime](../../runtime/)

    Agent Runtime
    - [Runtime Config](../../runtime/runconfig/)
    - [API Server](../../runtime/api-server/)
    - [Resume Agents](../../runtime/resume/)
  + [Deployment](../../deploy/)

    Deployment
    - [Agent Engine](../../deploy/agent-engine/)
    - [Cloud Run](../../deploy/cloud-run/)
    - [GKE](../../deploy/gke/)
  + Observability




    Observability
    - [Logging](../logging/)
    - [Cloud Trace](../cloud-trace/)
    - [AgentOps](../agentops/)
    - [Arize AX](../arize-ax/)
    - Freeplay

      [Freeplay](./)



      Table of contents
      * [Getting Started](#getting-started)

        + [Create a Freeplay Account](#create-a-freeplay-account)
        + [Use Freeplay ADK Library](#use-freeplay-adk-library)
      * [Observability](#observability)
      * [Prompt Management (optional)](#prompt-management-optional)

        + [System Message](#system-message)
        + [Agent Context Variable](#agent-context-variable)
        + [History Block](#history-block)
      * [Evaluation](#evaluation)
      * [Dataset Management](#dataset-management)
      * [Batch Testing](#batch-testing)
      * [Sign up now](#sign-up-now)
    - [Monocle](../monocle/)
    - [Phoenix](../phoenix/)
    - [W&B Weave](../weave/)
  + [Evaluation](../../evaluate/)

    Evaluation
    - [Criteria](../../evaluate/criteria/)
    - [User Simulation](../../evaluate/user-sim/)
  + [Safety and Security](../../safety/)

    Safety and Security
* Components




  Components
  + [Technical Overview](../../get-started/about/)
  + [Context](../../context/)

    Context
    - [Context caching](../../context/caching/)
    - [Context compression](../../context/compaction/)
  + [Sessions & Memory](../../sessions/)

    Sessions & Memory
    - [Session](../../sessions/session/)
    - [State](../../sessions/state/)
    - [Memory](../../sessions/memory/)
    - [Vertex AI Express Mode](../../sessions/express-mode/)
  + [Callbacks](../../callbacks/)

    Callbacks
    - [Types of callbacks](../../callbacks/types-of-callbacks/)
    - [Callback patterns](../../callbacks/design-patterns-and-best-practices/)
  + [Artifacts](../../artifacts/)

    Artifacts
  + [Events](../../events/)

    Events
  + [Apps](../../apps/)

    Apps
  + [Plugins](../../plugins/)

    Plugins
    - [Reflect and retry](../../plugins/reflect-and-retry/)
  + [MCP](../../mcp/)

    MCP
  + [A2A Protocol](../../a2a/)

    A2A Protocol
    - [Introduction to A2A](../../a2a/intro/)
    - A2A Quickstart (Exposing)




      A2A Quickstart (Exposing)
      * [Python](../../a2a/quickstart-exposing/)
      * [Go](../../a2a/quickstart-exposing-go/)
    - A2A Quickstart (Consuming)




      A2A Quickstart (Consuming)
      * [Python](../../a2a/quickstart-consuming/)
      * [Go](../../a2a/quickstart-consuming-go/)
  + [Bidi-streaming (live)](../../streaming/)

    Bidi-streaming (live)
    - [Custom Audio Bidi-streaming app sample (WebSockets)](../../streaming/custom-streaming-ws/)
    - [Bidi-streaming development guide series](../../streaming/dev-guide/part1/)
    - [Streaming Tools](../../streaming/streaming-tools/)
    - [Configurating Bidi-streaming behaviour](../../streaming/configuration/)
  + Grounding




    Grounding
    - [Understanding Google Search Grounding](../../grounding/google_search_grounding/)
    - [Understanding Vertex AI Search Grounding](../../grounding/vertex_ai_search_grounding/)
* Reference




  Reference
  + [API Reference](../../api-reference/)

    API Reference
    - [Python ADK](../../api-reference/python/)
    - [Go ADK](https://pkg.go.dev/google.golang.org/adk)
    - [Java ADK](../../api-reference/java/)
    - [CLI Reference](../../api-reference/cli/)
    - [Agent Config reference](../../api-reference/agentconfig/)
    - [REST API](../../api-reference/rest/)
  + [Community Resources](../../community/)
  + [Contributing Guide](../../contributing-guide/)

Table of contents

* [Getting Started](#getting-started)

  + [Create a Freeplay Account](#create-a-freeplay-account)
  + [Use Freeplay ADK Library](#use-freeplay-adk-library)
* [Observability](#observability)
* [Prompt Management (optional)](#prompt-management-optional)

  + [System Message](#system-message)
  + [Agent Context Variable](#agent-context-variable)
  + [History Block](#history-block)
* [Evaluation](#evaluation)
* [Dataset Management](#dataset-management)
* [Batch Testing](#batch-testing)
* [Sign up now](#sign-up-now)

# Agent Observability and Evaluation with Freeplay[¶](#agent-observability-and-evaluation-with-freeplay "Permanent link")

[Freeplay](https://freeplay.ai/) provides an end-to-end workflow for building
and optimizing AI agents, and it can be integrated with ADK. With Freeplay your
whole team can easily collaborate to iterate on agent instructions (prompts),
experiment with and compare different models and agent changes, run evals both
offline and online to measure quality, monitor production, and review data by
hand.

Key benefits of Freeplay:

* **Simple observability** - focused on agents, LLM calls and tool calls for easy human review
* **Online evals/automated scorers** - for error detection in production
* **Offline evals and experiment comparison** - to test changes before deploying
* **Prompt management** - supports pushing changes straight from the Freeplay playground to code
* **Human review workflow** - for collaboration on error analysis and data annotation
* **Powerful UI** - makes it possible for domain experts to collaborate closely with engineers

Freeplay and ADK complement one another. ADK gives you a powerful and expressive
agent orchestration framework while Freeplay plugs in for observability, prompt
management, evaluation and testing. Once you integrate with Freeplay, you can
update prompts and evals from the Freeplay UI or from code, so that anyone on
your team can contribute.

[Click here](https://www.loom.com/share/82f41ffde94949beb941cb191f53c3ec?sid=997aff3c-daa3-40ab-93a9-fdaf87ea2ea1) to see a demo.

## Getting Started[¶](#getting-started "Permanent link")

Below is a guide for getting started with Freeplay and ADK. You can also find a
full sample ADK agent repo
[here](https://github.com/228Labs/freeplay-google-demo).

### Create a Freeplay Account[¶](#create-a-freeplay-account "Permanent link")

Sign up for a free [Freeplay account](https://freeplay.ai/signup).

After creating an account, you can define the following environment variables:

```python
FREEPLAY_PROJECT_ID=
FREEPLAY_API_KEY=
FREEPLAY_API_URL=
```

### Use Freeplay ADK Library[¶](#use-freeplay-adk-library "Permanent link")

Install the Freeplay ADK library:

```python
pip install freeplay-python-adk
```

Freeplay will automatically capture OTel logs from your ADK application when
you initialize observability:

```python
from freeplay_python_adk.client import FreeplayADK
FreeplayADK.initialize_observability()
```

You'll also want to pass in the Freeplay plugin to your App:

```python
from app.agent import root_agent
from freeplay_python_adk.freeplay_observability_plugin import FreeplayObservabilityPlugin
from google.adk.runners import App

app = App(
    name="app",
    root_agent=root_agent,
    plugins=[FreeplayObservabilityPlugin()],
)

__all__ = ["app"]
```

You can now use ADK as you normally would, and you will see logs flowing to
Freeplay in the Observability section.

## Observability[¶](#observability "Permanent link")

Freeplay's Observability feature gives you a clear view into how your agent is
behaving in production. You can dig into to individual agent traces to
understand each step and diagnose issues:

![Trace detail](https://228labs.com/freeplay-google-demo/images/trace_detail.png)

You can also use Freeplay's filtering functionality to search and filter the
data across any segment of interest:

![Filter](https://228labs.com/freeplay-google-demo/images/filter.png)

## Prompt Management (optional)[¶](#prompt-management-optional "Permanent link")

Freeplay offers
[native prompt management](https://docs.freeplay.ai/docs/managing-prompts),
which simplifies the process of version and testing different prompt versions.
It allows you to experiment with changes to ADK agent instructions in the
Freeplay UI, test different models, and push updates straight to your code,
similar to a feature flag.

To leverage Freeplay's prompt management capabilities alongside ADK, you'll want
to use the Freeplay ADK agent wrapper. `FreeplayLLMAgent` extends ADK's base
`LlmAgent` class, so instead of having to hard code your prompts as agent
instructions, you can version prompts in the Freeplay application.

First define a prompt in Freeplay by going to Prompts -> Create prompt template:

![Prompt](https://228labs.com/freeplay-google-demo/images/prompt.png)

When creating your prompt template you'll need to add 3 elements, as described
in the following sections:

### System Message[¶](#system-message "Permanent link")

This corresponds to the "instructions" section in your code.

### Agent Context Variable[¶](#agent-context-variable "Permanent link")

Adding the following to the bottom of your system message will create a variable
for the ongoing agent context to be passed through:

```python
{{agent_context}}
```

### History Block[¶](#history-block "Permanent link")

Click new message and change the role to 'history'. This will ensure the past
messages are passed through when present.

![Prompt Editor](https://228labs.com/freeplay-google-demo/images/prompt_editor.png)

Now in your code you can use the `FreeplayLLMAgent`:

```python
from freeplay_python_adk.client import FreeplayADK
from freeplay_python_adk.freeplay_llm_agent import (
    FreeplayLLMAgent,
)

FreeplayADK.initialize_observability()

root_agent = FreeplayLLMAgent(
    name="social_product_researcher",
    tools=[tavily_search],
)
```

When the `social_product_researcher` is invoked, the prompt will be
retrieved from Freeplay and formatted with the proper input variables.

## Evaluation[¶](#evaluation "Permanent link")

Freeplay enables you to define, version, and run
[evaluations](https://docs.freeplay.ai/docs/evaluations) from the Freeplay web
application. You can define evaluations for any of your prompts or agents by
going to Evaluations -> "New evaluation".

![Creating a new evaluation in Freeplay](https://228labs.com/freeplay-google-demo/images/eval_create.png)

These evaluations can be configured to run for both online monitoring and
offline evaluation. Datasets for offline evaluation can be uploaded to Freeplay
or saved from log examples.

## Dataset Management[¶](#dataset-management "Permanent link")

As you get data flowing into Freeplay, you can use these logs to start building
up [datasets](https://docs.freeplay.ai/docs/datasets) to test against on a
repeated basis. Use production logs to create golden datasets or collections of
failure cases that you can use to test against as you make changes.

![Save test case](https://228labs.com/freeplay-google-demo/images/save_test_case.png)

## Batch Testing[¶](#batch-testing "Permanent link")

As you iterate on your agent, you can run batch tests (i.e., offline
experiments) at both the
[prompt](https://docs.freeplay.ai/docs/component-level-test-runs) and
[end-to-end](https://docs.freeplay.ai/docs/end-to-end-test-runs) agent level.
This allows you to compare multiple different models or prompt changes and
quantify changes head to head across your full agent execution.

[Here](https://github.com/228Labs/freeplay-google-demo/blob/main/examples/example_test_run.py)
is a code example for executing a batch test on Freeplay with ADK.
[Here](https://github.com/228Labs/freeplay-google-demo/blob/main/examples/example_test_run.py) is a code example for executing a batch test on Freeplay with the Google ADK.

## Sign up now[¶](#sign-up-now "Permanent link")

Go to [Freeplay](https://freeplay.ai/) to sign up for an account, and check out a full Freeplay <> ADK Integration [here](https://github.com/228Labs/freeplay-google-demo).

Back to top