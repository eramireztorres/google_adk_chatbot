ChatOllama - Docs by LangChain

1 de 16

Get started using Ollama

https://docs.langchain.com/oss/python/integrations/chat/o...

in LangChain.

allows you to run open-source Large Language Models �LLMs�, such as
, locally.
Ollama bundles model weights, configuration, and data into a single package, defined
by a Modelfile. It optimizes setup and configuration details, including GPU usage.
For a complete list of supported models and model variants, see the
.

For detailed documentation of all features and configuration options, head to the
API reference.

1.7M/month

31/1/26, 13:35

ChatOllama - Docs by LangChain

2 de 16

First, follow

https://docs.langchain.com/oss/python/integrations/chat/o...

to set up and run a local Ollama instance:
and install Ollama onto the available supported platforms (including

Windows Subsystem for Linux aka WSL, macOS, and Linux)
macOS users can install via Homebrew with

and start

with
Fetch available LLM model via
View a list of available models via the
e.g.,
This will download the default tagged version of the model. Typically, the default
points to the latest, smallest sized-parameter model.
On Linux (or WSL),

On Mac, the models will be download to
the models will be stored at
Specify the exact version of the model of interest as such
�View the

model in this instance)

To view all pulled models, use
To chat directly with a model from the command line, use

View the

for more commands. You can run

in the terminal to see available commands.
To enable automated tracing of your model calls, set your

API key:

31/1/26, 13:35

ChatOllama - Docs by LangChain

3 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

31/1/26, 13:35

ChatOllama - Docs by LangChain

4 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

31/1/26, 13:35

ChatOllama - Docs by LangChain

5 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

31/1/26, 13:35

ChatOllama - Docs by LangChain

6 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

31/1/26, 13:35

ChatOllama - Docs by LangChain

7 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

31/1/26, 13:35

ChatOllama - Docs by LangChain

8 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: "

31/1/26, 13:35

ChatOllama - Docs by LangChain

9 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

The LangChain Ollama integration lives in the

package:

pip install -qU langchain-ollama

Make sure youʼre using the latest Ollama version!

Update by running:

pip install -U ollama

Now we can instantiate our model object and generate chat completions:

from langchain_ollama import ChatOllama
llm = ChatOllama(
model="llama3.1",
temperature=0,
# other params...
)

31/1/26, 13:35

ChatOllama - Docs by LangChain

10 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

messages = [
(
"system",

"You are a helpful assistant that translates English to French. Translate the
),
("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg

AIMessage(content='The translation of "I love programming" in French is:\n\n"J\'adore

print(ai_msg.content)

The translation of "I love programming" in French is:
"J'adore le programmation."

uses the OpenAI compatible web server specification, and can be
used with the default

methods as described

Make sure to select an ollama model that supports
We can use

with an LLM

.

.
such as

:

ollama pull gpt-oss:20b

Details on creating custom tools are available in
how to create a tool using the

. Below, we demonstrate

decorator on a normal python function.

31/1/26, 13:35

ChatOllama - Docs by LangChain

11 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

from typing import List
from langchain.messages import AIMessage
from langchain.tools import tool
from langchain_ollama import ChatOllama

@tool
def validate_user(user_id: int, addresses: List[str]) -> bool:
"""Validate user using historical addresses.
Args:
user_id (int): the user ID.
addresses (List[str]): Previous addresses as a list of strings.
"""
return True

llm = ChatOllama(
model="gpt-oss:20b",
validate_model_on_init=True,
temperature=0,
).bind_tools([validate_user])
result = llm.invoke(
"Could you validate user 123? They previously lived at "
"123 Fake St in Boston MA and 234 Pretend Boulevard in "
"Houston TX."
)
if isinstance(result, AIMessage) and result.tool_calls:
print(result.tool_calls)

[{'name': 'validate_user', 'args': {'addresses': ['123 Fake St, Boston, MA', '234 Pre

Ollama has limited support for multi-modal LLMs, such as

31/1/26, 13:35

ChatOllama - Docs by LangChain

12 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

Be sure to update Ollama so that you have the most recent version to support multimodal.

pip install pillow

31/1/26, 13:35

ChatOllama - Docs by LangChain

13 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

import base64
from io import BytesIO
from IPython.display import HTML, display
from PIL import Image

def convert_to_base64(pil_image):
"""
Convert PIL images to Base64 encoded strings
:param pil_image: PIL image
:return: Re-sized Base64 string
"""
buffered = BytesIO()
pil_image.save(buffered, format="JPEG")

# You can change the format if needed

img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
return img_str

def plt_img_base64(img_base64):
"""
Disply base64 encoded string as image
:param img_base64:

Base64 string

"""
# Create an HTML img tag with the base64 string as the source
image_html = f'<img src="data:image/jpeg;base64,{img_base64}" />'
# Display the image by rendering the HTML
display(HTML(image_html))

file_path = "../../../static/img/ollama_example_img.jpg"
pil_image = Image.open(file_path)
image_b64 = convert_to_base64(pil_image)
plt_img_base64(image_b64)

<img src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgK

31/1/26, 13:35

ChatOllama - Docs by LangChain

14 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

from langchain.messages import HumanMessage
from langchain_ollama import ChatOllama
llm = ChatOllama(model="bakllava", temperature=0)

def prompt_func(data):
text = data["text"]
image = data["image"]
image_part = {
"type": "image_url",
"image_url": f"data:image/jpeg;base64,{image}",
}
content_parts = []
text_part = {"type": "text", "text": text}
content_parts.append(image_part)
content_parts.append(text_part)
return [HumanMessage(content=content_parts)]

from langchain_core.output_parsers import StrOutputParser
chain = prompt_func | llm | StrOutputParser()
query_chain = chain.invoke(
{"text": "What is the Dollar-based gross retention rate?", "image": image_b64}
)
print(query_chain)

90%

31/1/26, 13:35

ChatOllama - Docs by LangChain

15 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

Some models, such as IBMʼs

, support custom message roles to enable

thinking processes.
To access Granite 3.2ʼs thinking features, pass a message with a
content set to
can use a

. Because

role with

is a non-standard message role, we

object to implement it:

from langchain.messages import HumanMessage
from langchain_core.messages import ChatMessage
from langchain_ollama import ChatOllama
llm = ChatOllama(model="granite3.2:8b")
messages = [
ChatMessage(role="control", content="thinking"),
HumanMessage("What is 3^3?"),
]
response = llm.invoke(messages)
print(response.content)

Here is my thought process:

The user is asking for the value of 3 raised to the power of 3, which is a basic expo
Here is my response:
3^3 (read as "3 to the power of 3") equals 27.
This calculation is performed by multiplying 3 by itself three times: 3*3*3 = 27.

Note that the model exposes its thought process in addition to its final response.

For detailed documentation of all

features and configurations head to the

31/1/26, 13:35

ChatOllama - Docs by LangChain

16 de 16

https://docs.langchain.com/oss/python/integrations/chat/o...

.

or

.

to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

Yes

Forum

About

Changelog

Careers

LangChain Academy

Blog

No

Trust Center

Powered by

31/1/26, 13:35

