OllamaEmbeddings - Docs by LangChain

1 de 7

https://docs.langchain.com/oss/python/integrations/text_...

This will help you get started with Ollama embedding models using LangChain. For
detailed documentation on

features and configuration options,

please refer to the

First, follow

.

to set up and run a local Ollama instance:
and install Ollama onto the available supported platforms (including

Windows Subsystem for Linux aka WSL, macOS, and Linux)
macOS users can install via Homebrew with

and start

with
Fetch available LLM model via
View a list of available models via the
e.g.,
This will download the default tagged version of the model. Typically, the default
points to the latest, smallest sized-parameter model.
On Mac, the models will be download to

On Linux (or WSL),

the models will be stored at
Specify the exact version of the model of interest as such
�View the

model in this

31/1/26, 13:33

OllamaEmbeddings - Docs by LangChain

2 de 7

https://docs.langchain.com/oss/python/integrations/text_...

instance)
To view all pulled models, use
To chat directly with a model from the command line, use

View the

for more commands. You can run

in the terminal to see available commands.
To enable automated tracing of your model calls, set your

API key:

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: "

The LangChain Ollama integration lives in the

package:

pip install -qU langchain-ollama

Now we can instantiate our model object and generate embeddings:

from langchain_ollama import OllamaEmbeddings
embeddings = OllamaEmbeddings(
model="llama3",
)

Embedding models are often used in retrieval-augmented generation �RAG) flows, both
as part of indexing data as well as later retrieving it. For more detailed instructions,
please see our

.

31/1/26, 13:33

OllamaEmbeddings - Docs by LangChain

3 de 7

https://docs.langchain.com/oss/python/integrations/text_...

31/1/26, 13:33

OllamaEmbeddings - Docs by LangChain

4 de 7

https://docs.langchain.com/oss/python/integrations/text_...

Below, see how to index and retrieve data using the

object we initialized

above. In this example, we will index and retrieve a sample document in the
.

31/1/26, 13:33

OllamaEmbeddings - Docs by LangChain

5 de 7

https://docs.langchain.com/oss/python/integrations/text_...

# Create a vector store with a sample text
from langchain_core.vectorstores import InMemoryVectorStore

text = "LangChain is the framework for building context-aware reasoning applications"
vectorstore = InMemoryVectorStore.from_texts(
[text],
embedding=embeddings,
)
# Use the vectorstore as a retriever
retriever = vectorstore.as_retriever()
# Retrieve the most similar text
retrieved_documents = retriever.invoke("What is LangChain?")
# Show the retrieved document's content
print(retrieved_documents[0].page_content)

LangChain is the framework for building context-aware reasoning applications

Under the hood, the vectorstore and retriever implementations are calling
and
embeddings for the text(s) used in

to create
and retrieval

operations,

respectively.
You can directly call these methods to get embeddings for your own use cases.

You can embed single texts or documents with

:

single_vector = embeddings.embed_query(text)
print(str(single_vector)[:100])

# Show the first 100 characters of the vector

31/1/26, 13:33

OllamaEmbeddings - Docs by LangChain

6 de 7

https://docs.langchain.com/oss/python/integrations/text_...

[-0.0039849705, 0.023019705, -0.001768838, -0.0058736936, 0.00040999008, 0.017861595,

You can embed multiple texts with

:

text2 = (

"LangGraph is a library for building stateful, multi-actor applications with LLMs
)
two_vectors = embeddings.embed_documents([text, text2])
for vector in two_vectors:
print(str(vector)[:100])

# Show the first 100 characters of the vector

[-0.0039849705, 0.023019705, -0.001768838, -0.0058736936, 0.00040999008, 0.017861595,

[-0.0066985516, 0.009878328, 0.008019467, -0.009384944, -0.029560851, 0.025744654, 0.

For detailed documentation on
please refer to the

features and configuration options,
.

or

.

to Claude, VSCode, and more via MCP for real-time answers.

31/1/26, 13:33

OllamaEmbeddings - Docs by LangChain

7 de 7

https://docs.langchain.com/oss/python/integrations/text_...

Was this page helpful?

Yes

Forum

About

Changelog

Careers

LangChain Academy

Blog

No

Trust Center

Powered by

31/1/26, 13:33

