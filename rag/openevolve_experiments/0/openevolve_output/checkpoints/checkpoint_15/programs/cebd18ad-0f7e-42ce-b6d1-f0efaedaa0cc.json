{"id": "cebd18ad-0f7e-42ce-b6d1-f0efaedaa0cc", "code": "import os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 800               # Reduce chunk size for more focused chunks\n        self.chunk_overlap = 150            # Slightly less overlap to reduce redundancy\n        self.top_k = 7                     # Increase retrieval to get more context, improving recall\n        self.temperature = 0.3             # Lower temperature for more precise and faithful answers\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS) with normalization and ef construction for better search quality\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n            # Force normalize embeddings for cosine similarity (default)\n            self.vector_store.index.metric_type = \"cosine\"\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Improved chunking:\n        - Use header-based splitting to respect markdown structure.\n        - Split by code fences and keep atomic if dense.\n        - Smaller chunks for text with overlap.\n        \"\"\"\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        HEADER_RE = re.compile(r\"^(#{1,4})\\s*(.+)$\", re.MULTILINE)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while|var|let|const)\\b\")\n\n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n\n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines:\n                return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n\n        # First, split text by headers to respect document structure\n        header_positions = [(m.start(), m.group(0), len(m.group(1))) for m in HEADER_RE.finditer(text)]\n        header_positions.append((len(text), \"\", 0))  # Sentinel for end\n\n        for i in range(len(header_positions) - 1):\n            start_pos = header_positions[i][0]\n            end_pos = header_positions[i + 1][0]\n            section_text = text[start_pos:end_pos].strip()\n            if not section_text or _is_navigation_chunk(section_text):\n                continue\n\n            # Now within section, split out code fences first\n            cursor_sec = 0\n            for match in CODE_FENCE_RE.finditer(section_text):\n                cs, ce = match.span()\n                # Text before code fence\n                if cs > cursor_sec:\n                    pre_text = section_text[cursor_sec:cs].strip()\n                    if pre_text and not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n                # Code fence block\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        # Keep code block atomic\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        # Treat as text if low density code\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor_sec = ce\n            # Tail text after last code fence in section\n            tail_text = section_text[cursor_sec:].strip()\n            if tail_text and not _is_navigation_chunk(tail_text):\n                self._make_text_chunks(tail_text, source, chunks, self.chunk_size, self.chunk_overlap)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts with sentence boundary awareness\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n\n        sentences = sent_tokenize(text)\n        current_chunk = \"\"\n        for sentence in sentences:\n            if len(current_chunk) + len(sentence) + 1 <= size:\n                current_chunk += (sentence + \" \")\n            else:\n                chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                # Overlap by reusing last sentences from previous chunk to next chunk\n                overlap_sentences = current_chunk.strip().split()[-overlap//5:]  # approx overlap in words\n                current_chunk = \" \".join(overlap_sentences) + \" \" + sentence + \" \"\n        if current_chunk.strip():\n            chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval: use MMR to diversify retrieval, fallback to similarity if MMR unavailable\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=15)\n        except AttributeError:\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation: improved prompt with explicit instructions to enhance faithfulness and relevance\n        prompt = (\n            f\"You are an expert assistant answering questions based on the provided context from Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer the question strictly based on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"Do not make up information. If unsure, say 'I don't know based on the provided context.'\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "af10b932-3d5b-45bf-a513-6d41dd6c21b0", "generation": 1, "timestamp": 1769111667.025056, "iteration_found": 3, "metrics": {"combined_score": 0.5791666666666667, "num_samples": 8, "raw_scores": [0.0, 0.875, 0.8625]}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 4 lines with 4 lines\nChange 2: Replace 3 lines with 3 lines\nChange 3: Replace 5 lines with 7 lines\nChange 4: Replace 53 lines with 66 lines\nChange 5: Replace 12 lines with 24 lines\nChange 6: Replace 19 lines with 24 lines", "parent_metrics": {"combined_score": 0.6875, "num_samples": 8, "raw_scores": [0.75, 0.875, 0.4375]}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer. Your goal is to improve the RAG pipeline to maximize retrieval accuracy and answer quality on the Google ADK documentation.\n\nThe initial program has a basic chunking strategy and retrieval setup. You have full control to evolve:\n\n1. **Chunking Strategy**: Modify `_chunk_document` and `_make_text_chunks`. Experiment with:\n   - Code fence handling (density checks, language detection).\n   - Header-based splitting (markdown structure).\n   - Semantic chunking.\n   - Varying chunk sizes and overlaps.\n\n2. **Retrieval**: Modify `RAGSystem.__init__` and `query`. Experiment with:\n   - `top_k` (k) parameter.\n   - Vector store choice (prefer FAISS for indexing and retrieval).\n   - Retrieval strategy (similarity vs. MMR) and relevance thresholds if you implement them.\n   - Optional FAISS persistence (save/load local index) and relevance score filtering.\n    - Hybrid search (if you can implement keyword search to combine with vector search).\n    - Re-ranking (if feasible with available libraries/APIs, or simple heuristic re-ranking).\n    - Query expansion or rewriting before retrieval.\n   \n3. **Generation**: Modify the prompt in `query`.\n\n**Constraints**:\n- You MUST maintain the `evaluate_rag(docs_path, query)` function signature and return dictionary specific keys (`answer`, `contexts`).\n- You MUST keep the `RAGSystem` class structure (or equivalent) to ensure `evaluate_rag` works.\n- Code must be robust and handle potential parsing errors resiliently.\n\nThe evaluator uses \"Evidently\" to compute a combined score of Correctness, Faithfulness, Relevance, etc. Maximizing this score is your primary objective.\n", "user": "# Current Program Information\n- Fitness: 0.6875\n- Feature coordinates: combined_score=0.69\n- Focus areas: - Fitness unchanged at 0.6875\n- Exploring combined_score=0.69 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.6875, num_samples: 8.0000, raw_scores: [0.75, 0.875, 0.4375]\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.6875)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000\n        self.chunk_overlap = 200\n        self.top_k = 5\n        self.temperature = 0.7\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6875), Performs well on num_samples (8.0000), Performs well on raw_scores ([0.75, 0.875, 0.4375])\n\n\n\n# Current Program\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000\n        self.chunk_overlap = 200\n        self.top_k = 5\n        self.temperature = 0.7\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n        self.chunk_size = 1000\n        self.chunk_overlap = 200\n        self.top_k = 5\n        self.temperature = 0.7\n=======\n        self.chunk_size = 800               # Reduce chunk size for more focused chunks\n        self.chunk_overlap = 150            # Slightly less overlap to reduce redundancy\n        self.top_k = 7                     # Increase retrieval to get more context, improving recall\n        self.temperature = 0.3             # Lower temperature for more precise and faithful answers\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n=======\n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n=======\n        # Vector Store (FAISS) with normalization and ef construction for better search quality\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n            # Force normalize embeddings for cosine similarity (default)\n            self.vector_store.index.metric_type = \"cosine\"\n        else:\n            self.vector_store = None\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n=======\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Improved chunking:\n        - Use header-based splitting to respect markdown structure.\n        - Split by code fences and keep atomic if dense.\n        - Smaller chunks for text with overlap.\n        \"\"\"\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        HEADER_RE = re.compile(r\"^(#{1,4})\\s*(.+)$\", re.MULTILINE)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while|var|let|const)\\b\")\n\n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n\n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines:\n                return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n\n        # First, split text by headers to respect document structure\n        header_positions = [(m.start(), m.group(0), len(m.group(1))) for m in HEADER_RE.finditer(text)]\n        header_positions.append((len(text), \"\", 0))  # Sentinel for end\n\n        for i in range(len(header_positions) - 1):\n            start_pos = header_positions[i][0]\n            end_pos = header_positions[i + 1][0]\n            section_text = text[start_pos:end_pos].strip()\n            if not section_text or _is_navigation_chunk(section_text):\n                continue\n\n            # Now within section, split out code fences first\n            cursor_sec = 0\n            for match in CODE_FENCE_RE.finditer(section_text):\n                cs, ce = match.span()\n                # Text before code fence\n                if cs > cursor_sec:\n                    pre_text = section_text[cursor_sec:cs].strip()\n                    if pre_text and not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n                # Code fence block\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        # Keep code block atomic\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        # Treat as text if low density code\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor_sec = ce\n            # Tail text after last code fence in section\n            tail_text = section_text[cursor_sec:].strip()\n            if tail_text and not _is_navigation_chunk(tail_text):\n                self._make_text_chunks(tail_text, source, chunks, self.chunk_size, self.chunk_overlap)\n\n        return chunks\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n=======\n        # Standard rolling window split for text parts with sentence boundary awareness\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n\n        sentences = sent_tokenize(text)\n        current_chunk = \"\"\n        for sentence in sentences:\n            if len(current_chunk) + len(sentence) + 1 <= size:\n                current_chunk += (sentence + \" \")\n            else:\n                chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                # Overlap by reusing last sentences from previous chunk to next chunk\n                overlap_sentences = current_chunk.strip().split()[-overlap//5:]  # approx overlap in words\n                current_chunk = \" \".join(overlap_sentences) + \" \" + sentence + \" \"\n        if current_chunk.strip():\n            chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Retrieval\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n=======\n        # Retrieval: use MMR to diversify retrieval, fallback to similarity if MMR unavailable\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=15)\n        except AttributeError:\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation: improved prompt with explicit instructions to enhance faithfulness and relevance\n        prompt = (\n            f\"You are an expert assistant answering questions based on the provided context from Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer the question strictly based on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"Do not make up information. If unsure, say 'I don't know based on the provided context.'\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}