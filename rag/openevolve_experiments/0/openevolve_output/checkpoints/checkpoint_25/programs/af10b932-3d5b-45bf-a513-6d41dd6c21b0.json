{"id": "af10b932-3d5b-45bf-a513-6d41dd6c21b0", "code": "import os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000\n        self.chunk_overlap = 200\n        self.top_k = 5\n        self.temperature = 0.7\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": null, "generation": 0, "timestamp": 1769111277.4447718, "iteration_found": 0, "metrics": {"combined_score": 0.6875, "num_samples": 8, "raw_scores": [0.75, 0.875, 0.4375]}, "complexity": 0.0, "diversity": 0.0, "metadata": {"island": 0}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}