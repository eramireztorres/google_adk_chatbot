{"id": "74b7e8af-b7fb-45e6-a46e-6e1fa9f78e4f", "code": "import os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000\n        self.chunk_overlap = 200\n        self.top_k = 10\n        self.temperature = 0.2\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS) with persistence for faster reloads\n        index_path = os.path.join(self.docs_dir, \"faiss_index\")\n        if docs:\n            if os.path.exists(index_path):\n                try:\n                    self.vector_store = FAISS.load_local(index_path, embeddings)\n                except Exception:\n                    self.vector_store = FAISS.from_documents(docs, embeddings)\n                    self.vector_store.save_local(index_path)\n            else:\n                self.vector_store = FAISS.from_documents(docs, embeddings)\n                self.vector_store.save_local(index_path)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Enhanced chunking: \n        1) Split first by markdown headers (## or ###),\n        2) then apply code fence splitting and density check per section,\n        3) fallback to rolling window chunking with overlap.\n        \"\"\"\n        HEADER_RE = re.compile(r\"^(#{2,3})\\s+(.*)$\", re.MULTILINE)\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while|var|let|const|func)\\b\")\n\n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n\n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        def chunk_code_or_text(segment: str):\n            chunks = []\n            cursor = 0\n            for match in CODE_FENCE_RE.finditer(segment):\n                start, end = match.span()\n                if start > cursor:\n                    pre_text = segment[cursor:start]\n                    if pre_text.strip() and not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor = end\n            tail = segment[cursor:]\n            if tail.strip() and not _is_navigation_chunk(tail):\n                self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            return chunks\n\n        chunks = []\n        # Split by headers first for semantic chunking\n        positions = [(m.start(), m.group(2)) for m in HEADER_RE.finditer(text)]\n        if not positions:\n            # no headers found fallback to code fence chunking + rolling text\n            chunks.extend(chunk_code_or_text(text))\n            return chunks\n\n        # add a sentinel at the end\n        positions.append((len(text), None))\n\n        for i in range(len(positions)-1):\n            start_pos = positions[i][0]\n            end_pos = positions[i+1][0]\n            segment = text[start_pos:end_pos].strip()\n            if segment and not _is_navigation_chunk(segment):\n                # chunk code fences inside segment\n                segment_chunks = chunk_code_or_text(segment)\n                chunks.extend(segment_chunks)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Try splitting on markdown headers first to preserve semantic coherence for large texts\n        HEADER_RE = re.compile(r\"^(#{1,6})\\s*(.+)$\", re.MULTILINE)\n        positions = [m.start() for m in HEADER_RE.finditer(text)]\n        if positions and len(text) > size:\n            positions.append(len(text))\n            for i in range(len(positions)-1):\n                segment = text[positions[i]:positions[i+1]].strip()\n                if len(segment) > size:\n                    # Recursively chunk large segments with overlap to preserve continuity\n                    self._make_text_chunks(segment, source, chunks_list, size, overlap)\n                elif segment:\n                    chunks_list.append(Document(page_content=segment, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval using MMR for diversity and relevance\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=self.top_k*4, lambda_mult=0.7)\n        except Exception:\n            # fallback to similarity search if MMR unsupported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Enhanced prompt with instruction to answer precisely, mention source filenames, and handle code carefully\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer strictly using the context. Prioritize any code snippets in explanations. \"\n            \"Cite source filenames as [Source 1], [Source 2], etc. Do not hallucinate information. \"\n            \"If the answer is not in context, reply: \\\"I don't know based on the provided documents.\\\"\\n\"\n            \"Provide a concise, clear, and structured answer.\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "de8b57ac-8647-471d-89af-fa667211a5c1", "generation": 3, "timestamp": 1769084485.752368, "iteration_found": 15, "metrics": {"combined_score": 0.5270833333333333, "num_samples": 8, "raw_scores": [0.75, 0.75, 0.08125]}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 27 lines with 27 lines\nChange 2: Replace 6 lines with 6 lines\nChange 3: Replace 12 lines with 13 lines\nChange 4: Replace 32 lines with 41 lines", "parent_metrics": {"combined_score": 0.32208333333333333, "num_samples": 8, "raw_scores": [0.5, 0.375, 0.09125]}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer. Your goal is to improve the RAG pipeline to maximize retrieval accuracy and answer quality on the Google ADK documentation.\n\nThe initial program has a basic chunking strategy and retrieval setup. You have full control to evolve:\n\n1. **Chunking Strategy**: Modify `_chunk_document` and `_make_text_chunks`. Experiment with:\n   - Code fence handling (density checks, language detection).\n   - Header-based splitting (markdown structure).\n   - Semantic chunking.\n   - Varying chunk sizes and overlaps.\n\n2. **Retrieval**: Modify `RAGSystem.__init__` and `query`. Experiment with:\n   - `top_k` (k) parameter.\n   - Vector store choice (prefer FAISS for indexing and retrieval).\n   - Retrieval strategy (similarity vs. MMR) and relevance thresholds if you implement them.\n   - Optional FAISS persistence (save/load local index) and relevance score filtering.\n    - Hybrid search (if you can implement keyword search to combine with vector search).\n    - Re-ranking (if feasible with available libraries/APIs, or simple heuristic re-ranking).\n    - Query expansion or rewriting before retrieval.\n   \n3. **Generation**: Modify the prompt in `query`.\n\n**Constraints**:\n- You MUST maintain the `evaluate_rag(docs_path, query)` function signature and return dictionary specific keys (`answer`, `contexts`).\n- You MUST keep the `RAGSystem` class structure (or equivalent) to ensure `evaluate_rag` works.\n- Code must be robust and handle potential parsing errors resiliently.\n\nThe evaluator uses \"Evidently\" to compute a combined score of Correctness, Faithfulness, Relevance, etc. Maximizing this score is your primary objective.\n", "user": "# Current Program Information\n- Fitness: 0.3221\n- Feature coordinates: combined_score=0.32\n- Focus areas: - Fitness declined: 0.6083 \u2192 0.3221. Consider revising recent changes.\n- Exploring combined_score=0.32 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Metrics: combined_score: 0.6083, num_samples: 8.0000, raw_scores: [0.625, 0.75, 0.45000000000000007]\n- Outcome: Improvement in all metrics\n\n### Attempt 2\n- Changes: Change 1: Replace 4 lines with 4 lines\nChange 2: Replace 53 lines with 65 lines\nChange 3: Replace 23 lines with 28 lines\n- Metrics: combined_score: 0.6190, num_samples: 8.0000, raw_scores: [0.625, 0.875, 0.3571428571428571]\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 4 lines with 4 lines\nChange 2: Replace 53 lines with 59 lines\nChange 3: Replace 23 lines with 28 lines\n- Metrics: combined_score: 0.6807, num_samples: 8.0000, raw_scores: [0.75, 0.875, 0.41718750000000004]\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.6807)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200\n        self.chunk_overlap = 300\n        self.top_k = 8\n        self.temperature = 0.3\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Enhanced chunking: \n        1) Split first by markdown headers (## or ###),\n        2) Then apply code fence splitting and density checks per section,\n        3) Finally chunk text pieces with overlap.\n        \"\"\"\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while|var|let|const|async|await|struct|interface)\\b\")\n        HEADER_RE = re.compile(r\"^(#{2,3})\\s+(.+)$\", re.MULTILINE)\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n        \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        \n        # Split by headers first to respect logical sections\n        sections = []\n        last_pos = 0\n        for m in HEADER_RE.finditer(text):\n            start = m.start()\n            if start > last_pos:\n                sections.append(text[last_pos:start])\n            last_pos = start\n        sections.append(text[last_pos:])\n\n        for section in sections:\n            cursor = 0\n            # Within each section, split by code fences\n            for match in CODE_FENCE_RE.finditer(section):\n                start, end = match.span()\n                if start > cursor:\n                    pre_text = section[cursor:start]\n                    if pre_text.strip() and not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor = end\n            tail = section[cursor:]\n            if tail.strip() and not _is_navigation_chunk(tail):\n                self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval with query reformulation: add keywords from query for hybrid search\n        keywords = \" \".join(re.findall(r\"\\b\\w{4,}\\b\", query_str.lower()))\n        combined_query = f\"{query_str} {keywords}\"\n        \n        retrieved = self.vector_store.similarity_search(combined_query, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation prompt enhanced for faithfulness and code prioritization, plus answer format guidance\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question using only the context provided. \"\n            \"If code examples are present, highlight and explain them clearly. \"\n            \"Cite the source file names when relevant. \"\n            \"If the answer is not contained in the context, say 'I don't know based on the provided information.'\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6807), Performs well on num_samples (8.0000), Performs well on raw_scores ([0.75, 0.875, 0.41718750000000004])\n\n### Program 2 (Score: 0.6190)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200  # Slightly larger chunks to capture more context\n        self.chunk_overlap = 300 # Increase overlap to preserve context boundaries better\n        self.top_k = 7           # Retrieve more chunks to improve recall\n        self.temperature = 0.3   # Lower temperature for more precise answers\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Enhanced chunking that splits by markdown headers and code fences,\n        and then applies rolling window chunking to smaller text parts.\n        \"\"\"\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        HEADER_RE = re.compile(r\"^(#{1,4})\\s*(.+)$\", re.MULTILINE)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n\n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n\n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        def _split_by_header(text: str) -> List[str]:\n            # Split text into sections by markdown headers (1-4 #)\n            splits = []\n            last_index = 0\n            for match in HEADER_RE.finditer(text):\n                start = match.start()\n                if start > last_index:\n                    splits.append(text[last_index:start].strip())\n                last_index = start\n            splits.append(text[last_index:].strip())\n            return [s for s in splits if s]\n\n        chunks = []\n        cursor = 0\n        # Process code fences first, splitting text accordingly\n        last_pos = 0\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            pre_text = text[last_pos:start]\n            if pre_text.strip() and not _is_navigation_chunk(pre_text):\n                # Further split pre_text by headers\n                sections = _split_by_header(pre_text)\n                for sec in sections:\n                    self._make_text_chunks(sec, source, chunks, self.chunk_size, self.chunk_overlap)\n\n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                if density > 0.2:\n                    chunks.append(Document(\n                        page_content=fence,\n                        metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                    ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            last_pos = end\n\n        tail = text[last_pos:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            sections = _split_by_header(tail)\n            for sec in sections:\n                self._make_text_chunks(sec, source, chunks, self.chunk_size, self.chunk_overlap)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieve top_k with similarity search (FAISS)\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n\n        # Compose context block with numbered sources\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # More detailed prompt to improve faithfulness and answer quality\n        prompt = (\n            f\"You are an expert assistant answering questions about Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Instructions:\\n\"\n            \"- Answer based strictly on the context provided.\\n\"\n            \"- If the context contains code examples, prioritize them in your answer.\\n\"\n            \"- Be concise, accurate, and reference sources if applicable.\\n\"\n            \"- If unsure, say you don't know rather than hallucinate.\\n\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6190), Performs well on num_samples (8.0000), Performs well on raw_scores ([0.625, 0.875, 0.3571428571428571])\n\n### Program 3 (Score: 0.6083)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000\n        self.chunk_overlap = 200\n        self.top_k = 5\n        self.temperature = 0.7\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6083), Performs well on num_samples (8.0000), Performs well on raw_scores ([0.625, 0.75, 0.45000000000000007])\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.5927)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution - increased chunk size and overlap for more context,\n        # slightly higher top_k for better retrieval, lowered temperature for more precise answers\n        self.chunk_size = 1200\n        self.chunk_overlap = 300\n        self.top_k = 8\n        self.temperature = 0.3\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Improved chunking:\n        - Split text by markdown headers (## or ###) for logical semantic chunks.\n        - Then apply code fence detection per chunk.\n        - Avoid navigation chunks.\n        \"\"\"\n        HEADER_RE = re.compile(r\"^(#{2,3})\\s+(.*)$\", re.MULTILINE)\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while|var|const|let|function)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        # Split by headers first\n        splits = []\n        last_pos = 0\n        for m in HEADER_RE.finditer(text):\n            start = m.start()\n            if start > last_pos:\n                splits.append(text[last_pos:start])\n            last_pos = start\n        splits.append(text[last_pos:])\n\n        for segment in splits:\n            if not segment.strip() or _is_navigation_chunk(segment):\n                continue\n            cursor = 0\n            # Process code fences inside each segment\n            for match in CODE_FENCE_RE.finditer(segment):\n                start, end = match.span()\n                if start > cursor:\n                    pre_text = segment[cursor:start]\n                    if pre_text.strip():\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor = end\n            tail = segment[cursor:]\n            if tail.strip():\n                self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n### Program D2 (Score: 0.5260)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200\n        self.chunk_overlap = 300\n        self.top_k = 8\n        self.temperature = 0.3\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Enhanced chunking: \n        1) Split first by markdown headers (## or ###),\n        2) then apply code fence splitting and density check per section,\n        3) fallback to rolling window chunking with overlap.\n        \"\"\"\n        HEADER_RE = re.compile(r\"^(#{2,3})\\s+(.*)$\", re.MULTILINE)\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while|var|let|const|func)\\b\")\n\n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n\n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        def chunk_code_or_text(segment: str):\n            chunks = []\n            cursor = 0\n            for match in CODE_FENCE_RE.finditer(segment):\n                start, end = match.span()\n                if start > cursor:\n                    pre_text = segment[cursor:start]\n                    if pre_text.strip() and not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor = end\n            tail = segment[cursor:]\n            if tail.strip() and not _is_navigation_chunk(tail):\n                self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            return chunks\n\n        chunks = []\n        # Split by headers first for semantic chunking\n        positions = [(m.start(), m.group(2)) for m in HEADER_RE.finditer(text)]\n        if not positions:\n            # no headers found fallback to code fence chunking + rolling text\n            chunks.extend(chunk_code_or_text(text))\n            return chunks\n\n        # add a sentinel at the end\n        positions.append((len(text), None))\n\n        for i in range(len(positions)-1):\n            start_pos = positions[i][0]\n            end_pos = positions[i+1][0]\n            segment = text[start_pos:end_pos].strip()\n            if segment and not _is_navigation_chunk(segment):\n                # chunk code fences inside segment\n                segment_chunks = chunk_code_or_text(segment)\n                chunks.extend(segment_chunks)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval using MMR for diversity and relevance\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=self.top_k*3)\n        except Exception:\n            # fallback to similarity search if MMR unsupported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Enhanced prompt with instruction to answer precisely, mention source filenames, and handle code carefully\n        prompt = (\n            f\"You are a knowledgeable assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Please answer the question strictly based on the context. \"\n            \"If the context contains code snippets, explain them clearly. \"\n            \"Cite the source files by their names in your answer where relevant. \"\n            \"If insufficient information is available, respond that you don't know.\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.6807, Type: Alternative)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200\n        self.chunk_overlap = 300\n        self.top_k = 8\n        self.temperature = 0.3\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Enhanced chunking: \n        1) Split first by markdown headers (## or ###),\n        2) Then apply code fence splitting and density checks per section,\n        3) Finally chunk text pieces with overlap.\n        \"\"\"\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while|var|let|const|async|await|struct|interface)\\b\")\n        HEADER_RE = re.compile(r\"^(#{2,3})\\s+(.+)$\", re.MULTILINE)\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n        \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        \n        # Split by headers first to respect logical sections\n        sections = []\n        last_pos = 0\n        for m in HEADER_RE.finditer(text):\n            start = m.start()\n            if start > last_pos:\n                sections.append(text[last_pos:start])\n            last_pos = start\n        sections.append(text[last_pos:])\n\n        for section in sections:\n            cursor = 0\n            # Within each section, split by code fences\n            for match in CODE_FENCE_RE.finditer(section):\n                start, end = match.span()\n                if start > cursor:\n                    pre_text = section[cursor:start]\n                    if pre_text.strip() and not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor = end\n            tail = section[cursor:]\n            if tail.strip() and not _is_navigation_chunk(tail):\n                self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval with query reformulation: add keywords from query for hybrid search\n        keywords = \" \".join(re.findall(r\"\\b\\w{4,}\\b\", query_str.lower()))\n        combined_query = f\"{query_str} {keywords}\"\n        \n        retrieved = self.vector_store.similarity_search(combined_query, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation prompt enhanced for faithfulness and code prioritization, plus answer format guidance\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question using only the context provided. \"\n            \"If code examples are present, highlight and explain them clearly. \"\n            \"Cite the source file names when relevant. \"\n            \"If the answer is not contained in the context, say 'I don't know based on the provided information.'\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Mixed iteration strategies\n\n### Inspiration 2 (Score: 0.6083, Type: Alternative)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000\n        self.chunk_overlap = 200\n        self.top_k = 5\n        self.temperature = 0.7\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Mixed iteration strategies\n\n### Inspiration 3 (Score: 0.5927, Type: Experimental)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution - increased chunk size and overlap for more context,\n        # slightly higher top_k for better retrieval, lowered temperature for more precise answers\n        self.chunk_size = 1200\n        self.chunk_overlap = 300\n        self.top_k = 8\n        self.temperature = 0.3\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Improved chunking:\n        - Split text by markdown headers (## or ###) for logical semantic chunks.\n        - Then apply code fence detection per chunk.\n        - Avoid navigation chunks.\n        \"\"\"\n        HEADER_RE = re.compile(r\"^(#{2,3})\\s+(.*)$\", re.MULTILINE)\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while|var|const|let|function)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        # Split by headers first\n        splits = []\n        last_pos = 0\n        for m in HEADER_RE.finditer(text):\n            start = m.start()\n            if start > last_pos:\n                splits.append(text[last_pos:start])\n            last_pos = start\n        splits.append(text[last_pos:])\n\n        for segment in splits:\n            if not segment.strip() or _is_navigation_chunk(segment):\n                continue\n            cursor = 0\n            # Process code fences inside each segment\n            for match in CODE_FENCE_RE.finditer(segment):\n                start, end = match.span()\n                if start > cursor:\n                    pre_text = segment[cursor:start]\n                    if pre_text.strip():\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor = end\n            tail = segment[cursor:]\n            if tail.strip():\n                self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], Object-oriented approach\n\n# Current Program\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000\n        self.chunk_overlap = 200\n        self.top_k = 10\n        self.temperature = 0.2\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Enhanced chunking: \n        1) Split first by markdown headers (## or ###),\n        2) then apply code fence splitting and density check per section,\n        3) fallback to rolling window chunking with overlap.\n        \"\"\"\n        HEADER_RE = re.compile(r\"^(#{2,3})\\s+(.*)$\", re.MULTILINE)\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while|var|let|const|func)\\b\")\n\n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n\n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        def chunk_code_or_text(segment: str):\n            chunks = []\n            cursor = 0\n            for match in CODE_FENCE_RE.finditer(segment):\n                start, end = match.span()\n                if start > cursor:\n                    pre_text = segment[cursor:start]\n                    if pre_text.strip() and not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor = end\n            tail = segment[cursor:]\n            if tail.strip() and not _is_navigation_chunk(tail):\n                self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            return chunks\n\n        chunks = []\n        # Split by headers first for semantic chunking\n        positions = [(m.start(), m.group(2)) for m in HEADER_RE.finditer(text)]\n        if not positions:\n            # no headers found fallback to code fence chunking + rolling text\n            chunks.extend(chunk_code_or_text(text))\n            return chunks\n\n        # add a sentinel at the end\n        positions.append((len(text), None))\n\n        for i in range(len(positions)-1):\n            start_pos = positions[i][0]\n            end_pos = positions[i+1][0]\n            segment = text[start_pos:end_pos].strip()\n            if segment and not _is_navigation_chunk(segment):\n                # chunk code fences inside segment\n                segment_chunks = chunk_code_or_text(segment)\n                chunks.extend(segment_chunks)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Try splitting on markdown headers first to preserve semantic coherence for large texts\n        HEADER_RE = re.compile(r\"^(#{1,6})\\s*(.+)$\", re.MULTILINE)\n        positions = [m.start() for m in HEADER_RE.finditer(text)]\n        if positions and len(text) > size:\n            positions.append(len(text))\n            for i in range(len(positions)-1):\n                segment = text[positions[i]:positions[i+1]].strip()\n                if len(segment) > size:\n                    # Recursively chunk large segments without overlap to avoid redundancy\n                    self._make_text_chunks(segment, source, chunks_list, size, 0)\n                elif segment:\n                    chunks_list.append(Document(page_content=segment, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval using MMR for diversity and relevance\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=self.top_k*4, lambda_mult=0.5)\n        except Exception:\n            # fallback to similarity search if MMR unsupported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Enhanced prompt with instruction to answer precisely, mention source filenames, and handle code carefully\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer strictly using the context. Prioritize any code snippets in explanations. \"\n            \"Cite source filenames as [Source 1], [Source 2], etc. Do not hallucinate information. \"\n            \"If the answer is not in context, reply: \\\"I don't know based on the provided documents.\\\"\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Try splitting on markdown headers first to preserve semantic coherence for large texts\n        HEADER_RE = re.compile(r\"^(#{1,6})\\s*(.+)$\", re.MULTILINE)\n        positions = [m.start() for m in HEADER_RE.finditer(text)]\n        if positions and len(text) > size:\n            positions.append(len(text))\n            for i in range(len(positions)-1):\n                segment = text[positions[i]:positions[i+1]].strip()\n                if len(segment) > size:\n                    # Recursively chunk large segments without overlap to avoid redundancy\n                    self._make_text_chunks(segment, source, chunks_list, size, 0)\n                elif segment:\n                    chunks_list.append(Document(page_content=segment, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n=======\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Try splitting on markdown headers first to preserve semantic coherence for large texts\n        HEADER_RE = re.compile(r\"^(#{1,6})\\s*(.+)$\", re.MULTILINE)\n        positions = [m.start() for m in HEADER_RE.finditer(text)]\n        if positions and len(text) > size:\n            positions.append(len(text))\n            for i in range(len(positions)-1):\n                segment = text[positions[i]:positions[i+1]].strip()\n                if len(segment) > size:\n                    # Recursively chunk large segments with overlap to preserve continuity\n                    self._make_text_chunks(segment, source, chunks_list, size, overlap)\n                elif segment:\n                    chunks_list.append(Document(page_content=segment, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Retrieval using MMR for diversity and relevance\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=self.top_k*4, lambda_mult=0.5)\n        except Exception:\n            # fallback to similarity search if MMR unsupported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n=======\n        # Retrieval using MMR for diversity and relevance\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=self.top_k*4, lambda_mult=0.7)\n        except Exception:\n            # fallback to similarity search if MMR unsupported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Enhanced prompt with instruction to answer precisely, mention source filenames, and handle code carefully\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer strictly using the context. Prioritize any code snippets in explanations. \"\n            \"Cite source filenames as [Source 1], [Source 2], etc. Do not hallucinate information. \"\n            \"If the answer is not in context, reply: \\\"I don't know based on the provided documents.\\\"\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n=======\n        # Enhanced prompt with instruction to answer precisely, mention source filenames, and handle code carefully\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer strictly using the context. Prioritize any code snippets in explanations. \"\n            \"Cite source filenames as [Source 1], [Source 2], etc. Do not hallucinate information. \"\n            \"If the answer is not in context, reply: \\\"I don't know based on the provided documents.\\\"\\n\"\n            \"Provide a concise, clear, and structured answer.\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n=======\n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS) with persistence for faster reloads\n        index_path = os.path.join(self.docs_dir, \"faiss_index\")\n        if docs:\n            if os.path.exists(index_path):\n                try:\n                    self.vector_store = FAISS.load_local(index_path, embeddings)\n                except Exception:\n                    self.vector_store = FAISS.from_documents(docs, embeddings)\n                    self.vector_store.save_local(index_path)\n            else:\n                self.vector_store = FAISS.from_documents(docs, embeddings)\n                self.vector_store.save_local(index_path)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n>>>>>>> REPLACE\n\n# Explanation\n\n# 1) In _make_text_chunks, changed recursive chunking of large header-based segments to preserve overlap instead of removing overlap. This helps maintain context continuity across chunks, which can improve retrieval relevance and answer quality.\n\n# 2) In query retrieval, increased lambda_mult parameter for max_marginal_relevance_search from 0.5 to 0.7 to favor more diverse but still relevant results, potentially improving recall and reducing redundancy in retrieved chunks.\n\n# 3) Enhanced the generation prompt by explicitly instructing the assistant to produce concise, clear, and structured answers and added a newline before it to improve prompt formatting and potentially improve answer quality and faithfulness.\n\n# 4) Added persistence for FAISS index in _initialize_system to save/load local index from disk under docs_dir/faiss_index. This improves efficiency on repeated runs and may allow more consistent retrieval behavior, indirectly improving overall system performance.\n\n# These changes are targeted, maintain the overall structure and interfaces, and aim to improve retrieval context quality, diversity, and generation prompt clarity to maximize combined fitness."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}