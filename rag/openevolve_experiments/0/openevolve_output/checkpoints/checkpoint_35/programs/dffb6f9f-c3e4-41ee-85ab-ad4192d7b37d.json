{"id": "dffb6f9f-c3e4-41ee-85ab-ad4192d7b37d", "code": "import os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 800               # Reduce chunk size for more focused chunks\n        self.chunk_overlap = 150            # Slightly less overlap to reduce redundancy\n        self.top_k = 7                     # Increase retrieval to get more context\n        self.temperature = 0.3             # Lower temperature for more factual answers\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS) with normalization for cosine similarity\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n            # Enforce cosine similarity metric explicitly\n            try:\n                self.vector_store.index.metric_type = 1  # 1 for cosine in FAISS\n                # Normalize all vectors in index for cosine similarity to be accurate\n                import faiss\n                import numpy as np\n                xb = np.vstack([self.vector_store.index.reconstruct(i) for i in range(self.vector_store.index.ntotal)])\n                norms = np.linalg.norm(xb, axis=1, keepdims=True)\n                xb_norm = xb / (norms + 1e-10)\n                self.vector_store.index.reset()\n                self.vector_store.index = faiss.IndexFlatIP(self.vector_store.index.d)\n                self.vector_store.index.add(xb_norm)\n            except Exception:\n                pass\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Improved chunking: try to split on markdown headers to keep semantic coherence\n        HEADER_RE = re.compile(r\"(#{1,6} .+)\")\n        \n        # If text small enough, no splitting\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n        \n        # Split on headers if possible\n        splits = HEADER_RE.split(text)\n        # splits will alternate between non-header and header lines, e.g. ['', '# Header1', 'text1', '# Header2', 'text2', ...]\n        # We'll reconstruct sections as header + content\n        sections = []\n        i = 0\n        while i < len(splits):\n            if splits[i].startswith('#'):\n                header = splits[i].strip()\n                content = splits[i+1] if i+1 < len(splits) else ''\n                sections.append(header + \"\\n\" + content)\n                i += 2\n            else:\n                # Text before first header or in between headers\n                if splits[i].strip():\n                    sections.append(splits[i])\n                i += 1\n        \n        # Now chunk sections further if needed, with overlap and try to break at sentence boundaries when possible\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n        \n        for sec in sections:\n            if len(sec) <= size:\n                chunks_list.append(Document(page_content=sec, metadata={\"source\": source, \"type\": \"text\"}))\n            else:\n                # Use sentence-based rolling window with overlap\n                sentences = sent_tokenize(sec)\n                current_chunk = \"\"\n                current_len = 0\n                for i, sent in enumerate(sentences):\n                    sent_len = len(sent)\n                    if current_len + sent_len + 1 > size:\n                        if current_chunk:\n                            chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            # Overlap: reuse last sentences approximately corresponding to overlap chars\n                            # Prefer whole sentences instead of words for overlap to preserve semantics\n                            overlap_sentences = []\n                            overlap_char_count = 0\n                            # Collect sentences from end backward until overlap char count reached\n                            for s in reversed(sentences[:i]):\n                                overlap_sentences.insert(0, s)\n                                overlap_char_count += len(s) + 1\n                                if overlap_char_count >= overlap:\n                                    break\n                            current_chunk = \" \".join(overlap_sentences) + \" \" + sent\n                            current_len = sum(len(s) + 1 for s in overlap_sentences) + sent_len + 1\n                        else:\n                            # Single sentence longer than chunk size, just append\n                            chunks_list.append(Document(page_content=sent.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            current_chunk = \"\"\n                            current_len = 0\n                    else:\n                        if current_chunk:\n                            current_chunk += \" \" + sent\n                        else:\n                            current_chunk = sent\n                        current_len += sent_len + 1\n                if current_chunk.strip():\n                    chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval: use MMR for more diverse relevant results if available\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=15)\n        except AttributeError:\n            # fallback to similarity search if MMR not supported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        \n        # Filter retrieved documents by simple keyword presence to boost relevance\n        query_terms = set(re.findall(r\"\\w+\", query_str.lower()))\n        filtered_docs = []\n        for doc in retrieved:\n            content_lower = doc.page_content.lower()\n            # Require at least 2 distinct query terms present to improve relevance filtering\n            match_count = sum(1 for term in query_terms if term in content_lower)\n            if match_count >= 2:\n                filtered_docs.append(doc)\n        # Limit to top_k after filtering, fallback if none pass\n        filtered_docs = filtered_docs[:self.top_k] if filtered_docs else retrieved[:self.top_k]\n\n        context_block = \"\"\n        for i, doc in enumerate(filtered_docs):\n            src = doc.metadata.get(\"source\", \"unknown\")\n            context_block += f\"Source {i+1} ({src}):\\n{doc.page_content}\\n\\n\"\n\n        # Generation with clearer instructions on faithfulness, code priority, and uncertainty\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer strictly based on the context above. \"\n            \"Use code examples to illustrate your answer if available. \"\n            \"Do not invent information. If the answer is not contained in the context, say 'I don't know based on the provided context.' \"\n            \"Provide concise, accurate, and faithful responses.\\n\"\n            \"Always cite source filenames when relevant to support your answers.\\n\"\n            \"If the question is ambiguous or context insufficient, respond accordingly.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": [doc.page_content for doc in filtered_docs]}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "7864fff2-dfcf-4ae9-97a7-886c4f26705c", "generation": 4, "timestamp": 1769116770.7141826, "iteration_found": 34, "metrics": {"combined_score": 0.620436507936508, "num_samples": 8, "raw_scores": [0.5, 1.0, 0.36130952380952386]}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 9 lines with 11 lines\nChange 2: Replace 65 lines with 73 lines\nChange 3: Replace 14 lines with 15 lines", "parent_metrics": {"combined_score": 0.5625, "num_samples": 8, "raw_scores": [0.625, 0.75, 0.31250000000000006]}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer. Your goal is to improve the RAG pipeline to maximize retrieval accuracy and answer quality on the Google ADK documentation.\n\nThe initial program has a basic chunking strategy and retrieval setup. You have full control to evolve:\n\n1. **Chunking Strategy**: Modify `_chunk_document` and `_make_text_chunks`. Experiment with:\n   - Code fence handling (density checks, language detection).\n   - Header-based splitting (markdown structure).\n   - Semantic chunking.\n   - Varying chunk sizes and overlaps.\n\n2. **Retrieval**: Modify `RAGSystem.__init__` and `query`. Experiment with:\n   - `top_k` (k) parameter.\n   - Vector store choice (prefer FAISS for indexing and retrieval).\n   - Retrieval strategy (similarity vs. MMR) and relevance thresholds if you implement them.\n   - Optional FAISS persistence (save/load local index) and relevance score filtering.\n    - Hybrid search (if you can implement keyword search to combine with vector search).\n    - Re-ranking (if feasible with available libraries/APIs, or simple heuristic re-ranking).\n    - Query expansion or rewriting before retrieval.\n   \n3. **Generation**: Modify the prompt in `query`.\n\n**Constraints**:\n- You MUST maintain the `evaluate_rag(docs_path, query)` function signature and return dictionary specific keys (`answer`, `contexts`).\n- You MUST keep the `RAGSystem` class structure (or equivalent) to ensure `evaluate_rag` works.\n- Code must be robust and handle potential parsing errors resiliently.\n\nThe evaluator uses \"Evidently\" to compute a combined score of Correctness, Faithfulness, Relevance, etc. Maximizing this score is your primary objective.\n", "user": "# Current Program Information\n- Fitness: 0.5625\n- Feature coordinates: combined_score=0.56\n- Focus areas: - Fitness declined: 0.6875 \u2192 0.5625. Consider revising recent changes.\n- Exploring combined_score=0.56 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Metrics: combined_score: 0.6875, num_samples: 8.0000, raw_scores: [0.75, 0.875, 0.4375]\n- Outcome: Improvement in all metrics\n\n### Attempt 2\n- Changes: Change 1: Replace 4 lines with 4 lines\nChange 2: Replace 53 lines with 59 lines\nChange 3: Replace 23 lines with 25 lines\n- Metrics: combined_score: 0.7494, num_samples: 8.0000, raw_scores: [0.875, 1.0, 0.3732142857142858]\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 40 lines with 50 lines\nChange 2: Replace 50 lines with 52 lines\nChange 3: Replace 12 lines with 13 lines\n- Metrics: combined_score: 0.9542, num_samples: 8.0000, raw_scores: [1.0, 1.0, 0.8625]\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9542)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000              # Increase chunk size for richer context per chunk\n        self.chunk_overlap = 250            # Increase overlap to preserve context continuity\n        self.top_k = 8                      # Slightly higher top_k for more context coverage\n        self.temperature = 0.2              # Lower temperature for more precise answers\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n            # Normalize vectors for cosine similarity to improve retrieval quality\n            try:\n                import faiss\n                # Ensure index uses cosine similarity by normalizing vectors internally\n                # We must normalize vectors ourselves before adding to index (FAISS expects this)\n                from numpy import linalg, asarray\n                def normalize(vectors):\n                    norms = linalg.norm(vectors, axis=1, keepdims=True)\n                    return vectors / (norms + 1e-10)\n                # Extract embeddings matrix and normalize\n                xb = self.vector_store.index.reconstruct_n(0, self.vector_store.index.ntotal)\n                xb_norm = normalize(xb)\n                # Rebuild index with normalized vectors\n                self.vector_store.index.reset()\n                self.vector_store.index = faiss.IndexFlatIP(embeddings.embedding_dimension)\n                self.vector_store.index.add(xb_norm)\n            except Exception:\n                pass\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Improved chunking: split on markdown headers and respect sentence boundaries for semantic coherence\n        HEADER_RE = re.compile(r\"(#{1,6} .+)\")\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n\n        # If text small enough, no splitting\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n        \n        # Split on headers if possible\n        splits = HEADER_RE.split(text)\n        # splits alternate between non-header and header lines\n        sections = []\n        i = 0\n        while i < len(splits):\n            if splits[i].startswith('#'):\n                header = splits[i].strip()\n                content = splits[i+1] if i+1 < len(splits) else ''\n                sections.append(header + \"\\n\" + content)\n                i += 2\n            else:\n                # Text before first header or in between headers\n                if splits[i].strip():\n                    sections.append(splits[i])\n                i += 1\n        \n        # Now chunk sections further using sentence tokenization if needed\n        for sec in sections:\n            sec = sec.strip()\n            if len(sec) <= size:\n                chunks_list.append(Document(page_content=sec, metadata={\"source\": source, \"type\": \"text\"}))\n            else:\n                sentences = sent_tokenize(sec)\n                current_chunk = \"\"\n                for idx, sentence in enumerate(sentences):\n                    if len(current_chunk) + len(sentence) + 1 <= size:\n                        current_chunk += sentence + \" \"\n                    else:\n                        chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                        # Overlap: reuse last ~overlap/6 words from previous chunk but try to not break sentences abruptly\n                        overlap_words = current_chunk.strip().split()[-max(overlap//6,5):]\n                        # Try to extend overlap to full sentences (simple heuristic: include sentences that fit)\n                        overlap_text = \" \".join(overlap_words)\n                        current_chunk = overlap_text + \" \" + sentence + \" \"\n                if current_chunk.strip():\n                    chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval: use MMR for more diverse relevant results if available\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=10)\n        except AttributeError:\n            # fallback to similarity search if MMR not supported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        \n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, use them to illustrate your answer. \"\n            \"Be concise, accurate, and faithful to the context. \"\n            \"If you cannot answer from the context, say 'I don't know based on the provided context.'\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.9542), Performs well on num_samples (8.0000), Performs well on raw_scores ([1.0, 1.0, 0.8625])\n\n### Program 2 (Score: 0.7494)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200\n        self.chunk_overlap = 300\n        self.top_k = 7\n        self.temperature = 0.3\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Improved chunking:\n        - Split text by markdown headers (## or ###) to align with doc structure.\n        - Then apply code fence splitting within each header chunk.\n        \"\"\"\n        HEADER_RE = re.compile(r'^(#{2,3})\\s*(.+)$', re.MULTILINE)\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n\n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n\n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n\n        # Split by headers\n        sections = []\n        last_pos = 0\n        for m in HEADER_RE.finditer(text):\n            start = m.start()\n            if start > last_pos:\n                sections.append(text[last_pos:start])\n            last_pos = start\n        sections.append(text[last_pos:])\n\n        for section in sections:\n            cursor = 0\n            # Within each section, split by code fences\n            for match in CODE_FENCE_RE.finditer(section):\n                start, end = match.span()\n                if start > cursor:\n                    pre_text = section[cursor:start]\n                    if pre_text.strip() and not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor = end\n            tail = section[cursor:]\n            if tail.strip() and not _is_navigation_chunk(tail):\n                self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval with MMR to diversify results\n        retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=self.top_k*3)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation with more explicit instructions for faithfulness and code usage\n        prompt = (\n            f\"You are a helpful assistant specialized in Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer the question based strictly on the context. Do NOT hallucinate. \"\n            \"If code examples are present, use them to clarify your answer. \"\n            \"If the answer is not contained in the context, say 'I don't know.'\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.7494), Performs well on num_samples (8.0000), Performs well on raw_scores ([0.875, 1.0, 0.3732142857142858])\n\n### Program 3 (Score: 0.6875)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000\n        self.chunk_overlap = 200\n        self.top_k = 5\n        self.temperature = 0.7\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6875), Performs well on num_samples (8.0000), Performs well on raw_scores ([0.75, 0.875, 0.4375])\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.6857)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200\n        self.chunk_overlap = 300\n        self.top_k = 7\n        self.temperature = 0.3\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Improved chunking:\n        - Split text by markdown headers (## or ###) to align with doc structure.\n        - Then apply code fence splitting within each header chunk.\n        \"\"\"\n        HEADER_RE = re.compile(r'^(#{2,3})\\s*(.+)$', re.MULTILINE)\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n\n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n\n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n\n        # Split by headers\n        sections = []\n        last_pos = 0\n        for m in HEADER_RE.finditer(text):\n            start = m.start()\n            if start > last_pos:\n                sections.append(text[last_pos:start])\n            last_pos = start\n        sections.append(text[last_pos:])\n\n        for section in sections:\n            cursor = 0\n            # Within each section, split by code fences\n            for match in CODE_FENCE_RE.finditer(section):\n                start, end = match.span()\n                if start > cursor:\n                    pre_text = section[cursor:start]\n                    if pre_text.strip() and not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n\n                lang = (match.group(1) or \"\").strip()\n                code = match.group(2)\n                if code.strip():\n                    fence = f\"```{lang}\\n{code}\\n```\"\n                    density = _code_density(code)\n                    if density > 0.2:\n                        chunks.append(Document(\n                            page_content=fence,\n                            metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                        ))\n                    else:\n                        self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n                cursor = end\n            tail = section[cursor:]\n            if tail.strip() and not _is_navigation_chunk(tail):\n                self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n\n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Standard rolling window split for text parts\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n            \n        start = 0\n        while start < len(text):\n            end = min(start + size, len(text))\n            chunk_text = text[start:end]\n            chunks_list.append(Document(page_content=chunk_text, metadata={\"source\": source, \"type\": \"text\"}))\n            start += size - overlap\n            if start >= len(text): break\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval with MMR to diversify results\n        retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=self.top_k*3)\n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation with more explicit instructions for faithfulness and code usage\n        prompt = (\n            f\"You are a helpful assistant specialized in Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer the question based strictly on the context. Do NOT hallucinate. \"\n            \"If code examples are present, use them to clarify your answer. \"\n            \"If the answer is not contained in the context, say 'I don't know.'\"\n        )\n\n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n### Program D2 (Score: 0.6867)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 800               # Reduce chunk size for more focused chunks\n        self.chunk_overlap = 150            # Slightly less overlap to reduce redundancy\n        self.top_k = 7                     # Increase retrieval to get more context\n        self.temperature = 0.3             # Lower temperature for more factual answers\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS) with normalization for cosine similarity\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n            # Enforce cosine similarity metric explicitly\n            try:\n                self.vector_store.index.metric_type = 1  # 1 for cosine in FAISS\n            except Exception:\n                pass\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Improved chunking: try to split on markdown headers to keep semantic coherence\n        HEADER_RE = re.compile(r\"(#{1,6} .+)\")\n        \n        # If text small enough, no splitting\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n        \n        # Split on headers if possible\n        splits = HEADER_RE.split(text)\n        # splits will alternate between non-header and header lines, e.g. ['', '# Header1', 'text1', '# Header2', 'text2', ...]\n        # We'll reconstruct sections as header + content\n        sections = []\n        i = 0\n        while i < len(splits):\n            if splits[i].startswith('#'):\n                header = splits[i].strip()\n                content = splits[i+1] if i+1 < len(splits) else ''\n                sections.append(header + \"\\n\" + content)\n                i += 2\n            else:\n                # Text before first header or in between headers\n                if splits[i].strip():\n                    sections.append(splits[i])\n                i += 1\n        \n        # Now chunk sections further if needed, with overlap and try to break at sentence boundaries when possible\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n        \n        for sec in sections:\n            if len(sec) <= size:\n                chunks_list.append(Document(page_content=sec, metadata={\"source\": source, \"type\": \"text\"}))\n            else:\n                # Use sentence-based rolling window with overlap\n                sentences = sent_tokenize(sec)\n                current_chunk = \"\"\n                current_len = 0\n                for i, sent in enumerate(sentences):\n                    sent_len = len(sent)\n                    if current_len + sent_len + 1 > size:\n                        if current_chunk:\n                            chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            # Overlap: reuse last sentences approximately corresponding to overlap chars\n                            overlap_words = current_chunk.strip().split()[-max(overlap//5, 5):]\n                            current_chunk = \" \".join(overlap_words) + \" \" + sent\n                            current_len = sum(len(w) + 1 for w in overlap_words) + sent_len + 1\n                        else:\n                            # Single sentence longer than chunk size, just append\n                            chunks_list.append(Document(page_content=sent.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            current_chunk = \"\"\n                            current_len = 0\n                    else:\n                        # Add a space before sentence if chunk not empty\n                        if current_chunk:\n                            current_chunk += \" \" + sent\n                            current_len += sent_len + 1\n                        else:\n                            current_chunk = sent\n                            current_len = sent_len\n                if current_chunk.strip():\n                    chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval: use MMR for more diverse relevant results if available\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=20)\n        except AttributeError:\n            # fallback to similarity search if MMR not supported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        \n        # Filter retrieved documents by simple keyword presence to boost relevance\n        query_terms = set(re.findall(r\"\\w+\", query_str.lower()))\n        filtered_docs = []\n        for doc in retrieved:\n            content_lower = doc.page_content.lower()\n            # Require that at least 2 distinct query terms appear to improve relevance filtering\n            match_count = sum(1 for term in query_terms if term in content_lower)\n            if match_count >= 2:\n                filtered_docs.append(doc)\n        # If no docs pass filter, fallback to top retrieved\n        filtered_docs = filtered_docs[:self.top_k] if filtered_docs else retrieved[:self.top_k]\n\n        context_block = \"\"\n        for i, doc in enumerate(filtered_docs):\n            src = doc.metadata.get(\"source\", \"unknown\")\n            context_block += f\"Source {i+1} ({src}):\\n{doc.page_content}\\n\\n\"\n\n        # Generation with clearer instructions on faithfulness, code priority, and uncertainty\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer strictly based on the context above. \"\n            \"Use code examples to illustrate your answer if available. \"\n            \"Do not invent information. If the answer is not contained in the context, say 'I don't know based on the provided context.' \"\n            \"Provide concise, accurate, and faithful responses. \"\n            \"Cite the source filenames when relevant.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": [doc.page_content for doc in filtered_docs]}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.9542, Type: High-Performer)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000              # Increase chunk size for richer context per chunk\n        self.chunk_overlap = 250            # Increase overlap to preserve context continuity\n        self.top_k = 8                      # Slightly higher top_k for more context coverage\n        self.temperature = 0.2              # Lower temperature for more precise answers\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS)\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n            # Normalize vectors for cosine similarity to improve retrieval quality\n            try:\n                import faiss\n                # Ensure index uses cosine similarity by normalizing vectors internally\n                # We must normalize vectors ourselves before adding to index (FAISS expects this)\n                from numpy import linalg, asarray\n                def normalize(vectors):\n                    norms = linalg.norm(vectors, axis=1, keepdims=True)\n                    return vectors / (norms + 1e-10)\n                # Extract embeddings matrix and normalize\n                xb = self.vector_store.index.reconstruct_n(0, self.vector_store.index.ntotal)\n                xb_norm = normalize(xb)\n                # Rebuild index with normalized vectors\n                self.vector_store.index.reset()\n                self.vector_store.index = faiss.IndexFlatIP(embeddings.embedding_dimension)\n                self.vector_store.index.add(xb_norm)\n            except Exception:\n                pass\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Improved chunking: split on markdown headers and respect sentence boundaries for semantic coherence\n        HEADER_RE = re.compile(r\"(#{1,6} .+)\")\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n\n        # If text small enough, no splitting\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n        \n        # Split on headers if possible\n        splits = HEADER_RE.split(text)\n        # splits alternate between non-header and header lines\n        sections = []\n        i = 0\n        while i < len(splits):\n            if splits[i].startswith('#'):\n                header = splits[i].strip()\n                content = splits[i+1] if i+1 < len(splits) else ''\n                sections.append(header + \"\\n\" + content)\n                i += 2\n            else:\n                # Text before first header or in between headers\n                if splits[i].strip():\n                    sections.append(splits[i])\n                i += 1\n        \n        # Now chunk sections further using sentence tokenization if needed\n        for sec in sections:\n            sec = sec.strip()\n            if len(sec) <= size:\n                chunks_list.append(Document(page_content=sec, metadata={\"source\": source, \"type\": \"text\"}))\n            else:\n                sentences = sent_tokenize(sec)\n                current_chunk = \"\"\n                for idx, sentence in enumerate(sentences):\n                    if len(current_chunk) + len(sentence) + 1 <= size:\n                        current_chunk += sentence + \" \"\n                    else:\n                        chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                        # Overlap: reuse last ~overlap/6 words from previous chunk but try to not break sentences abruptly\n                        overlap_words = current_chunk.strip().split()[-max(overlap//6,5):]\n                        # Try to extend overlap to full sentences (simple heuristic: include sentences that fit)\n                        overlap_text = \" \".join(overlap_words)\n                        current_chunk = overlap_text + \" \" + sentence + \" \"\n                if current_chunk.strip():\n                    chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval: use MMR for more diverse relevant results if available\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=10)\n        except AttributeError:\n            # fallback to similarity search if MMR not supported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        \n        contexts = [d.page_content for d in retrieved]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in retrieved]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({src}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, use them to illustrate your answer. \"\n            \"Be concise, accurate, and faithful to the context. \"\n            \"If you cannot answer from the context, say 'I don't know based on the provided context.'\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], Object-oriented approach\n\n### Inspiration 2 (Score: 0.6867, Type: Alternative)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 800               # Reduce chunk size for more focused chunks\n        self.chunk_overlap = 150            # Slightly less overlap to reduce redundancy\n        self.top_k = 7                     # Increase retrieval to get more context\n        self.temperature = 0.3             # Lower temperature for more factual answers\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS) with normalization for cosine similarity\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n            # Enforce cosine similarity metric explicitly\n            try:\n                self.vector_store.index.metric_type = 1  # 1 for cosine in FAISS\n            except Exception:\n                pass\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Improved chunking: try to split on markdown headers to keep semantic coherence\n        HEADER_RE = re.compile(r\"(#{1,6} .+)\")\n        \n        # If text small enough, no splitting\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n        \n        # Split on headers if possible\n        splits = HEADER_RE.split(text)\n        # splits will alternate between non-header and header lines, e.g. ['', '# Header1', 'text1', '# Header2', 'text2', ...]\n        # We'll reconstruct sections as header + content\n        sections = []\n        i = 0\n        while i < len(splits):\n            if splits[i].startswith('#'):\n                header = splits[i].strip()\n                content = splits[i+1] if i+1 < len(splits) else ''\n                sections.append(header + \"\\n\" + content)\n                i += 2\n            else:\n                # Text before first header or in between headers\n                if splits[i].strip():\n                    sections.append(splits[i])\n                i += 1\n        \n        # Now chunk sections further if needed, with overlap and try to break at sentence boundaries when possible\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n        \n        for sec in sections:\n            if len(sec) <= size:\n                chunks_list.append(Document(page_content=sec, metadata={\"source\": source, \"type\": \"text\"}))\n            else:\n                # Use sentence-based rolling window with overlap\n                sentences = sent_tokenize(sec)\n                current_chunk = \"\"\n                current_len = 0\n                for i, sent in enumerate(sentences):\n                    sent_len = len(sent)\n                    if current_len + sent_len + 1 > size:\n                        if current_chunk:\n                            chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            # Overlap: reuse last sentences approximately corresponding to overlap chars\n                            overlap_words = current_chunk.strip().split()[-max(overlap//5, 5):]\n                            current_chunk = \" \".join(overlap_words) + \" \" + sent\n                            current_len = sum(len(w) + 1 for w in overlap_words) + sent_len + 1\n                        else:\n                            # Single sentence longer than chunk size, just append\n                            chunks_list.append(Document(page_content=sent.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            current_chunk = \"\"\n                            current_len = 0\n                    else:\n                        # Add a space before sentence if chunk not empty\n                        if current_chunk:\n                            current_chunk += \" \" + sent\n                            current_len += sent_len + 1\n                        else:\n                            current_chunk = sent\n                            current_len = sent_len\n                if current_chunk.strip():\n                    chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval: use MMR for more diverse relevant results if available\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=20)\n        except AttributeError:\n            # fallback to similarity search if MMR not supported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        \n        # Filter retrieved documents by simple keyword presence to boost relevance\n        query_terms = set(re.findall(r\"\\w+\", query_str.lower()))\n        filtered_docs = []\n        for doc in retrieved:\n            content_lower = doc.page_content.lower()\n            # Require that at least 2 distinct query terms appear to improve relevance filtering\n            match_count = sum(1 for term in query_terms if term in content_lower)\n            if match_count >= 2:\n                filtered_docs.append(doc)\n        # If no docs pass filter, fallback to top retrieved\n        filtered_docs = filtered_docs[:self.top_k] if filtered_docs else retrieved[:self.top_k]\n\n        context_block = \"\"\n        for i, doc in enumerate(filtered_docs):\n            src = doc.metadata.get(\"source\", \"unknown\")\n            context_block += f\"Source {i+1} ({src}):\\n{doc.page_content}\\n\\n\"\n\n        # Generation with clearer instructions on faithfulness, code priority, and uncertainty\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer strictly based on the context above. \"\n            \"Use code examples to illustrate your answer if available. \"\n            \"Do not invent information. If the answer is not contained in the context, say 'I don't know based on the provided context.' \"\n            \"Provide concise, accurate, and faithful responses. \"\n            \"Cite the source filenames when relevant.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": [doc.page_content for doc in filtered_docs]}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Mixed iteration strategies\n\n### Inspiration 3 (Score: 0.6680, Type: Alternative)\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1000               # Increase chunk size for richer context chunks\n        self.chunk_overlap = 250            # Increase overlap to better preserve context continuity\n        self.top_k = 8                      # Slightly increase top_k for more context coverage\n        self.temperature = 0.2              # Lower temperature for more precise and faithful answers\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS) with normalization for cosine similarity\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n            # Normalize embeddings to enforce cosine similarity metric in FAISS index\n            try:\n                import faiss\n                import numpy as np\n                # Extract all vectors, normalize, and rebuild index with normalized vectors\n                xb = np.vstack([self.vector_store.index.reconstruct(i) for i in range(self.vector_store.index.ntotal)])\n                norms = np.linalg.norm(xb, axis=1, keepdims=True) + 1e-10\n                xb_norm = xb / norms\n                # Rebuild index with normalized vectors\n                self.vector_store.index = faiss.IndexFlatIP(embeddings.embedding_dimension)\n                self.vector_store.index.add(xb_norm)\n            except Exception:\n                pass\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Improved chunking: try to split on markdown headers to keep semantic coherence\n        HEADER_RE = re.compile(r\"(#{1,6} .+)\")\n        \n        # If text small enough, no splitting\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n        \n        # Split on headers if possible\n        splits = HEADER_RE.split(text)\n        # splits will alternate between non-header and header lines, e.g. ['', '# Header1', 'text1', '# Header2', 'text2', ...]\n        # We'll reconstruct sections as header + content\n        sections = []\n        i = 0\n        while i < len(splits):\n            if splits[i].startswith('#'):\n                header = splits[i].strip()\n                content = splits[i+1] if i+1 < len(splits) else ''\n                sections.append(header + \"\\n\" + content)\n                i += 2\n            else:\n                # Text before first header or in between headers\n                if splits[i].strip():\n                    sections.append(splits[i])\n                i += 1\n        \n        # Now chunk sections further if needed, with overlap and try to break at sentence boundaries when possible\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n        \n        for sec in sections:\n            if len(sec) <= size:\n                chunks_list.append(Document(page_content=sec, metadata={\"source\": source, \"type\": \"text\"}))\n            else:\n                # Use sentence-based rolling window with overlap\n                sentences = sent_tokenize(sec)\n                current_chunk = \"\"\n                current_len = 0\n                for i, sent in enumerate(sentences):\n                    sent_len = len(sent)\n                    if current_len + sent_len + 1 > size:\n                        if current_chunk:\n                            chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            # Overlap: reuse last sentences approximately corresponding to overlap chars\n                            # Prefer whole sentences instead of words for overlap to preserve semantics\n                            overlap_sentences = []\n                            overlap_char_count = 0\n                            # Collect sentences from end backward until overlap char count reached\n                            for s in reversed(sentences[:i]):\n                                overlap_sentences.insert(0, s)\n                                overlap_char_count += len(s) + 1\n                                if overlap_char_count >= overlap:\n                                    break\n                            current_chunk = \" \".join(overlap_sentences) + \" \" + sent\n                            current_len = sum(len(s) + 1 for s in overlap_sentences) + sent_len + 1\n                        else:\n                            # Single sentence longer than chunk size, just append\n                            chunks_list.append(Document(page_content=sent.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            current_chunk = \"\"\n                            current_len = 0\n                    else:\n                        current_chunk += \" \" + sent\n                        current_len += sent_len + 1\n                if current_chunk.strip():\n                    chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval: use MMR for more diverse relevant results if available\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=15)\n        except AttributeError:\n            # fallback to similarity search if MMR not supported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        \n        # Filter retrieved documents by simple keyword presence to boost relevance\n        query_terms = set(re.findall(r\"\\w+\", query_str.lower()))\n        filtered_docs = []\n        for doc in retrieved:\n            content_lower = doc.page_content.lower()\n            # Require at least 2 distinct query terms present to improve relevance filtering\n            if sum(term in content_lower for term in query_terms) >= 2:\n                filtered_docs.append(doc)\n        # Limit to top_k after filtering\n        filtered_docs = filtered_docs[:self.top_k] if filtered_docs else retrieved[:self.top_k]\n\n        context_block = \"\"\n        for i, doc in enumerate(filtered_docs):\n            src = doc.metadata.get(\"source\", \"unknown\")\n            context_block += f\"Source {i+1} ({src}):\\n{doc.page_content}\\n\\n\"\n\n        # Generation with clearer instructions on faithfulness, code priority, and uncertainty\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer strictly based on the context above. \"\n            \"Use code examples to illustrate your answer if available. \"\n            \"Do not invent information. If the answer is not contained in the context, say 'I don't know based on the provided context.' \"\n            \"Provide concise, accurate, and faithful responses.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": [doc.page_content for doc in filtered_docs]}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, NumPy-based implementation\n\n# Current Program\n```python\nimport os\nimport re\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 800               # Reduce chunk size for more focused chunks\n        self.chunk_overlap = 150            # Slightly less overlap to reduce redundancy\n        self.top_k = 7                     # Increase retrieval to get more context\n        self.temperature = 0.3             # Lower temperature for more factual answers\n        \n        # Load env\n        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))\n        self._initialize_system()\n\n    def _initialize_system(self):\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        docs = []\n        if os.path.exists(self.docs_dir):\n            for root, _, files in os.walk(self.docs_dir):\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        docs.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        # Vector Store (FAISS) with normalization for cosine similarity\n        if docs:\n            self.vector_store = FAISS.from_documents(docs, embeddings)\n            # Enforce cosine similarity metric explicitly\n            try:\n                self.vector_store.index.metric_type = 1  # 1 for cosine in FAISS\n                # Normalize all vectors in index for cosine similarity to be accurate\n                import faiss\n                import numpy as np\n                xb = np.vstack([self.vector_store.index.reconstruct(i) for i in range(self.vector_store.index.ntotal)])\n                norms = np.linalg.norm(xb, axis=1, keepdims=True)\n                xb_norm = xb / (norms + 1e-10)\n                self.vector_store.index.reset()\n                self.vector_store.index = faiss.IndexFlatIP(self.vector_store.index.d)\n                self.vector_store.index.add(xb_norm)\n            except Exception:\n                pass\n        else:\n            self.vector_store = None\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n\n    def _chunk_document(self, text: str, source: str) -> List[Document]:\n        \"\"\"\n        Ad-hoc chunking logic adapted from run_ingestion.py.\n        \"\"\"\n        # Regex patterns from run_ingestion.py\n        CODE_FENCE_RE = re.compile(r\"```([a-zA-Z0-9_+-]*)\\n(.*?)\\n```\", re.DOTALL)\n        CODE_SIGNAL_RE = re.compile(r\"\\b(def|class|import|from|package|func|public|private|return|if|for|while)\\b\")\n        \n        def _is_navigation_chunk(txt: str) -> bool:\n            return \"Skip to main content\" in txt and \"Navigation\" in txt\n            \n        def _code_density(txt: str) -> float:\n            lines = [line.strip() for line in txt.splitlines() if line.strip()]\n            if not lines: return 0.0\n            code_like = sum(1 for line in lines if CODE_SIGNAL_RE.search(line))\n            return code_like / len(lines)\n\n        chunks = []\n        cursor = 0\n        \n        # Split by code fences first\n        for match in CODE_FENCE_RE.finditer(text):\n            start, end = match.span()\n            if start > cursor:\n                pre_text = text[cursor:start]\n                if pre_text.strip():\n                    if not _is_navigation_chunk(pre_text):\n                        self._make_text_chunks(pre_text, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            lang = (match.group(1) or \"\").strip()\n            code = match.group(2)\n            \n            # Logic to keep code blocks atomic\n            if code.strip():\n                fence = f\"```{lang}\\n{code}\\n```\"\n                density = _code_density(code)\n                # If high density or explicitly fenced, treat as code\n                if density > 0.2:\n                     chunks.append(Document(\n                         page_content=fence, \n                         metadata={\"source\": source, \"type\": \"code\", \"lang\": lang, \"density\": density}\n                     ))\n                else:\n                    # Treat as text if low density code\n                    self._make_text_chunks(fence, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n            cursor = end\n            \n        tail = text[cursor:]\n        if tail.strip() and not _is_navigation_chunk(tail):\n            self._make_text_chunks(tail, source, chunks, self.chunk_size, self.chunk_overlap)\n            \n        return chunks\n\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Improved chunking: try to split on markdown headers to keep semantic coherence\n        HEADER_RE = re.compile(r\"(#{1,6} .+)\")\n        \n        # If text small enough, no splitting\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n        \n        # Split on headers if possible\n        splits = HEADER_RE.split(text)\n        # splits will alternate between non-header and header lines, e.g. ['', '# Header1', 'text1', '# Header2', 'text2', ...]\n        # We'll reconstruct sections as header + content\n        sections = []\n        i = 0\n        while i < len(splits):\n            if splits[i].startswith('#'):\n                header = splits[i].strip()\n                content = splits[i+1] if i+1 < len(splits) else ''\n                sections.append(header + \"\\n\" + content)\n                i += 2\n            else:\n                # Text before first header or in between headers\n                if splits[i].strip():\n                    sections.append(splits[i])\n                i += 1\n        \n        # Now chunk sections further if needed, with overlap and try to break at sentence boundaries when possible\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n        \n        for sec in sections:\n            if len(sec) <= size:\n                chunks_list.append(Document(page_content=sec, metadata={\"source\": source, \"type\": \"text\"}))\n            else:\n                # Use sentence-based rolling window with overlap\n                sentences = sent_tokenize(sec)\n                current_chunk = \"\"\n                current_len = 0\n                for i, sent in enumerate(sentences):\n                    sent_len = len(sent)\n                    if current_len + sent_len + 1 > size:\n                        if current_chunk:\n                            chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            # Overlap: reuse last sentences approximately corresponding to overlap chars\n                            overlap_words = current_chunk.strip().split()[-max(overlap//5, 8):]\n                            current_chunk = \" \".join(overlap_words) + \" \" + sent\n                            current_len = sum(len(w) + 1 for w in overlap_words) + sent_len + 1\n                        else:\n                            # Single sentence longer than chunk size, just append\n                            chunks_list.append(Document(page_content=sent.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            current_chunk = \"\"\n                            current_len = 0\n                    else:\n                        if current_chunk:\n                            current_chunk += \" \" + sent\n                        else:\n                            current_chunk = sent\n                        current_len += sent_len + 1\n                if current_chunk.strip():\n                    chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.vector_store:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval: use MMR for more diverse relevant results if available\n        try:\n            retrieved = self.vector_store.max_marginal_relevance_search(query_str, k=self.top_k, fetch_k=15)\n        except AttributeError:\n            # fallback to similarity search if MMR not supported\n            retrieved = self.vector_store.similarity_search(query_str, k=self.top_k)\n        \n        # Filter retrieved documents by simple keyword presence to boost relevance\n        query_terms = set(re.findall(r\"\\w+\", query_str.lower()))\n        filtered_docs = []\n        for doc in retrieved:\n            content_lower = doc.page_content.lower()\n            if any(term in content_lower for term in query_terms):\n                filtered_docs.append(doc)\n        # Limit to top_k after filtering\n        filtered_docs = filtered_docs[:self.top_k] if filtered_docs else retrieved[:self.top_k]\n\n        context_block = \"\"\n        for i, doc in enumerate(filtered_docs):\n            src = doc.metadata.get(\"source\", \"unknown\")\n            context_block += f\"Source {i+1} ({src}):\\n{doc.page_content}\\n\\n\"\n\n        # Generation with clearer instructions on faithfulness, code priority, and uncertainty\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer strictly based on the context above. \"\n            \"Use code examples to illustrate your answer if available. \"\n            \"Do not invent information. If the answer is not contained in the context, say 'I don't know based on the provided context.' \"\n            \"Provide concise, accurate, and faithful responses.\\n\"\n            \"Always cite source filenames when relevant to support your answers.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": [doc.page_content for doc in filtered_docs]}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n        # Filter retrieved documents by simple keyword presence to boost relevance\n        query_terms = set(re.findall(r\"\\w+\", query_str.lower()))\n        filtered_docs = []\n        for doc in retrieved:\n            content_lower = doc.page_content.lower()\n            if any(term in content_lower for term in query_terms):\n                filtered_docs.append(doc)\n        # Limit to top_k after filtering\n        filtered_docs = filtered_docs[:self.top_k] if filtered_docs else retrieved[:self.top_k]\n=======\n        # Filter retrieved documents by simple keyword presence to boost relevance\n        query_terms = set(re.findall(r\"\\w+\", query_str.lower()))\n        filtered_docs = []\n        for doc in retrieved:\n            content_lower = doc.page_content.lower()\n            # Require at least 2 distinct query terms present to improve relevance filtering\n            match_count = sum(1 for term in query_terms if term in content_lower)\n            if match_count >= 2:\n                filtered_docs.append(doc)\n        # Limit to top_k after filtering, fallback if none pass\n        filtered_docs = filtered_docs[:self.top_k] if filtered_docs else retrieved[:self.top_k]\n>>>>>>> REPLACE\n\nExplanation:  \nRequiring at least two distinct query terms in the retrieved documents helps filter out less relevant chunks, improving retrieval precision and answer quality.\n\n---\n\n<<<<<<< SEARCH\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Improved chunking: try to split on markdown headers to keep semantic coherence\n        HEADER_RE = re.compile(r\"(#{1,6} .+)\")\n        \n        # If text small enough, no splitting\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n        \n        # Split on headers if possible\n        splits = HEADER_RE.split(text)\n        # splits will alternate between non-header and header lines, e.g. ['', '# Header1', 'text1', '# Header2', 'text2', ...]\n        # We'll reconstruct sections as header + content\n        sections = []\n        i = 0\n        while i < len(splits):\n            if splits[i].startswith('#'):\n                header = splits[i].strip()\n                content = splits[i+1] if i+1 < len(splits) else ''\n                sections.append(header + \"\\n\" + content)\n                i += 2\n            else:\n                # Text before first header or in between headers\n                if splits[i].strip():\n                    sections.append(splits[i])\n                i += 1\n        \n        # Now chunk sections further if needed, with overlap and try to break at sentence boundaries when possible\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n        \n        for sec in sections:\n            if len(sec) <= size:\n                chunks_list.append(Document(page_content=sec, metadata={\"source\": source, \"type\": \"text\"}))\n            else:\n                # Use sentence-based rolling window with overlap\n                sentences = sent_tokenize(sec)\n                current_chunk = \"\"\n                current_len = 0\n                for i, sent in enumerate(sentences):\n                    sent_len = len(sent)\n                    if current_len + sent_len + 1 > size:\n                        if current_chunk:\n                            chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            # Overlap: reuse last sentences approximately corresponding to overlap chars\n                            overlap_words = current_chunk.strip().split()[-max(overlap//5, 8):]\n                            current_chunk = \" \".join(overlap_words) + \" \" + sent\n                            current_len = sum(len(w) + 1 for w in overlap_words) + sent_len + 1\n                        else:\n                            # Single sentence longer than chunk size, just append\n                            chunks_list.append(Document(page_content=sent.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            current_chunk = \"\"\n                            current_len = 0\n                    else:\n                        if current_chunk:\n                            current_chunk += \" \" + sent\n                        else:\n                            current_chunk = sent\n                        current_len += sent_len + 1\n                if current_chunk.strip():\n                    chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n=======\n    def _make_text_chunks(self, text: str, source: str, chunks_list: List[Document], size: int, overlap: int):\n        # Improved chunking: try to split on markdown headers to keep semantic coherence\n        HEADER_RE = re.compile(r\"(#{1,6} .+)\")\n        \n        # If text small enough, no splitting\n        if len(text) <= size:\n            chunks_list.append(Document(page_content=text, metadata={\"source\": source, \"type\": \"text\"}))\n            return\n        \n        # Split on headers if possible\n        splits = HEADER_RE.split(text)\n        # splits will alternate between non-header and header lines, e.g. ['', '# Header1', 'text1', '# Header2', 'text2', ...]\n        # We'll reconstruct sections as header + content\n        sections = []\n        i = 0\n        while i < len(splits):\n            if splits[i].startswith('#'):\n                header = splits[i].strip()\n                content = splits[i+1] if i+1 < len(splits) else ''\n                sections.append(header + \"\\n\" + content)\n                i += 2\n            else:\n                # Text before first header or in between headers\n                if splits[i].strip():\n                    sections.append(splits[i])\n                i += 1\n        \n        # Now chunk sections further if needed, with overlap and try to break at sentence boundaries when possible\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n        \n        for sec in sections:\n            if len(sec) <= size:\n                chunks_list.append(Document(page_content=sec, metadata={\"source\": source, \"type\": \"text\"}))\n            else:\n                # Use sentence-based rolling window with overlap\n                sentences = sent_tokenize(sec)\n                current_chunk = \"\"\n                current_len = 0\n                for i, sent in enumerate(sentences):\n                    sent_len = len(sent)\n                    if current_len + sent_len + 1 > size:\n                        if current_chunk:\n                            chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            # Overlap: reuse last sentences approximately corresponding to overlap chars\n                            # Prefer whole sentences instead of words for overlap to preserve semantics\n                            overlap_sentences = []\n                            overlap_char_count = 0\n                            # Collect sentences from end backward until overlap char count reached\n                            for s in reversed(sentences[:i]):\n                                overlap_sentences.insert(0, s)\n                                overlap_char_count += len(s) + 1\n                                if overlap_char_count >= overlap:\n                                    break\n                            current_chunk = \" \".join(overlap_sentences) + \" \" + sent\n                            current_len = sum(len(s) + 1 for s in overlap_sentences) + sent_len + 1\n                        else:\n                            # Single sentence longer than chunk size, just append\n                            chunks_list.append(Document(page_content=sent.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n                            current_chunk = \"\"\n                            current_len = 0\n                    else:\n                        if current_chunk:\n                            current_chunk += \" \" + sent\n                        else:\n                            current_chunk = sent\n                        current_len += sent_len + 1\n                if current_chunk.strip():\n                    chunks_list.append(Document(page_content=current_chunk.strip(), metadata={\"source\": source, \"type\": \"text\"}))\n>>>>>>> REPLACE\n\nExplanation:  \nOverlapping whole sentences rather than arbitrary words for chunk overlap preserves semantic coherence and avoids cutting sentences abruptly. This may improve retrieval contextuality and answer faithfulness.\n\n---\n\n<<<<<<< SEARCH\n        # Generation with clearer instructions on faithfulness, code priority, and uncertainty\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer strictly based on the context above. \"\n            \"Use code examples to illustrate your answer if available. \"\n            \"Do not invent information. If the answer is not contained in the context, say 'I don't know based on the provided context.' \"\n            \"Provide concise, accurate, and faithful responses.\\n\"\n            \"Always cite source filenames when relevant to support your answers.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": [doc.page_content for doc in filtered_docs]}\n=======\n        # Generation with clearer instructions on faithfulness, code priority, and uncertainty\n        prompt = (\n            f\"You are an expert assistant for Google ADK documentation.\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\"\n            \"Answer strictly based on the context above. \"\n            \"Use code examples to illustrate your answer if available. \"\n            \"Do not invent information. If the answer is not contained in the context, say 'I don't know based on the provided context.' \"\n            \"Provide concise, accurate, and faithful responses.\\n\"\n            \"Always cite source filenames when relevant to support your answers.\\n\"\n            \"If the question is ambiguous or context insufficient, respond accordingly.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        return {\"answer\": res.content.strip(), \"contexts\": [doc.page_content for doc in filtered_docs]}\n>>>>>>> REPLACE\n\nExplanation:  \nAdding explicit instruction to handle ambiguous or insufficient context questions encourages more cautious, faithful answers, reducing hallucination and improving faithfulness.\n\n---\n\nSummary of changes:  \n- Stronger keyword-based filtering requiring multiple query terms presence in retrieved docs to improve relevance precision.  \n- Overlap chunking at sentence granularity (whole sentences) for better semantic chunk continuity.  \n- Prompt update to explicitly instruct cautious answers on ambiguity or insufficient context, improving faithfulness.  \n\nThese changes are targeted, maintain structure and constraints, and likely improve combined score by improving retrieval relevance and answer faithfulness."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}