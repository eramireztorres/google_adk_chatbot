max_iterations: 100
checkpoint_interval: 5
diff_based_evolution: true
max_code_length: 30000
log_level: DEBUG


llm:
  api_base: "https://api.openai.com/v1"
  models:
    - name: "gpt-4.1-mini"
      weight: 1.0
  temperature: 0.7
  max_tokens: 16000
  timeout: 120
  retries: 3

prompt:
  system_message: |
    You are an expert RAG system optimizer. Your goal is to improve the RAG pipeline to maximize retrieval accuracy and answer quality on the Google ADK documentation.
    
    The initial program has a hybrid retrieval setup (FAISS + BM25), recursive chunking, and a FlashRank reranker. You have full control to evolve:
    
    1. **Advanced Chunking**: Modify `_chunk_document`. Experiment with:
       - `RecursiveCharacterTextSplitter` separators (e.g., adding markdown header support like `\n# `, `\n## `).
       - Context-aware splitting: Adding metadata like "header_path" to chunks to help the LLM understand hierarchy.
       - Small-to-Big retrieval: Chunking small but returning parents.
    
    2. **Retrieval & Reranking**: Modify `RAGSystem.__init__` and `query`. Experiment with:
       - **Ad-Hoc Reordering**: In `query`, use Python to sort `docs` by length, metadata, or keyword presence.
       - **Ensemble Weights**: Optimize the balance between semantic (FAISS) and keyword (BM25) search.
       
    **Verified Import Laws (USE ONLY THESE PATHS)**:
    - `from langchain_community.document_transformers import LongContextReorder`
    - `from langchain_classic.retrievers.document_compressors import EmbeddingsFilter`
    - `from langchain_text_splitters import RecursiveCharacterTextSplitter`
    
    **Forbidden Imports (DO NOT USE - CAUSE CRASHES)**:
    - `FlashrankRerank`
    - `CohereRerank`
    - `ContextualCompressionRetriever`
    - `CrossEncoder`
    
    **Performance & Architecture Laws**:
    - **No Neural Rerankers**: Do NOT use ONNX, Torch, or external reranking models. Use pure Python logic only.
    - **Variable Safety**: Always check your loop variables.
    
    3. **Generation Precision**: Modify the prompt in `query`. Experiment with:
       - Context compression: Re-summarizing retrieved chunks before passing to the final LLM call.
       - Thinking/CoT steps: Asking the LLM to analyze the context for mandatory terms before answering.
    
    **Example of lightweight ad-hoc reranking**:
    ```python
    def query(self, query):
        docs = self.retriever.invoke(query)
        # Custom logic: Prioritize "Reference" docs
        docs.sort(key=lambda d: "Reference" in d.metadata.get("source", ""), reverse=True)
    ```
    
    **Constraints**:
    - You MUST maintain the `evaluate_rag(docs_path, query)` function signature and return dictionary keys: `answer`, `contexts`.
    - You MUST keep the `RAGSystem` class structure.
  include_artifacts: true

database:
  feature_dimensions: ["complexity", "combined_score"]
  exploitation_ratio: 0.5
  population_size: 20

evaluator:
  timeout: 1200
  parallel_evaluations: 1
  cascade_evaluation: false
