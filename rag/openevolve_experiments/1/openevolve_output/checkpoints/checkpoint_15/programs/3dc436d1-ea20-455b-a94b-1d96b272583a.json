{"id": "3dc436d1-ea20-455b-a94b-1d96b272583a", "code": "import os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Add context-aware header path metadata and improved markdown splitting\n        # Use a custom splitter to add \"header_path\" metadata to each chunk for hierarchy context\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        docs = splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # Add header_path metadata by parsing markdown headers hierarchically\n        header_stack = []\n        enhanced_docs = []\n        header_pattern = re.compile(r\"^(#{1,4})\\s*(.*)\", re.MULTILINE)\n        \n        # Find all headers in the text with their positions\n        headers = [(m.start(), len(m.group(1)), m.group(2).strip()) for m in header_pattern.finditer(text)]\n        headers.append((len(text), 0, \"\"))  # Sentinel for end\n        \n        for doc in docs:\n            # Find the headers applicable to this chunk (by overlap)\n            chunk_start = text.find(doc.page_content)\n            if chunk_start == -1:\n                # fallback: no header_path\n                enhanced_docs.append(doc)\n                continue\n            \n            # Find header path for this chunk start by scanning headers in order\n            current_path = []\n            for i in range(len(headers)-1):\n                start_pos, level, header_text = headers[i]\n                next_start = headers[i+1][0]\n                if start_pos <= chunk_start < next_start:\n                    # Build header path stack up to this level\n                    for h in headers[:i+1]:\n                        # Only headers up to this level\n                        if h[1] <= level:\n                            # Remove deeper levels before adding this header\n                            while len(current_path) >= h[1]:\n                                current_path.pop()\n                            current_path.append(h[2])\n                    break\n            \n            # Create new metadata with header_path\n            new_meta = dict(doc.metadata)\n            new_meta[\"header_path\"] = \" > \".join(current_path) if current_path else \"\"\n            enhanced_docs.append(doc.__class__(page_content=doc.page_content, metadata=new_meta))\n        \n        return enhanced_docs\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Combine source filename signals with presence of query keywords in content for reranking\n        def doc_priority(d):\n            source = d.metadata.get(\"source\", \"\").lower()\n            content = d.page_content.lower()\n            score = 0\n            if \"reference\" in source:\n                score += 5\n            if \"api\" in source:\n                score += 3\n            # Boost if query terms appear in content (partial matching on words)\n            query_terms = set(query_str.lower().split())\n            content_words = set(re.findall(r\"\\w+\", content))\n            overlap = len(query_terms.intersection(content_words))\n            score += overlap * 0.7  # weight for keyword match\n            # Prefer longer chunks (likely more informative), capped\n            score += min(len(d.page_content) / 1000, 2)\n            return score\n        docs = sorted(docs, key=doc_priority, reverse=True)\n        \n        # Keep top rerank_top_n documents only to reduce noise\n        docs = docs[:self.rerank_top_n]\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({os.path.basename(src)}):\\n{content}\\n\\n\"\n\n        # Generation\n        # Add chain-of-thought style instructions with mandatory term emphasis and context compression\n        # Context compression: summarize each context chunk briefly before answering\n        compressed_contexts = []\n        for i, c in enumerate(contexts):\n            comp_prompt = (\n                f\"Summarize key points and mandatory terms in the following documentation snippet:\\n\\n{c}\\n\\nSummary:\"\n            )\n            comp_res = self.llm.invoke(comp_prompt)\n            summary = comp_res.content.strip()\n            compressed_contexts.append(f\"Snippet {i+1} Summary:\\n{summary}\\n\")\n\n        compressed_context_block = \"\\n\".join(compressed_contexts)\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Summaries of relevant documentation:\\n{compressed_context_block}\\n\\n\"\n            \"Step 1: Identify mandatory terms and concepts from these summaries relevant to the question.\\n\"\n            \"Step 2: Answer the question strictly based on the detailed context provided earlier. \"\n            \"Prioritize code examples if present. \"\n            \"Provide a thorough and faithful answer. \"\n            \"If the answer is not in the context, respond with 'I don't know'.\"\n        )\n\n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "5b86410d-326a-4253-b3de-df698cf6d31a", "generation": 2, "timestamp": 1769526347.8174474, "iteration_found": 10, "metrics": {"combined_score": 0.4353571428571429, "num_samples": 10, "raw_scores": [0.28, 0.5, 0.5650000000000001, 0.6050000000000001, 0.16714285714285715, 0.3485714285714286, 0.42000000000000004, 0.4828571428571429, 0.42000000000000004, 0.565]}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 47 lines with 38 lines\nChange 2: Replace 23 lines with 29 lines\nChange 3: Replace 17 lines with 30 lines", "parent_metrics": {"combined_score": 0.4690714285714286, "num_samples": 10, "raw_scores": [0.21500000000000002, 0.46, 0.595, 0.64, 0.2478571428571429, 0.465, 0.49, 0.5478571428571429, 0.45499999999999996, 0.575]}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer. Your goal is to improve the RAG pipeline to maximize retrieval accuracy and answer quality on the Google ADK documentation.\n\nThe initial program has a hybrid retrieval setup (FAISS + BM25), recursive chunking, and a FlashRank reranker. You have full control to evolve:\n\n1. **Advanced Chunking**: Modify `_chunk_document`. Experiment with:\n   - `RecursiveCharacterTextSplitter` separators (e.g., adding markdown header support like `\\n# `, `\\n## `).\n   - Context-aware splitting: Adding metadata like \"header_path\" to chunks to help the LLM understand hierarchy.\n   - Small-to-Big retrieval: Chunking small but returning parents.\n\n2. **Retrieval & Reranking**: Modify `RAGSystem.__init__` and `query`. Experiment with:\n   - **Ad-Hoc Reordering**: In `query`, use Python to sort `docs` by length, metadata, or keyword presence.\n   - **Ensemble Weights**: Optimize the balance between semantic (FAISS) and keyword (BM25) search.\n   \n**Verified Import Laws (USE ONLY THESE PATHS)**:\n- `from langchain_community.document_transformers import LongContextReorder`\n- `from langchain_classic.retrievers.document_compressors import EmbeddingsFilter`\n- `from langchain_text_splitters import RecursiveCharacterTextSplitter`\n\n**Forbidden Imports (DO NOT USE - CAUSE CRASHES)**:\n- `FlashrankRerank`\n- `CohereRerank`\n- `ContextualCompressionRetriever`\n- `CrossEncoder`\n\n**Performance & Architecture Laws**:\n- **No Neural Rerankers**: Do NOT use ONNX, Torch, or external reranking models. Use pure Python logic only.\n- **Variable Safety**: Always check your loop variables.\n\n3. **Generation Precision**: Modify the prompt in `query`. Experiment with:\n   - Context compression: Re-summarizing retrieved chunks before passing to the final LLM call.\n   - Thinking/CoT steps: Asking the LLM to analyze the context for mandatory terms before answering.\n\n**Example of lightweight ad-hoc reranking**:\n```python\ndef query(self, query):\n    docs = self.retriever.invoke(query)\n    # Custom logic: Prioritize \"Reference\" docs\n    docs.sort(key=lambda d: \"Reference\" in d.metadata.get(\"source\", \"\"), reverse=True)\n```\n\n**Constraints**:\n- You MUST maintain the `evaluate_rag(docs_path, query)` function signature and return dictionary keys: `answer`, `contexts`.\n- You MUST keep the `RAGSystem` class structure.\n", "user": "# Current Program Information\n- Fitness: 0.4691\n- Feature coordinates: combined_score=0.47\n- Focus areas: - Fitness unchanged at 0.4691\n- Exploring combined_score=0.47 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 12 lines with 53 lines\nChange 2: Replace 27 lines with 41 lines\n- Metrics: combined_score: 0.4691, num_samples: 10.0000, raw_scores: [np.float64(0.21500000000000002), np.float64(0.46), np.float64(0.595), np.float64(0.64), np.float64(0.2478571428571429), np.float64(0.465), np.float64(0.49), np.float64(0.5478571428571429), np.float64(0.45499999999999996), np.float64(0.575)]\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 12 lines with 46 lines\nChange 2: Replace 27 lines with 43 lines\n- Metrics: combined_score: 0.4720, num_samples: 10.0000, raw_scores: [np.float64(0.42000000000000004), np.float64(0.455), np.float64(0.56), np.float64(0.4783333333333333), np.float64(0.13214285714285712), np.float64(0.4635714285714286), np.float64(0.78), np.float64(0.5414285714285715), np.float64(0.36250000000000004), np.float64(0.5275000000000001)]\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4963, num_samples: 10.0000, raw_scores: [np.float64(0.48000000000000004), np.float64(0.4064285714285715), np.float64(0.5391666666666667), np.float64(0.5521739130434783), np.float64(0.5145833333333334), np.float64(0.5610119047619048), np.float64(0.679), np.float64(0.5444285714285715), np.float64(0.42272727272727273), np.float64(0.2630952380952381)]\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4963)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Advanced separators to honor markdown hierarchy\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        return splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Sort by length or metadata (optional placeholder logic)\n        # docs = sorted(docs, key=lambda d: len(d.page_content), reverse=True)\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({os.path.basename(src)}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"Include as much relevant detail as possible while staying faithful. \"\n            \"If the answer is not in the context, say you don't know.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.4963), Performs well on num_samples (10.0000), Performs well on raw_scores ([np.float64(0.48000000000000004), np.float64(0.4064285714285715), np.float64(0.5391666666666667), np.float64(0.5521739130434783), np.float64(0.5145833333333334), np.float64(0.5610119047619048), np.float64(0.679), np.float64(0.5444285714285715), np.float64(0.42272727272727273), np.float64(0.2630952380952381)])\n\n### Program 2 (Score: 0.4720)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Add \"header_path\" metadata to preserve markdown hierarchy context\n        # We do a first pass split by headers, then chunk each section smaller.\n        # This helps the LLM understand the chunk's location in the doc.\n\n        # Split by top level headers to get sections with header paths\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,  # Larger chunk to isolate header sections\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_docs = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Now chunk each header section into smaller chunks with overlap\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for hd in header_docs:\n            # Extract the header path from the start of hd.page_content lines\n            lines = hd.page_content.splitlines()\n            header_path = \"\"\n            content_start_idx = 0\n            for i, line in enumerate(lines):\n                if line.strip().startswith(\"#\"):\n                    header_path = line.strip()\n                    content_start_idx = i + 1\n                    break\n            content = \"\\n\".join(lines[content_start_idx:])\n            if not content.strip():\n                # If no content after header, fallback to whole content\n                content = hd.page_content\n\n            chunks = small_splitter.create_documents(\n                [content],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Boost chunks with \"Reference\" or \"Tutorial\" in source metadata\n        def rerank_key(doc):\n            src = doc.metadata.get(\"source\", \"\").lower()\n            score = 0\n            if \"reference\" in src:\n                score += 2\n            if \"tutorial\" in src:\n                score += 1\n            # Prefer longer chunks moderately\n            score += min(len(doc.page_content) / 1000, 1)\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n\n        # Limit contexts to rerank_top_n for generation prompt to reduce noise\n        top_docs = docs[:self.rerank_top_n]\n\n        contexts = [d.page_content for d in top_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in top_docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = top_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Generation with chain-of-thought style instruction and mandatory term check\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Before answering, analyze the context carefully for any mandatory terms or definitions related to the question. \"\n            \"Then provide a detailed answer strictly based on this context. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"If the answer is not found in the context, respond with 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.4720), Performs well on num_samples (10.0000), Performs well on raw_scores ([np.float64(0.42000000000000004), np.float64(0.455), np.float64(0.56), np.float64(0.4783333333333333), np.float64(0.13214285714285712), np.float64(0.4635714285714286), np.float64(0.78), np.float64(0.5414285714285715), np.float64(0.36250000000000004), np.float64(0.5275000000000001)])\n\n### Program 3 (Score: 0.4691)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Add context-aware header path metadata and improved markdown splitting\n        # Use a custom splitter to add \"header_path\" metadata to each chunk for hierarchy context\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        docs = splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # Add header_path metadata by parsing markdown headers hierarchically\n        header_stack = []\n        enhanced_docs = []\n        header_pattern = re.compile(r\"^(#{1,4})\\s*(.*)\", re.MULTILINE)\n        \n        # Find all headers in the text with their positions\n        headers = [(m.start(), len(m.group(1)), m.group(2).strip()) for m in header_pattern.finditer(text)]\n        headers.append((len(text), 0, \"\"))  # Sentinel for end\n        \n        for doc in docs:\n            # Find the headers applicable to this chunk (by overlap)\n            chunk_start = text.find(doc.page_content)\n            if chunk_start == -1:\n                # fallback: no header_path\n                enhanced_docs.append(doc)\n                continue\n            \n            # Find header path for this chunk start by scanning headers in order\n            current_path = []\n            for i in range(len(headers)-1):\n                start_pos, level, header_text = headers[i]\n                next_start = headers[i+1][0]\n                if start_pos <= chunk_start < next_start:\n                    # Build header path stack up to this level\n                    for h in headers[:i+1]:\n                        # Only headers up to this level\n                        if h[1] <= level:\n                            # Remove deeper levels before adding this header\n                            while len(current_path) >= h[1]:\n                                current_path.pop()\n                            current_path.append(h[2])\n                    break\n            \n            # Create new metadata with header_path\n            new_meta = dict(doc.metadata)\n            new_meta[\"header_path\"] = \" > \".join(current_path) if current_path else \"\"\n            enhanced_docs.append(doc.__class__(page_content=doc.page_content, metadata=new_meta))\n        \n        return enhanced_docs\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Sort docs prioritizing chunks from files named \"reference\" or containing \"api\"\n        def doc_priority(d):\n            source = d.metadata.get(\"source\", \"\").lower()\n            score = 0\n            if \"reference\" in source:\n                score += 5\n            if \"api\" in source:\n                score += 3\n            # Prefer longer chunks (likely more informative)\n            score += min(len(d.page_content) / 1000, 2)\n            return score\n        docs = sorted(docs, key=doc_priority, reverse=True)\n        \n        # Only keep top rerank_top_n documents for generation to reduce noise\n        docs = docs[:self.rerank_top_n]\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({os.path.basename(src)}):\\n{content}\\n\\n\"\n\n        # Generation\n        # Add a step to ask LLM to first identify mandatory terms/concepts before answering\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"First, list key terms and concepts in the context relevant to the question. Then, answer based strictly on the context. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"Include as much relevant detail as possible while staying faithful. \"\n            \"If the answer is not in the context, say you don't know.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.4691), Performs well on num_samples (10.0000), Performs well on raw_scores ([np.float64(0.21500000000000002), np.float64(0.46), np.float64(0.595), np.float64(0.64), np.float64(0.2478571428571429), np.float64(0.465), np.float64(0.49), np.float64(0.5478571428571429), np.float64(0.45499999999999996), np.float64(0.575)])\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4195)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Add header_path metadata to help LLM understand document hierarchy\n        # Use RecursiveCharacterTextSplitter with markdown header separators and preserve hierarchy\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        # Create initial chunks\n        chunks = splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # Post-process chunks to add header_path metadata by scanning markdown headers\n        header_re = re.compile(r'^(#{1,4})\\s+(.*)', re.MULTILINE)\n        headers = []\n        for match in header_re.finditer(text):\n            level = len(match.group(1))\n            title = match.group(2).strip()\n            start = match.start()\n            headers.append((start, level, title))\n        headers.append((len(text), 0, \"\"))  # sentinel for end\n        \n        def find_header_path(pos):\n            path = []\n            for i in range(len(headers)-1):\n                if headers[i][0] <= pos < headers[i+1][0]:\n                    # Collect all headers up to i with decreasing levels\n                    # Build path by scanning backwards\n                    for j in range(i, -1, -1):\n                        if headers[j][1] > 0:\n                            # Only include headers with level > 0\n                            if not path or headers[j][1] < path[-1][0]:\n                                path.append((headers[j][1], headers[j][2]))\n                    break\n            # Sort path by level ascending to get proper hierarchy\n            path = sorted(path, key=lambda x: x[0])\n            return \" > \".join([p[1] for p in path])\n        \n        # Add header_path metadata to chunks\n        for chunk in chunks:\n            start_pos = text.find(chunk.page_content[:30])  # heuristic: find chunk start\n            if start_pos < 0:\n                start_pos = 0\n            chunk.metadata[\"header_path\"] = find_header_path(start_pos)\n        \n        return chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING ---\n        # Prioritize chunks with header_path containing keywords like \"Reference\" or \"API\"\n        def rerank_key(d):\n            header_path = d.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in header_path:\n                score += 2\n            if \"api\" in header_path:\n                score += 1.5\n            # Also prioritize longer chunks (more informative)\n            score += len(d.page_content) / 1000.0\n            return score\n        docs = sorted(docs, key=rerank_key, reverse=True)\n        \n        # Limit to rerank_top_n best docs for generation context to reduce noise\n        limited_docs = docs[:self.rerank_top_n]\n        \n        contexts = [d.page_content for d in limited_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in limited_docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = limited_docs[i].metadata.get(\"header_path\", \"unknown\")\n            context_block += f\"Source {i+1} ({os.path.basename(src)}) [{header_path}]:\\n{content}\\n\\n\"\n\n        # Generation prompt with explicit CoT and mandatory terms check\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Step 1: Identify if the context mentions any mandatory terms or key concepts related to the question.\\n\"\n            \"Step 2: Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"Include as much relevant detail as possible while staying faithful.\\n\"\n            \"If the answer is not in the context, say you don't know.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n### Program D2 (Score: 0.4526)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Add \"header_path\" metadata to preserve markdown hierarchy context\n        # We chunk small but add parent context by injecting header_path in metadata\n        # This helps retrieval and reranking downstream\n        \n        # Recursive splitter with markdown headings as separators\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        chunks = splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # Post-process chunks to add header_path metadata\n        header_path_re = re.compile(r\"^(#+) (.*)\", re.MULTILINE)\n        headers = []\n        for match in header_path_re.finditer(text):\n            level = len(match.group(1))\n            title = match.group(2).strip()\n            headers.append((match.start(), level, title))\n        \n        def find_header_path(pos):\n            # Find headers before pos to build path\n            path = []\n            for i, (start, level, title) in enumerate(headers):\n                if start > pos:\n                    break\n                # Remove lower level headers if current level <= last\n                while path and path[-1][0] >= level:\n                    path.pop()\n                path.append((level, title))\n            return \" > \".join([t for _, t in path])\n        \n        # Add header_path metadata\n        for doc in chunks:\n            pos = text.find(doc.page_content[:30])  # approximate position\n            if pos == -1:\n                pos = 0\n            doc.metadata[\"header_path\"] = find_header_path(pos)\n        \n        return chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Prioritize docs with 'reference' or 'api' in header_path or source metadata\n        def score_doc(d):\n            score = 0\n            header_path = d.metadata.get(\"header_path\", \"\").lower()\n            source = d.metadata.get(\"source\", \"\").lower()\n            content = d.page_content.lower()\n            if \"reference\" in header_path or \"reference\" in source:\n                score += 3\n            if \"api\" in header_path or \"api\" in source:\n                score += 2\n            # Prioritize longer docs (more context)\n            score += len(d.page_content) / 1000.0\n            # Bonus if query keywords appear in doc content\n            for kw in query_str.lower().split():\n                if kw in content:\n                    score += 0.5\n            return score\n        \n        docs = sorted(docs, key=score_doc, reverse=True)\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({os.path.basename(src)}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"First, analyze the context to identify any mandatory terms or concepts that must appear in the answer. \"\n            \"Then, provide a detailed and precise answer strictly based on the context. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"If the answer is not found in the context, respond with 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4963, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Advanced separators to honor markdown hierarchy\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        return splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Sort by length or metadata (optional placeholder logic)\n        # docs = sorted(docs, key=lambda d: len(d.page_content), reverse=True)\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({os.path.basename(src)}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Answer the question based strictly on the context provided. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"Include as much relevant detail as possible while staying faithful. \"\n            \"If the answer is not in the context, say you don't know.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Mixed iteration strategies\n\n### Inspiration 2 (Score: 0.4720, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Add \"header_path\" metadata to preserve markdown hierarchy context\n        # We do a first pass split by headers, then chunk each section smaller.\n        # This helps the LLM understand the chunk's location in the doc.\n\n        # Split by top level headers to get sections with header paths\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,  # Larger chunk to isolate header sections\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_docs = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Now chunk each header section into smaller chunks with overlap\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for hd in header_docs:\n            # Extract the header path from the start of hd.page_content lines\n            lines = hd.page_content.splitlines()\n            header_path = \"\"\n            content_start_idx = 0\n            for i, line in enumerate(lines):\n                if line.strip().startswith(\"#\"):\n                    header_path = line.strip()\n                    content_start_idx = i + 1\n                    break\n            content = \"\\n\".join(lines[content_start_idx:])\n            if not content.strip():\n                # If no content after header, fallback to whole content\n                content = hd.page_content\n\n            chunks = small_splitter.create_documents(\n                [content],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Boost chunks with \"Reference\" or \"Tutorial\" in source metadata\n        def rerank_key(doc):\n            src = doc.metadata.get(\"source\", \"\").lower()\n            score = 0\n            if \"reference\" in src:\n                score += 2\n            if \"tutorial\" in src:\n                score += 1\n            # Prefer longer chunks moderately\n            score += min(len(doc.page_content) / 1000, 1)\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n\n        # Limit contexts to rerank_top_n for generation prompt to reduce noise\n        top_docs = docs[:self.rerank_top_n]\n\n        contexts = [d.page_content for d in top_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in top_docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = top_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Generation with chain-of-thought style instruction and mandatory term check\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Before answering, analyze the context carefully for any mandatory terms or definitions related to the question. \"\n            \"Then provide a detailed answer strictly based on this context. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"If the answer is not found in the context, respond with 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], Object-oriented approach\n\n### Inspiration 3 (Score: 0.4526, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Add \"header_path\" metadata to preserve markdown hierarchy context\n        # We chunk small but add parent context by injecting header_path in metadata\n        # This helps retrieval and reranking downstream\n        \n        # Recursive splitter with markdown headings as separators\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        chunks = splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # Post-process chunks to add header_path metadata\n        header_path_re = re.compile(r\"^(#+) (.*)\", re.MULTILINE)\n        headers = []\n        for match in header_path_re.finditer(text):\n            level = len(match.group(1))\n            title = match.group(2).strip()\n            headers.append((match.start(), level, title))\n        \n        def find_header_path(pos):\n            # Find headers before pos to build path\n            path = []\n            for i, (start, level, title) in enumerate(headers):\n                if start > pos:\n                    break\n                # Remove lower level headers if current level <= last\n                while path and path[-1][0] >= level:\n                    path.pop()\n                path.append((level, title))\n            return \" > \".join([t for _, t in path])\n        \n        # Add header_path metadata\n        for doc in chunks:\n            pos = text.find(doc.page_content[:30])  # approximate position\n            if pos == -1:\n                pos = 0\n            doc.metadata[\"header_path\"] = find_header_path(pos)\n        \n        return chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Prioritize docs with 'reference' or 'api' in header_path or source metadata\n        def score_doc(d):\n            score = 0\n            header_path = d.metadata.get(\"header_path\", \"\").lower()\n            source = d.metadata.get(\"source\", \"\").lower()\n            content = d.page_content.lower()\n            if \"reference\" in header_path or \"reference\" in source:\n                score += 3\n            if \"api\" in header_path or \"api\" in source:\n                score += 2\n            # Prioritize longer docs (more context)\n            score += len(d.page_content) / 1000.0\n            # Bonus if query keywords appear in doc content\n            for kw in query_str.lower().split():\n                if kw in content:\n                    score += 0.5\n            return score\n        \n        docs = sorted(docs, key=score_doc, reverse=True)\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({os.path.basename(src)}):\\n{content}\\n\\n\"\n\n        # Generation\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"First, analyze the context to identify any mandatory terms or concepts that must appear in the answer. \"\n            \"Then, provide a detailed and precise answer strictly based on the context. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"If the answer is not found in the context, respond with 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Mixed iteration strategies\n\n# Current Program\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Add context-aware header path metadata and improved markdown splitting\n        # Use a custom splitter to add \"header_path\" metadata to each chunk for hierarchy context\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        docs = splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # Add header_path metadata by parsing markdown headers hierarchically\n        header_stack = []\n        enhanced_docs = []\n        header_pattern = re.compile(r\"^(#{1,4})\\s*(.*)\", re.MULTILINE)\n        \n        # Find all headers in the text with their positions\n        headers = [(m.start(), len(m.group(1)), m.group(2).strip()) for m in header_pattern.finditer(text)]\n        headers.append((len(text), 0, \"\"))  # Sentinel for end\n        \n        for doc in docs:\n            # Find the headers applicable to this chunk (by overlap)\n            chunk_start = text.find(doc.page_content)\n            if chunk_start == -1:\n                # fallback: no header_path\n                enhanced_docs.append(doc)\n                continue\n            \n            # Find header path for this chunk start by scanning headers in order\n            current_path = []\n            for i in range(len(headers)-1):\n                start_pos, level, header_text = headers[i]\n                next_start = headers[i+1][0]\n                if start_pos <= chunk_start < next_start:\n                    # Build header path stack up to this level\n                    for h in headers[:i+1]:\n                        # Only headers up to this level\n                        if h[1] <= level:\n                            # Remove deeper levels before adding this header\n                            while len(current_path) >= h[1]:\n                                current_path.pop()\n                            current_path.append(h[2])\n                    break\n            \n            # Create new metadata with header_path\n            new_meta = dict(doc.metadata)\n            new_meta[\"header_path\"] = \" > \".join(current_path) if current_path else \"\"\n            enhanced_docs.append(doc.__class__(page_content=doc.page_content, metadata=new_meta))\n        \n        return enhanced_docs\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Sort docs prioritizing chunks from files named \"reference\" or containing \"api\"\n        def doc_priority(d):\n            source = d.metadata.get(\"source\", \"\").lower()\n            score = 0\n            if \"reference\" in source:\n                score += 5\n            if \"api\" in source:\n                score += 3\n            # Prefer longer chunks (likely more informative)\n            score += min(len(d.page_content) / 1000, 2)\n            return score\n        docs = sorted(docs, key=doc_priority, reverse=True)\n        \n        # Only keep top rerank_top_n documents for generation to reduce noise\n        docs = docs[:self.rerank_top_n]\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({os.path.basename(src)}):\\n{content}\\n\\n\"\n\n        # Generation\n        # Add a step to ask LLM to first identify mandatory terms/concepts before answering\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"First, list key terms and concepts in the context relevant to the question. Then, answer based strictly on the context. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"Include as much relevant detail as possible while staying faithful. \"\n            \"If the answer is not in the context, say you don't know.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n        # Advanced separators to honor markdown hierarchy\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        docs = splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # Add header_path metadata by parsing markdown headers hierarchically\n        header_stack = []\n        enhanced_docs = []\n        header_pattern = re.compile(r\"^(#{1,4})\\s*(.*)\", re.MULTILINE)\n        \n        # Find all headers in the text with their positions\n        headers = [(m.start(), len(m.group(1)), m.group(2).strip()) for m in header_pattern.finditer(text)]\n        headers.append((len(text), 0, \"\"))  # Sentinel for end\n        \n        for doc in docs:\n            # Find the headers applicable to this chunk (by overlap)\n            chunk_start = text.find(doc.page_content)\n            if chunk_start == -1:\n                # fallback: no header_path\n                enhanced_docs.append(doc)\n                continue\n            \n            # Find header path for this chunk start by scanning headers in order\n            current_path = []\n            for i in range(len(headers)-1):\n                start_pos, level, header_text = headers[i]\n                next_start = headers[i+1][0]\n                if start_pos <= chunk_start < next_start:\n                    # Build header path stack up to this level\n                    for h in headers[:i+1]:\n                        # Only headers up to this level\n                        if h[1] <= level:\n                            # Remove deeper levels before adding this header\n                            while len(current_path) >= h[1]:\n                                current_path.pop()\n                            current_path.append(h[2])\n                    break\n            \n            # Create new metadata with header_path\n            new_meta = dict(doc.metadata)\n            new_meta[\"header_path\"] = \" > \".join(current_path) if current_path else \"\"\n            enhanced_docs.append(doc.__class__(page_content=doc.page_content, metadata=new_meta))\n        \n        return enhanced_docs\n=======\n        # Advanced separators to honor markdown hierarchy and support recursive chunking with parent context\n        from langchain_core.documents import Document\n\n        # Step 1: Split by top-level headers to isolate sections for context-aware chunking\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_docs = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Step 2: Further chunk each section into smaller chunks with overlap\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for hd in header_docs:\n            lines = hd.page_content.splitlines()\n            header_path = \"\"\n            content_start_idx = 0\n            for i, line in enumerate(lines):\n                if line.strip().startswith(\"#\"):\n                    header_path = line.strip()\n                    content_start_idx = i + 1\n                    break\n            content = \"\\n\".join(lines[content_start_idx:]).strip()\n            if not content:\n                content = hd.page_content\n            chunks = small_splitter.create_documents(\n                [content],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n\n        return all_chunks\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Sort docs prioritizing chunks from files named \"reference\" or containing \"api\"\n        def doc_priority(d):\n            source = d.metadata.get(\"source\", \"\").lower()\n            score = 0\n            if \"reference\" in source:\n                score += 5\n            if \"api\" in source:\n                score += 3\n            # Prefer longer chunks (likely more informative)\n            score += min(len(d.page_content) / 1000, 2)\n            return score\n        docs = sorted(docs, key=doc_priority, reverse=True)\n        \n        # Only keep top rerank_top_n documents for generation to reduce noise\n        docs = docs[:self.rerank_top_n]\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({os.path.basename(src)}):\\n{content}\\n\\n\"\n=======\n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Combine source filename signals with presence of query keywords in content for reranking\n        def doc_priority(d):\n            source = d.metadata.get(\"source\", \"\").lower()\n            content = d.page_content.lower()\n            score = 0\n            if \"reference\" in source:\n                score += 5\n            if \"api\" in source:\n                score += 3\n            # Boost if query terms appear in content (partial matching on words)\n            query_terms = set(query_str.lower().split())\n            content_words = set(re.findall(r\"\\w+\", content))\n            overlap = len(query_terms.intersection(content_words))\n            score += overlap * 0.7  # weight for keyword match\n            # Prefer longer chunks (likely more informative), capped\n            score += min(len(d.page_content) / 1000, 2)\n            return score\n        docs = sorted(docs, key=doc_priority, reverse=True)\n        \n        # Keep top rerank_top_n documents only to reduce noise\n        docs = docs[:self.rerank_top_n]\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            context_block += f\"Source {i+1} ({os.path.basename(src)}):\\n{content}\\n\\n\"\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Generation\n        # Add a step to ask LLM to first identify mandatory terms/concepts before answering\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"First, list key terms and concepts in the context relevant to the question. Then, answer based strictly on the context. \"\n            \"If the context contains code examples, prioritize them in your answer. \"\n            \"Include as much relevant detail as possible while staying faithful. \"\n            \"If the answer is not in the context, say you don't know.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n=======\n        # Generation\n        # Add chain-of-thought style instructions with mandatory term emphasis and context compression\n        # Context compression: summarize each context chunk briefly before answering\n        compressed_contexts = []\n        for i, c in enumerate(contexts):\n            comp_prompt = (\n                f\"Summarize key points and mandatory terms in the following documentation snippet:\\n\\n{c}\\n\\nSummary:\"\n            )\n            comp_res = self.llm.invoke(comp_prompt)\n            summary = comp_res.content.strip()\n            compressed_contexts.append(f\"Snippet {i+1} Summary:\\n{summary}\\n\")\n\n        compressed_context_block = \"\\n\".join(compressed_contexts)\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Summaries of relevant documentation:\\n{compressed_context_block}\\n\\n\"\n            \"Step 1: Identify mandatory terms and concepts from these summaries relevant to the question.\\n\"\n            \"Step 2: Answer the question strictly based on the detailed context provided earlier. \"\n            \"Prioritize code examples if present. \"\n            \"Provide a thorough and faithful answer. \"\n            \"If the answer is not in the context, respond with 'I don't know'.\"\n        )\n\n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}