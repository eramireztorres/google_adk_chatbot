{"id": "56be2b04-b4df-48a8-9815-5784f6edaf75", "code": "import os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n\n        # Extended recursive splitter with markdown headers and blank lines for better section isolation\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n##### \"]\n        )\n        header_sections = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Smaller chunks with overlap and enriched header_path metadata for context-aware retrieval\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for section_doc in header_sections:\n            lines = section_doc.page_content.splitlines()\n            header_lines = []\n            content_start = 0\n            # Accumulate multiple consecutive markdown headers (levels 1-5) for richer hierarchy path\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    # Limit header level to max 5 hashes\n                    hashes = len(stripped) - len(stripped.lstrip('#'))\n                    if hashes <= 5:\n                        header_lines.append(stripped)\n                        content_start = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content_text = \"\\n\".join(lines[content_start:]) if content_start < len(lines) else section_doc.page_content\n\n            # Chunk smaller with header_path metadata\n            chunks = small_splitter.create_documents(\n                [content_text],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n\n            # Include parent large chunk as bigger context chunk with same header_path for small-to-big retrieval\n            parent_doc = Document(page_content=section_doc.page_content, metadata={\"source\": source, \"header_path\": header_path})\n            all_chunks.append(parent_doc)\n\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING ---\n        # Rerank retrieved docs boosting header_path with \"Reference\" or \"API\"\n        # Boost based on presence of query keywords and length\n        def rerank_key(doc):\n            header_path = doc.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in header_path:\n                score += 2.5\n            if \"api\" in header_path:\n                score += 1.7\n            if \"tutorial\" in header_path:\n                score += 0.7\n            # Boost longer chunks moderately (up to 1.2 points)\n            score += min(len(doc.page_content) / 1000.0, 1.2)\n\n            # Boost if query keywords appear in doc content (weight 0.8 per unique keyword)\n            query_words = set(query_str.lower().split())\n            content_words = set(re.findall(r'\\w+', doc.page_content.lower()))\n            common_words = query_words.intersection(content_words)\n            score += 0.8 * len(common_words)\n\n            # Penalty to reduce redundancy: penalize if chunk content is very similar to already selected chunks\n            # Lightweight heuristic: substring check of first 50 chars\n            limited_contexts = [d.page_content for d in docs[:self.rerank_top_n]]\n            snippet = doc.page_content[:50]\n            if any(snippet in c for c in limited_contexts if c != doc.page_content):\n                score -= 1.1\n\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n\n        limited_docs = docs[:self.rerank_top_n]\n\n        contexts = [d.page_content for d in limited_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in limited_docs]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = limited_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Context compression: summarize retrieved chunks focusing on question relevance\n        summary_prompt = (\n            f\"Summarize the following context chunks concisely focusing on information relevant to the question: {query_str}\\n\\n\"\n        )\n        summary_input = \"\\n---\\n\".join(contexts[:self.rerank_top_n])\n        summary_prompt += summary_input + \"\\n\\nSummary:\"\n\n        summary_res = self.llm.invoke(summary_prompt)\n        compressed_context = summary_res.content\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context Summary:\\n{compressed_context}\\n\\n\"\n            \"Before answering, analyze the context carefully for any mandatory terms or definitions related to the question. \"\n            \"Then provide a detailed answer strictly based on this summary. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"If the answer is not found in the context, respond with 'I don't know'.\"\n        )\n\n        # Use deterministic temperature for final answer\n        res = self.llm.invoke(prompt)\n\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "b2a771b9-f046-4077-818f-284d20a8dfe6", "generation": 5, "timestamp": 1769541962.986464, "iteration_found": 69, "metrics": {"combined_score": 0.5064285714285715, "num_samples": 10, "raw_scores": [0.27, 0.5800000000000001, 0.6000000000000001, 0.645, 0.5750000000000001, 0.707857142857143, 0.375, 0.32642857142857146, 0.42000000000000004, 0.565]}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 52 lines with 52 lines\nChange 2: Replace 34 lines with 31 lines\nChange 3: Replace 22 lines with 31 lines", "parent_metrics": {"combined_score": 0.5686190476190477, "num_samples": 10, "raw_scores": [0.28, 0.74, 0.595, 0.64, 0.5700000000000001, 0.5714285714285715, 0.51, 0.5764285714285715, 0.6383333333333333, 0.565]}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer. Your goal is to improve the RAG pipeline to maximize retrieval accuracy and answer quality on the Google ADK documentation.\n\nThe initial program has a hybrid retrieval setup (FAISS + BM25), recursive chunking, and a FlashRank reranker. You have full control to evolve:\n\n1. **Advanced Chunking**: Modify `_chunk_document`. Experiment with:\n   - `RecursiveCharacterTextSplitter` separators (e.g., adding markdown header support like `\\n# `, `\\n## `).\n   - Context-aware splitting: Adding metadata like \"header_path\" to chunks to help the LLM understand hierarchy.\n   - Small-to-Big retrieval: Chunking small but returning parents.\n\n2. **Retrieval & Reranking**: Modify `RAGSystem.__init__` and `query`. Experiment with:\n   - **Ad-Hoc Reordering**: In `query`, use Python to sort `docs` by length, metadata, or keyword presence.\n   - **Ensemble Weights**: Optimize the balance between semantic (FAISS) and keyword (BM25) search.\n   \n**Verified Import Laws (USE ONLY THESE PATHS)**:\n- `from langchain_community.document_transformers import LongContextReorder`\n- `from langchain_classic.retrievers.document_compressors import EmbeddingsFilter`\n- `from langchain_text_splitters import RecursiveCharacterTextSplitter`\n\n**Forbidden Imports (DO NOT USE - CAUSE CRASHES)**:\n- `FlashrankRerank`\n- `CohereRerank`\n- `ContextualCompressionRetriever`\n- `CrossEncoder`\n\n**Performance & Architecture Laws**:\n- **No Neural Rerankers**: Do NOT use ONNX, Torch, or external reranking models. Use pure Python logic only.\n- **Variable Safety**: Always check your loop variables.\n\n3. **Generation Precision**: Modify the prompt in `query`. Experiment with:\n   - Context compression: Re-summarizing retrieved chunks before passing to the final LLM call.\n   - Thinking/CoT steps: Asking the LLM to analyze the context for mandatory terms before answering.\n\n**Example of lightweight ad-hoc reranking**:\n```python\ndef query(self, query):\n    docs = self.retriever.invoke(query)\n    # Custom logic: Prioritize \"Reference\" docs\n    docs.sort(key=lambda d: \"Reference\" in d.metadata.get(\"source\", \"\"), reverse=True)\n```\n\n**Constraints**:\n- You MUST maintain the `evaluate_rag(docs_path, query)` function signature and return dictionary keys: `answer`, `contexts`.\n- You MUST keep the `RAGSystem` class structure.\n", "user": "# Current Program Information\n- Fitness: 0.5686\n- Feature coordinates: combined_score=0.57\n- Focus areas: - Fitness improved: 0.5327 \u2192 0.5686\n- Exploring combined_score=0.57 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 53 lines with 51 lines\nChange 2: Replace 41 lines with 56 lines\n- Metrics: combined_score: 0.5327, num_samples: 10.0000, raw_scores: [np.float64(0.35666666666666663), np.float64(0.595), np.float64(0.475), np.float64(0.6000000000000001), np.float64(0.5650000000000001), np.float64(0.39714285714285713), np.float64(0.5650000000000001), np.float64(0.5664285714285715), np.float64(0.6416666666666667), np.float64(0.565)]\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 12 lines with 43 lines\nChange 2: Replace 27 lines with 44 lines\n- Metrics: combined_score: 0.5342, num_samples: 10.0000, raw_scores: [np.float64(0.48500000000000004), np.float64(0.48), np.float64(0.645), np.float64(0.655), np.float64(0.4700000000000001), np.float64(0.6578571428571429), np.float64(0.77), np.float64(0.4492857142857143), np.float64(0.45999999999999996), np.float64(0.27)]\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 32 lines with 38 lines\nChange 2: Replace 54 lines with 62 lines\nChange 3: Replace 16 lines with 16 lines\n- Metrics: combined_score: 0.5686, num_samples: 10.0000, raw_scores: [np.float64(0.28), np.float64(0.74), np.float64(0.595), np.float64(0.64), np.float64(0.5700000000000001), np.float64(0.5714285714285715), np.float64(0.51), np.float64(0.5764285714285715), np.float64(0.6383333333333333), np.float64(0.565)]\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.5686)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n\n        # Two-stage chunking with richer markdown header hierarchy preservation\n        # First split by top-level headers (larger chunks) to isolate sections\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_sections = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Then chunk each section into smaller chunks with overlap and add \"header_path\" metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for section_doc in header_sections:\n            lines = section_doc.page_content.splitlines()\n            header_lines = []\n            content_start = 0\n            # Collect all consecutive header lines at the start for richer header path (supports nested headers)\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    header_lines.append(stripped)\n                    content_start = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content_text = \"\\n\".join(lines[content_start:]) if content_start < len(lines) else section_doc.page_content\n\n            # Chunk smaller with header_path metadata for context-aware retrieval\n            chunks = small_splitter.create_documents(\n                [content_text],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n\n            # Also include the parent large chunk (the whole section_doc) as a bigger context chunk\n            # with the same header_path for small-to-big retrieval benefit\n            from langchain_core.documents import Document\n            parent_doc = Document(page_content=section_doc.page_content, metadata={\"source\": source, \"header_path\": header_path})\n            all_chunks.append(parent_doc)\n\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING ---\n        # Rerank retrieved docs by boosting those with header_path containing \"Reference\" or \"API\"\n        # Also boost docs containing exact query keywords for relevance\n        def rerank_key(doc):\n            header_path = doc.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in header_path:\n                score += 2\n            if \"api\" in header_path:\n                score += 1.5\n            # Boost longer chunks moderately (up to 1 point)\n            score += min(len(doc.page_content) / 1200.0, 1.0)\n\n            # Boost if query keywords appear in doc content (weight 0.75 per unique keyword)\n            query_words = set(query_str.lower().split())\n            content_words = set(re.findall(r'\\w+', doc.page_content.lower()))\n            common_words = query_words.intersection(content_words)\n            score += 0.75 * len(common_words)\n\n            # Slight boost if chunk header_path contains \"Tutorial\" to diversify knowledge\n            if \"tutorial\" in header_path:\n                score += 0.5\n\n            # Penalty to reduce redundancy: penalize if chunk content is very similar to already selected chunks\n            # This is a lightweight heuristic to diversify context\n            # Use simple substring check of first 50 chars\n            limited_contexts = [d.page_content for d in docs[:self.rerank_top_n]]\n            snippet = doc.page_content[:50]\n            if any(snippet in c for c in limited_contexts if c != doc.page_content):\n                score -= 1.0\n\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n\n        limited_docs = docs[:self.rerank_top_n]\n\n        contexts = [d.page_content for d in limited_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in limited_docs]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = limited_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Step 1: Identify any mandatory terms, definitions, or key concepts related to the question in the context.\\n\"\n            \"Step 2: Based strictly on the context, provide a detailed answer. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"Include relevant detail while staying faithful. \"\n            \"If the answer is not found, say 'I don't know'.\"\n        )\n\n        # Add explicit temperature override for deterministic output\n        res = self.llm.invoke(prompt)\n\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5686), Performs well on num_samples (10.0000), Performs well on raw_scores ([np.float64(0.28), np.float64(0.74), np.float64(0.595), np.float64(0.64), np.float64(0.5700000000000001), np.float64(0.5714285714285715), np.float64(0.51), np.float64(0.5764285714285715), np.float64(0.6383333333333333), np.float64(0.565)])\n\n### Program 2 (Score: 0.5342)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # 2-stage chunking with header path metadata for context-aware splitting\n        # First, split by top-level markdown headers (to isolate sections)\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 3,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_sections = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # Then chunk each section into smaller chunks with overlap and add header_path metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        all_chunks = []\n        for section_doc in header_sections:\n            # Extract header path from the start lines of the section content\n            lines = section_doc.page_content.splitlines()\n            header_path = \"\"\n            for line in lines:\n                if line.strip().startswith(\"#\"):\n                    header_path = line.strip()\n                    break\n            content_start = 0\n            if header_path:\n                try:\n                    content_start = lines.index(header_path) + 1\n                except Exception:\n                    content_start = 0\n            content_text = \"\\n\".join(lines[content_start:]) if content_start < len(lines) else section_doc.page_content\n\n            # Chunk the content smaller with header_path metadata\n            chunks = small_splitter.create_documents([content_text], metadatas=[{\"source\": source, \"header_path\": header_path}])\n            all_chunks.extend(chunks)\n        \n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING ---\n        # Rerank retrieved docs by boosting those with header_path containing \"Reference\" or \"API\"\n        def rerank_key(doc):\n            header_path = doc.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in header_path:\n                score += 2\n            if \"api\" in header_path:\n                score += 1.5\n            # Also boost longer chunks moderately (up to 1 point)\n            score += min(len(doc.page_content) / 1000.0, 1.0)\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n        \n        # Limit to top rerank_top_n docs for prompt context to reduce noise\n        limited_docs = docs[:self.rerank_top_n]\n        \n        contexts = [d.page_content for d in limited_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in limited_docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = limited_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Generation prompt with chain-of-thought and mandatory terms analysis\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Step 1: Identify any mandatory terms, definitions, or key concepts related to the question in the context.\\n\"\n            \"Step 2: Based strictly on the context, provide a detailed answer. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"Include relevant detail while staying faithful. \"\n            \"If the answer is not found, say 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5342), Performs well on num_samples (10.0000), Performs well on raw_scores ([np.float64(0.48500000000000004), np.float64(0.48), np.float64(0.645), np.float64(0.655), np.float64(0.4700000000000001), np.float64(0.6578571428571429), np.float64(0.77), np.float64(0.4492857142857143), np.float64(0.45999999999999996), np.float64(0.27)])\n\n### Program 3 (Score: 0.5327)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Two-stage chunking with richer markdown header hierarchy preservation:\n        # 1. Split by top-level headers (larger chunks) to isolate sections\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_sections = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # 2. Chunk each section into smaller chunks with overlap and add \"header_path\" metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        all_chunks = []\n        for section_doc in header_sections:\n            lines = section_doc.page_content.splitlines()\n            header_lines = []\n            content_start = 0\n            # Collect all consecutive header lines at the start for richer header path (supports nested headers)\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    header_lines.append(stripped)\n                    content_start = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content_text = \"\\n\".join(lines[content_start:]) if content_start < len(lines) else section_doc.page_content\n\n            # Chunk smaller with header_path metadata for context-aware retrieval\n            chunks = small_splitter.create_documents(\n                [content_text],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n\n            # Also include the parent large chunk (the whole section_doc) as a bigger context chunk\n            # with the same header_path for small-to-big retrieval benefit\n            parent_doc = Document(page_content=section_doc.page_content, metadata={\"source\": source, \"header_path\": header_path})\n            all_chunks.append(parent_doc)\n\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Sort docs prioritizing chunks from files named \"reference\" or containing \"api\"\n        # Also boost docs containing \"reference\" or \"api\" in header_path metadata\n        # Also boost docs containing exact query keywords for relevance\n        def doc_priority(d):\n            source = d.metadata.get(\"source\", \"\").lower()\n            header_path = d.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in source:\n                score += 4\n            if \"api\" in source:\n                score += 2.5\n            if \"reference\" in header_path:\n                score += 2\n            if \"api\" in header_path:\n                score += 1.5\n            # Boost if query keywords appear in content (weight 0.75 per unique keyword)\n            query_words = set(query_str.lower().split())\n            content_words = set(re.findall(r'\\w+', d.page_content.lower()))\n            common_words = query_words.intersection(content_words)\n            score += 0.75 * len(common_words)\n            # Prefer longer chunks (likely more informative)\n            score += min(len(d.page_content) / 1000, 2)\n            return score\n        docs = sorted(docs, key=doc_priority, reverse=True)\n        \n        # Only keep top rerank_top_n documents for generation to reduce noise\n        docs = docs[:self.rerank_top_n]\n        \n        contexts = [d.page_content for d in docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Generation\n        # Add a step to ask LLM to first identify mandatory terms/concepts before answering\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Step 1: Identify any mandatory terms, definitions, or key concepts related to the question in the context.\\n\"\n            \"Step 2: Based strictly on the context, provide a detailed answer. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"Include relevant detail while staying faithful. \"\n            \"If the answer is not found, say 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5327), Performs well on num_samples (10.0000), Performs well on raw_scores ([np.float64(0.35666666666666663), np.float64(0.595), np.float64(0.475), np.float64(0.6000000000000001), np.float64(0.5650000000000001), np.float64(0.39714285714285713), np.float64(0.5650000000000001), np.float64(0.5664285714285715), np.float64(0.6416666666666667), np.float64(0.565)])\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.5273)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Improved header-aware chunking with markdown header separators including level 5 headers\n        # Also add support for small-to-big retrieval: keep mapping from small chunks to their parent header chunk\n        \n        # First split by even larger header sections to capture big context\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 5,  # Larger chunk to isolate header sections\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n##### \"]\n        )\n        header_docs = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Then chunk each header section into smaller chunks with overlap and add rich header_path metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for hd in header_docs:\n            lines = hd.page_content.splitlines()\n            header_lines = []\n            content_start_idx = 0\n            # Accumulate multiple header levels for richer context path, keep levels 1-5\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    # Limit header level to 5 hashes max for path\n                    hashes = len(stripped) - len(stripped.lstrip('#'))\n                    if hashes <= 5:\n                        header_lines.append(stripped)\n                        content_start_idx = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content = \"\\n\".join(lines[content_start_idx:])\n            if not content.strip():\n                content = hd.page_content\n\n            # Create smaller chunks with header_path metadata\n            chunks = small_splitter.create_documents(\n                [content],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            # Add parent chunk (header_doc) as well to enable small-to-big retrieval if needed\n            # Mark parent chunk with header_path and source, content is full header_doc content\n            parent_doc = hd\n            # Add header_path explicitly for parent_doc metadata (in case missing)\n            parent_meta = dict(parent_doc.metadata)\n            parent_meta[\"header_path\"] = header_path\n            parent_doc = parent_doc.__class__(page_content=parent_doc.page_content, metadata=parent_meta)\n            all_chunks.extend(chunks)\n            all_chunks.append(parent_doc)  # Append parent chunk for retrieval completeness\n\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Boost chunks with \"Reference\" or \"Tutorial\" in source metadata and presence of query keywords in chunk\n        def rerank_key(doc):\n            src = doc.metadata.get(\"source\", \"\").lower()\n            score = 0\n            if \"reference\" in src:\n                score += 3\n            if \"tutorial\" in src:\n                score += 1.5\n            # Prefer longer chunks moderately up to 1.5 points\n            score += min(len(doc.page_content) / 800, 1.5)\n            # Boost if query keywords appear in chunk (case-insensitive)\n            query_lower = query_str.lower()\n            query_words = set(query_lower.split())\n            content_lower = doc.page_content.lower()\n            # Count unique query words in content\n            count = sum(1 for w in query_words if w in content_lower)\n            score += count * 1.2  # Stronger weight for keyword presence\n\n            # Penalize redundancy to increase diversity: if snippet is in another top chunk, reduce score\n            snippet = doc.page_content[:50]\n            limited_contexts = [d.page_content for d in docs[:self.rerank_top_n]]\n            if any(snippet in c for c in limited_contexts if c != doc.page_content):\n                score -= 1.0\n\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n\n        # Limit contexts to rerank_top_n for generation prompt to reduce noise\n        top_docs = docs[:self.rerank_top_n]\n\n        contexts = [d.page_content for d in top_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in top_docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = top_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Generation with chain-of-thought style instruction and mandatory term check\n        # Add context compression by summarizing retrieved chunks before final prompt\n        summary_prompt = (\n            f\"Summarize the following context chunks concisely focusing on information relevant to the question: {query_str}\\n\\n\"\n        )\n        # For safety, limit context chunks for summarization\n        summary_input = \"\\n---\\n\".join(contexts[:self.rerank_top_n])\n        summary_prompt += summary_input + \"\\n\\nSummary:\"\n\n        summary_res = self.llm.invoke(summary_prompt)\n        compressed_context = summary_res.content\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context Summary:\\n{compressed_context}\\n\\n\"\n            \"Before answering, analyze the context carefully for any mandatory terms or definitions related to the question. \"\n            \"Then provide a detailed answer strictly based on this summary. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"If the answer is not found in the context, respond with 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n### Program D2 (Score: 0.5301)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n\n        # Two-stage chunking with richer markdown header hierarchy preservation\n        # First split by top-level headers (larger chunks) to isolate sections\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_sections = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Then chunk each section into smaller chunks with overlap and add \"header_path\" metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for section_doc in header_sections:\n            lines = section_doc.page_content.splitlines()\n            header_lines = []\n            content_start = 0\n            # Collect all consecutive header lines at the start for richer header path (supports nested headers)\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    header_lines.append(stripped)\n                    content_start = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content_text = \"\\n\".join(lines[content_start:]) if content_start < len(lines) else section_doc.page_content\n\n            # Chunk smaller with header_path metadata for context-aware retrieval\n            chunks = small_splitter.create_documents(\n                [content_text],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n\n            # Add the parent large chunk (the whole section_doc) as a bigger context chunk with the same header_path\n            # to enable small-to-big retrieval benefits\n            from langchain_core.documents import Document\n            parent_doc = Document(page_content=section_doc.page_content, metadata={\"source\": source, \"header_path\": header_path})\n            all_chunks.append(parent_doc)\n\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING ---\n        # Rerank retrieved docs by boosting those with header_path containing \"Reference\" or \"API\"\n        # Also boost docs containing exact query keywords for relevance\n        def rerank_key(doc):\n            header_path = doc.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in header_path:\n                score += 2.5\n            if \"api\" in header_path:\n                score += 1.7\n            # Boost longer chunks moderately (up to 1.2 points)\n            score += min(len(doc.page_content) / 1000.0, 1.2)\n\n            # Boost if query keywords appear in doc content (weight 0.8 per unique keyword)\n            query_words = set(query_str.lower().split())\n            content_words = set(re.findall(r'\\w+', doc.page_content.lower()))\n            common_words = query_words.intersection(content_words)\n            score += 0.8 * len(common_words)\n\n            # Slight boost if chunk header_path contains \"Tutorial\" to diversify knowledge\n            if \"tutorial\" in header_path:\n                score += 0.7\n\n            # Penalty to reduce redundancy: penalize if chunk content is very similar to already selected chunks\n            # This is a lightweight heuristic to diversify context\n            # Use simple substring check of first 50 chars\n            limited_contexts = [d.page_content for d in docs[:self.rerank_top_n]]\n            snippet = doc.page_content[:50]\n            if any(snippet in c for c in limited_contexts if c != doc.page_content):\n                score -= 1.2\n\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n\n        limited_docs = docs[:self.rerank_top_n]\n\n        contexts = [d.page_content for d in limited_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in limited_docs]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = limited_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Step 1: Identify any mandatory terms, definitions, or key concepts related to the question in the context.\\n\"\n            \"Step 2: Based strictly on the context, provide a detailed answer. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"Include relevant detail while staying faithful. \"\n            \"If the answer is not found, say 'I don't know'.\"\n        )\n\n        # Add explicit temperature override for deterministic output\n        res = self.llm.invoke(prompt)\n\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.5273, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Improved header-aware chunking with markdown header separators including level 5 headers\n        # Also add support for small-to-big retrieval: keep mapping from small chunks to their parent header chunk\n        \n        # First split by even larger header sections to capture big context\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 5,  # Larger chunk to isolate header sections\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n##### \"]\n        )\n        header_docs = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Then chunk each header section into smaller chunks with overlap and add rich header_path metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for hd in header_docs:\n            lines = hd.page_content.splitlines()\n            header_lines = []\n            content_start_idx = 0\n            # Accumulate multiple header levels for richer context path, keep levels 1-5\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    # Limit header level to 5 hashes max for path\n                    hashes = len(stripped) - len(stripped.lstrip('#'))\n                    if hashes <= 5:\n                        header_lines.append(stripped)\n                        content_start_idx = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content = \"\\n\".join(lines[content_start_idx:])\n            if not content.strip():\n                content = hd.page_content\n\n            # Create smaller chunks with header_path metadata\n            chunks = small_splitter.create_documents(\n                [content],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            # Add parent chunk (header_doc) as well to enable small-to-big retrieval if needed\n            # Mark parent chunk with header_path and source, content is full header_doc content\n            parent_doc = hd\n            # Add header_path explicitly for parent_doc metadata (in case missing)\n            parent_meta = dict(parent_doc.metadata)\n            parent_meta[\"header_path\"] = header_path\n            parent_doc = parent_doc.__class__(page_content=parent_doc.page_content, metadata=parent_meta)\n            all_chunks.extend(chunks)\n            all_chunks.append(parent_doc)  # Append parent chunk for retrieval completeness\n\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Boost chunks with \"Reference\" or \"Tutorial\" in source metadata and presence of query keywords in chunk\n        def rerank_key(doc):\n            src = doc.metadata.get(\"source\", \"\").lower()\n            score = 0\n            if \"reference\" in src:\n                score += 3\n            if \"tutorial\" in src:\n                score += 1.5\n            # Prefer longer chunks moderately up to 1.5 points\n            score += min(len(doc.page_content) / 800, 1.5)\n            # Boost if query keywords appear in chunk (case-insensitive)\n            query_lower = query_str.lower()\n            query_words = set(query_lower.split())\n            content_lower = doc.page_content.lower()\n            # Count unique query words in content\n            count = sum(1 for w in query_words if w in content_lower)\n            score += count * 1.2  # Stronger weight for keyword presence\n\n            # Penalize redundancy to increase diversity: if snippet is in another top chunk, reduce score\n            snippet = doc.page_content[:50]\n            limited_contexts = [d.page_content for d in docs[:self.rerank_top_n]]\n            if any(snippet in c for c in limited_contexts if c != doc.page_content):\n                score -= 1.0\n\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n\n        # Limit contexts to rerank_top_n for generation prompt to reduce noise\n        top_docs = docs[:self.rerank_top_n]\n\n        contexts = [d.page_content for d in top_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in top_docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = top_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Generation with chain-of-thought style instruction and mandatory term check\n        # Add context compression by summarizing retrieved chunks before final prompt\n        summary_prompt = (\n            f\"Summarize the following context chunks concisely focusing on information relevant to the question: {query_str}\\n\\n\"\n        )\n        # For safety, limit context chunks for summarization\n        summary_input = \"\\n---\\n\".join(contexts[:self.rerank_top_n])\n        summary_prompt += summary_input + \"\\n\\nSummary:\"\n\n        summary_res = self.llm.invoke(summary_prompt)\n        compressed_context = summary_res.content\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context Summary:\\n{compressed_context}\\n\\n\"\n            \"Before answering, analyze the context carefully for any mandatory terms or definitions related to the question. \"\n            \"Then provide a detailed answer strictly based on this summary. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"If the answer is not found in the context, respond with 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], Object-oriented approach\n\n### Inspiration 2 (Score: 0.5119, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # Improved header-aware chunking with markdown header separators including level 5 headers\n        # Also add support for small-to-big retrieval: keep mapping from small chunks to their parent header chunk\n        \n        # First split by even larger header sections to capture big context\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 5,  # Larger chunk to isolate header sections\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n##### \"]\n        )\n        header_docs = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Then chunk each header section into smaller chunks with overlap and add rich header_path metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for hd in header_docs:\n            lines = hd.page_content.splitlines()\n            header_lines = []\n            content_start_idx = 0\n            # Accumulate multiple header levels for richer context path, keep levels 1-5\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    # Limit header level to 5 hashes max for path\n                    hashes = len(stripped) - len(stripped.lstrip('#'))\n                    if hashes <= 5:\n                        header_lines.append(stripped)\n                        content_start_idx = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content = \"\\n\".join(lines[content_start_idx:])\n            if not content.strip():\n                content = hd.page_content\n\n            # Create smaller chunks with header_path metadata\n            chunks = small_splitter.create_documents(\n                [content],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            # Add parent chunk (header_doc) as well to enable small-to-big retrieval if needed\n            # Mark parent chunk with header_path and source, content is full header_doc content\n            parent_doc = hd\n            # Add header_path explicitly for parent_doc metadata (in case missing)\n            parent_meta = dict(parent_doc.metadata)\n            parent_meta[\"header_path\"] = header_path\n            parent_doc = parent_doc.__class__(page_content=parent_doc.page_content, metadata=parent_meta)\n            all_chunks.extend(chunks)\n            all_chunks.append(parent_doc)  # Append parent chunk for retrieval completeness\n\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING EXAMPLE (Lightweight) ---\n        # Boost chunks with \"Reference\" or \"Tutorial\" in source metadata and presence of query keywords in chunk\n        def rerank_key(doc):\n            src = doc.metadata.get(\"source\", \"\").lower()\n            score = 0\n            if \"reference\" in src:\n                score += 3\n            if \"tutorial\" in src:\n                score += 1.5\n            # Prefer longer chunks moderately up to 1.5 points\n            score += min(len(doc.page_content) / 800, 1.5)\n            # Boost if query keywords appear in chunk (case-insensitive)\n            query_lower = query_str.lower()\n            query_words = set(query_lower.split())\n            content_lower = doc.page_content.lower()\n            # Count unique query words in content\n            count = sum(1 for w in query_words if w in content_lower)\n            score += count * 1.2  # Stronger weight for keyword presence\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n\n        # Limit contexts to rerank_top_n for generation prompt to reduce noise\n        top_docs = docs[:self.rerank_top_n]\n\n        contexts = [d.page_content for d in top_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in top_docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = top_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Generation with chain-of-thought style instruction and mandatory term check\n        # Add context compression by summarizing retrieved chunks before final prompt\n        summary_prompt = (\n            f\"Summarize the following context chunks concisely focusing on information relevant to the question: {query_str}\\n\\n\"\n        )\n        # For safety, limit context chunks for summarization\n        summary_input = \"\\n---\\n\".join(contexts[:self.rerank_top_n])\n        summary_prompt += summary_input + \"\\n\\nSummary:\"\n\n        summary_res = self.llm.invoke(summary_prompt)\n        compressed_context = summary_res.content\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context Summary:\\n{compressed_context}\\n\\n\"\n            \"Before answering, analyze the context carefully for any mandatory terms or definitions related to the question. \"\n            \"Then provide a detailed answer strictly based on this summary. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"If the answer is not found in the context, respond with 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], Object-oriented approach\n\n### Inspiration 3 (Score: 0.5342, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        \n        # 2-stage chunking with header path metadata for context-aware splitting\n        # First, split by top-level markdown headers (to isolate sections)\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 3,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_sections = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n        \n        # Then chunk each section into smaller chunks with overlap and add header_path metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        all_chunks = []\n        for section_doc in header_sections:\n            # Extract header path from the start lines of the section content\n            lines = section_doc.page_content.splitlines()\n            header_path = \"\"\n            for line in lines:\n                if line.strip().startswith(\"#\"):\n                    header_path = line.strip()\n                    break\n            content_start = 0\n            if header_path:\n                try:\n                    content_start = lines.index(header_path) + 1\n                except Exception:\n                    content_start = 0\n            content_text = \"\\n\".join(lines[content_start:]) if content_start < len(lines) else section_doc.page_content\n\n            # Chunk the content smaller with header_path metadata\n            chunks = small_splitter.create_documents([content_text], metadatas=[{\"source\": source, \"header_path\": header_path}])\n            all_chunks.extend(chunks)\n        \n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING ---\n        # Rerank retrieved docs by boosting those with header_path containing \"Reference\" or \"API\"\n        def rerank_key(doc):\n            header_path = doc.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in header_path:\n                score += 2\n            if \"api\" in header_path:\n                score += 1.5\n            # Also boost longer chunks moderately (up to 1 point)\n            score += min(len(doc.page_content) / 1000.0, 1.0)\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n        \n        # Limit to top rerank_top_n docs for prompt context to reduce noise\n        limited_docs = docs[:self.rerank_top_n]\n        \n        contexts = [d.page_content for d in limited_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in limited_docs]\n        \n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = limited_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Generation prompt with chain-of-thought and mandatory terms analysis\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Step 1: Identify any mandatory terms, definitions, or key concepts related to the question in the context.\\n\"\n            \"Step 2: Based strictly on the context, provide a detailed answer. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"Include relevant detail while staying faithful. \"\n            \"If the answer is not found, say 'I don't know'.\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        \n        # Cleanup\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], Object-oriented approach\n\n# Current Program\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 1200 # Increased for better context\n        self.chunk_overlap = 250\n        self.top_k_vector = 15 # High k for retrieval phase\n        self.top_k_bm25 = 10\n        self.weight_vector = 0.6 # Preference for semantic\n        self.weight_bm25 = 0.4\n        self.rerank_top_n = 5 # Strict reranking for generation\n        self.temperature = 0.1\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        \n        # Ingestion\n        documents = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                # Efficiently skip hidden directories (in-place)\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        documents.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not documents:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(documents, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(documents)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n\n        # Generation Config\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        \n        # Immediate cleanup of temporary ingestion objects\n        gc.collect()\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n\n        # Two-stage chunking with richer markdown header hierarchy preservation\n        # First split by top-level headers (larger chunks) to isolate sections\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_sections = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Then chunk each section into smaller chunks with overlap and add \"header_path\" metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for section_doc in header_sections:\n            lines = section_doc.page_content.splitlines()\n            header_lines = []\n            content_start = 0\n            # Collect all consecutive header lines at the start for richer header path (supports nested headers)\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    header_lines.append(stripped)\n                    content_start = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content_text = \"\\n\".join(lines[content_start:]) if content_start < len(lines) else section_doc.page_content\n\n            # Chunk smaller with header_path metadata for context-aware retrieval\n            chunks = small_splitter.create_documents(\n                [content_text],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n\n            # Also include the parent large chunk (the whole section_doc) as a bigger context chunk\n            # with the same header_path for small-to-big retrieval benefit\n            from langchain_core.documents import Document\n            parent_doc = Document(page_content=section_doc.page_content, metadata={\"source\": source, \"header_path\": header_path})\n            all_chunks.append(parent_doc)\n\n        return all_chunks\n\n    def _expand_query_logic(self, query: str) -> str:\n        if not self.expand_query:\n            return query\n        \n        prompt = f\"Provide 2-3 different ways to ask the following question to improve search recall:\\nQuestion: {query}\\nOutput ONLY the expanded queries, one per line.\"\n        res = self.llm.invoke(prompt)\n        expansions = res.content.split('\\n')\n        return query + \" \" + \" \".join(expansions[:2])\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # Retrieval\n        search_query = self._expand_query_logic(query_str)\n        docs = self.retriever.invoke(search_query)\n        \n        # --- AD-HOC RERANKING ---\n        # Rerank retrieved docs by boosting those with header_path containing \"Reference\" or \"API\"\n        # Also boost docs containing exact query keywords for relevance\n        def rerank_key(doc):\n            header_path = doc.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in header_path:\n                score += 2\n            if \"api\" in header_path:\n                score += 1.5\n            # Boost longer chunks moderately (up to 1 point)\n            score += min(len(doc.page_content) / 1200.0, 1.0)\n\n            # Boost if query keywords appear in doc content (weight 0.75 per unique keyword)\n            query_words = set(query_str.lower().split())\n            content_words = set(re.findall(r'\\w+', doc.page_content.lower()))\n            common_words = query_words.intersection(content_words)\n            score += 0.75 * len(common_words)\n\n            # Slight boost if chunk header_path contains \"Tutorial\" to diversify knowledge\n            if \"tutorial\" in header_path:\n                score += 0.5\n\n            # Penalty to reduce redundancy: penalize if chunk content is very similar to already selected chunks\n            # This is a lightweight heuristic to diversify context\n            # Use simple substring check of first 50 chars\n            limited_contexts = [d.page_content for d in docs[:self.rerank_top_n]]\n            snippet = doc.page_content[:50]\n            if any(snippet in c for c in limited_contexts if c != doc.page_content):\n                score -= 1.0\n\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n\n        limited_docs = docs[:self.rerank_top_n]\n\n        contexts = [d.page_content for d in limited_docs]\n        sources = [d.metadata.get(\"source\", \"unknown\") for d in limited_docs]\n\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = limited_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Step 1: Identify any mandatory terms, definitions, or key concepts related to the question in the context.\\n\"\n            \"Step 2: Based strictly on the context, provide a detailed answer. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"Include relevant detail while staying faithful. \"\n            \"If the answer is not found, say 'I don't know'.\"\n        )\n\n        # Add explicit temperature override for deterministic output\n        res = self.llm.invoke(prompt)\n\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n\n        # Two-stage chunking with richer markdown header hierarchy preservation\n        # First split by top-level headers (larger chunks) to isolate sections\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \"]\n        )\n        header_sections = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Then chunk each section into smaller chunks with overlap and add \"header_path\" metadata\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for section_doc in header_sections:\n            lines = section_doc.page_content.splitlines()\n            header_lines = []\n            content_start = 0\n            # Collect all consecutive header lines at the start for richer header path (supports nested headers)\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    header_lines.append(stripped)\n                    content_start = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content_text = \"\\n\".join(lines[content_start:]) if content_start < len(lines) else section_doc.page_content\n\n            # Chunk smaller with header_path metadata for context-aware retrieval\n            chunks = small_splitter.create_documents(\n                [content_text],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n\n            # Also include the parent large chunk (the whole section_doc) as a bigger context chunk\n            # with the same header_path for small-to-big retrieval benefit\n            from langchain_core.documents import Document\n            parent_doc = Document(page_content=section_doc.page_content, metadata={\"source\": source, \"header_path\": header_path})\n            all_chunks.append(parent_doc)\n\n        return all_chunks\n=======\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        # Lazy import for safety\n        from langchain_text_splitters import RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n\n        # Extended recursive splitter with markdown headers and blank lines for better section isolation\n        header_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size * 4,\n            chunk_overlap=0,\n            separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n##### \"]\n        )\n        header_sections = header_splitter.create_documents([text], metadatas=[{\"source\": source}])\n\n        # Smaller chunks with overlap and enriched header_path metadata for context-aware retrieval\n        small_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        all_chunks = []\n        for section_doc in header_sections:\n            lines = section_doc.page_content.splitlines()\n            header_lines = []\n            content_start = 0\n            # Accumulate multiple consecutive markdown headers (levels 1-5) for richer hierarchy path\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped.startswith(\"#\"):\n                    # Limit header level to max 5 hashes\n                    hashes = len(stripped) - len(stripped.lstrip('#'))\n                    if hashes <= 5:\n                        header_lines.append(stripped)\n                        content_start = i + 1\n                else:\n                    if header_lines:\n                        break\n            header_path = \" > \".join(header_lines)\n            content_text = \"\\n\".join(lines[content_start:]) if content_start < len(lines) else section_doc.page_content\n\n            # Chunk smaller with header_path metadata\n            chunks = small_splitter.create_documents(\n                [content_text],\n                metadatas=[{\"source\": source, \"header_path\": header_path}]\n            )\n            all_chunks.extend(chunks)\n\n            # Include parent large chunk as bigger context chunk with same header_path for small-to-big retrieval\n            parent_doc = Document(page_content=section_doc.page_content, metadata={\"source\": source, \"header_path\": header_path})\n            all_chunks.append(parent_doc)\n\n        return all_chunks\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # --- AD-HOC RERANKING ---\n        # Rerank retrieved docs by boosting those with header_path containing \"Reference\" or \"API\"\n        # Also boost docs containing exact query keywords for relevance\n        def rerank_key(doc):\n            header_path = doc.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in header_path:\n                score += 2\n            if \"api\" in header_path:\n                score += 1.5\n            # Boost longer chunks moderately (up to 1 point)\n            score += min(len(doc.page_content) / 1200.0, 1.0)\n\n            # Boost if query keywords appear in doc content (weight 0.75 per unique keyword)\n            query_words = set(query_str.lower().split())\n            content_words = set(re.findall(r'\\w+', doc.page_content.lower()))\n            common_words = query_words.intersection(content_words)\n            score += 0.75 * len(common_words)\n\n            # Slight boost if chunk header_path contains \"Tutorial\" to diversify knowledge\n            if \"tutorial\" in header_path:\n                score += 0.5\n\n            # Penalty to reduce redundancy: penalize if chunk content is very similar to already selected chunks\n            # This is a lightweight heuristic to diversify context\n            # Use simple substring check of first 50 chars\n            limited_contexts = [d.page_content for d in docs[:self.rerank_top_n]]\n            snippet = doc.page_content[:50]\n            if any(snippet in c for c in limited_contexts if c != doc.page_content):\n                score -= 1.0\n\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n=======\n        # --- AD-HOC RERANKING ---\n        # Rerank retrieved docs boosting header_path with \"Reference\" or \"API\"\n        # Boost based on presence of query keywords and length\n        def rerank_key(doc):\n            header_path = doc.metadata.get(\"header_path\", \"\").lower()\n            score = 0\n            if \"reference\" in header_path:\n                score += 2.5\n            if \"api\" in header_path:\n                score += 1.7\n            if \"tutorial\" in header_path:\n                score += 0.7\n            # Boost longer chunks moderately (up to 1.2 points)\n            score += min(len(doc.page_content) / 1000.0, 1.2)\n\n            # Boost if query keywords appear in doc content (weight 0.8 per unique keyword)\n            query_words = set(query_str.lower().split())\n            content_words = set(re.findall(r'\\w+', doc.page_content.lower()))\n            common_words = query_words.intersection(content_words)\n            score += 0.8 * len(common_words)\n\n            # Penalty to reduce redundancy: penalize if chunk content is very similar to already selected chunks\n            # Lightweight heuristic: substring check of first 50 chars\n            limited_contexts = [d.page_content for d in docs[:self.rerank_top_n]]\n            snippet = doc.page_content[:50]\n            if any(snippet in c for c in limited_contexts if c != doc.page_content):\n                score -= 1.1\n\n            return score\n\n        docs = sorted(docs, key=rerank_key, reverse=True)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = limited_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            \"Step 1: Identify any mandatory terms, definitions, or key concepts related to the question in the context.\\n\"\n            \"Step 2: Based strictly on the context, provide a detailed answer. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"Include relevant detail while staying faithful. \"\n            \"If the answer is not found, say 'I don't know'.\"\n        )\n\n        # Add explicit temperature override for deterministic output\n        res = self.llm.invoke(prompt)\n\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": contexts}\n=======\n        context_block = \"\"\n        for i, (content, src) in enumerate(zip(contexts, sources)):\n            header_path = limited_docs[i].metadata.get(\"header_path\", \"\")\n            header_info = f\" [{header_path}]\" if header_path else \"\"\n            context_block += f\"Source {i+1} ({os.path.basename(src)}){header_info}:\\n{content}\\n\\n\"\n\n        # Context compression: summarize retrieved chunks focusing on question relevance\n        summary_prompt = (\n            f\"Summarize the following context chunks concisely focusing on information relevant to the question: {query_str}\\n\\n\"\n        )\n        summary_input = \"\\n---\\n\".join(contexts[:self.rerank_top_n])\n        summary_prompt += summary_input + \"\\n\\nSummary:\"\n\n        summary_res = self.llm.invoke(summary_prompt)\n        compressed_context = summary_res.content\n\n        prompt = (\n            f\"Question: {query_str}\\n\\n\"\n            f\"Context Summary:\\n{compressed_context}\\n\\n\"\n            \"Before answering, analyze the context carefully for any mandatory terms or definitions related to the question. \"\n            \"Then provide a detailed answer strictly based on this summary. \"\n            \"If code examples are present, prioritize them in your explanation. \"\n            \"If the answer is not found in the context, respond with 'I don't know'.\"\n        )\n\n        # Use deterministic temperature for final answer\n        res = self.llm.invoke(prompt)\n\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}