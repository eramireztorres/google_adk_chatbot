{"id": "1c2e4697-e05b-4858-a889-92be7e126ad1", "code": "import os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Normalize label to title case and add language tag explicitly in header for clarity\n            normalized_label = label.title()\n            # Add a note for multi-language tabs for reranker synergy\n            return f\"\\n\\n#### [{lang_tag}] {normalized_label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            # Normalize breadcrumb for case-insensitive matching\n            breadcrumb_lower = breadcrumb.lower()\n            if re.search(r'\\[a2a\\]', breadcrumb_lower): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[workflow\\]', breadcrumb_lower): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {p.metadata.get('tags','')} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            \n            # Improve language detection in reranker by parsing chunk content for LANG tag fallback to metadata\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', d.page_content)\n            lang = lang_tag_search.group(1).lower() if lang_tag_search else d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            \n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            # Add project area tag if available for synergy\n            project_area = d.metadata.get(\"project_area\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} {project_area} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Assign a relevance score (0-100) to each chunk for the user query.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            \"- Penalize partial snippets and chunks in other languages.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                # Fallback heuristic: rank by presence of requested language and completeness\n                def score_heuristic(doc):\n                    score = 0\n                    content = doc.page_content.lower()\n                    if requested_lang and f\"[lang={requested_lang}]\" in content:\n                        score += 50\n                    if re.search(r'\\b(class|func|def)\\b', content):\n                        score += 30\n                    if query.lower() in content:\n                        score += 20\n                    # Bonus: prefer chunks with project area tags if query contains project area keywords\n                    project_area_keywords = {\n                        \"a2a\": \"[A2A]\",\n                        \"auth\": \"[A2A]\",\n                        \"workflow\": \"[WORKFLOW]\"\n                    }\n                    for kw, tag in project_area_keywords.items():\n                        if kw in query.lower() and tag.lower() in content:\n                            score += 10\n                    return score\n                scored_docs = sorted(docs, key=score_heuristic, reverse=True)\n                return scored_docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            # Use word boundary to avoid partial matches\n            if re.search(r'\\b' + lang + r'\\b', lower_query):\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n\n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Normalize language names for comparison\n            chunk_lang_norm = chunk_lang.lower()\n            requested_lang_norm = requested_lang.lower() if requested_lang else \"\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang_norm == \"python\" and chunk_lang_norm in (\"python\", \"unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang_norm == requested_lang_norm:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # Fallback to original to preserve recall if filtered set too small\n            pass\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                parent_lang_norm = parent_lang.lower()\n                requested_lang_norm = requested_lang.lower() if requested_lang else \"\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang_norm == \"python\" and parent_lang_norm not in (\"python\", \"unknown\"):\n                    continue\n                elif requested_lang_norm != \"python\" and parent_lang_norm != requested_lang_norm:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use LLM invoke with retry to improve robustness\n        try:\n            res = self.llm.invoke(prompt)\n        except Exception as e:\n            print(f\"LLM generation failed: {e}\")\n            return {\"answer\": \"I don't know from the provided documentation\", \"contexts\": final_contexts}\n        finally:\n            gc.collect()\n        \n        # Explicitly call gc after invoke for memory stability\n        gc.collect()\n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "bb876c88-57e6-4320-a85a-520eb22c8fd2", "generation": 4, "timestamp": 1769742927.8720706, "iteration_found": 94, "metrics": {"combined_score": 0.6142083333333334, "num_samples": 10}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 8 lines with 10 lines\nChange 2: Replace 5 lines with 5 lines\nChange 3: Replace 29 lines with 32 lines\nChange 4: Replace 10 lines with 12 lines\nChange 5: Replace 16 lines with 17 lines\nChange 6: Replace 18 lines with 20 lines\nChange 7: Replace 11 lines with 13 lines\nChange 8: Replace 25 lines with 34 lines", "parent_metrics": {"combined_score": 0.636732142857143, "num_samples": 10}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer for Google ADK documentation. Improve the code to maximize RAG-pipeline performance.\n\nThe current architecture uses a **Parent-Child Retrieval** system with **Hybrid Search** and **LLM Reranking**.\n\n### High-Leverage Evolution Points:\n\n1. **Hyperparameter Synergy**: Optimize the balance between `chunk_size` (for retrieval precision), `top_k_vector/bm25` (for recall), and `weight_vector/bm25`. The documentation has many exact API names which may favor BM25, while conceptual \"how-to\" queries favor Vector.\n2. **Reranker Prompt Engineering**: The `_llm_rerank` prompt is critical. Evolve it to better identify relevance, handle multi-language snippets, or detect if a chunk contains a complete API definition vs. a partial snippet.\n3. **Metadata-Rich Ingestion**: Enhance `_chunk_document` to include more context in `breadcrumb` and `tagging`. Consider adding language-specific tags or project-area identifiers (e.g. `[A2A]`, `[WORKFLOW]`) to help the reranker and generator.\n4. **Context Assembly & Prompting**: Optimize the `context_block` formatting in `query`. Experiment with summarizing parent context or using XML-style tags to separate sources. Refine the final generation prompt for better \"Quote then Explain\" adherence.\n5. **Cross-Language Filtering**: Implement logic to detect the user's requested language and prioritize or filter retrieved chunks to reduce cross-language noise.\n\n**STRICT RULES**:\n- **Fork-Safety**: Avoid any top-level imports of `langchain`, `agno`, or `openai`. Use lazy imports inside methods.\n- **Compatibility**: Ensure `evaluate_rag(docs_path, query)` maintains its signature.\n- **Cleanup**: Always call `gc.collect()` after LLM calls or retrieval to maintain memory stability.\n\n**Example Improvement (Reranker Logic)**:\n```python\n# You can evolve the reranker prompt to be more specific:\nprompt = (\n    \"Role: ADK Expert Reranker. \"\n    \"Goal: Select chunks that provide EXECUTABLE code for the query. \"\n    f\"Query: {query}\\n\"\n    \"Chunks: ... \"\n)\n```\n- Maintain `evaluate_rag(docs_path, query)` signature and return keys: `answer`, `contexts`.\n- Keep `RAGSystem` class structure and lazy-loading in `__init__`.\n", "user": "# Current Program Information\n- Fitness: 0.6367\n- Feature coordinates: combined_score=0.64\n- Focus areas: - Fitness improved: 0.6292 \u2192 0.6367\n- Exploring combined_score=0.64 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### raw_scores\n```\n[0.36166666666666664, 0.515, 0.7, 0.6821428571428572, 0.6150000000000001, 0.79375, 0.6950000000000001, 0.6547619047619048, 0.575, 0.7750000000000001]\n```\n\n### error_details\n```\n[{'sample': 1, 'type': 'code_error', 'message': '[\"Instantiation Error on line 8: name \\'Event\\' is not defined\"]', 'bad_code': '# Simplified view of logic inside Agent.run_async, callbacks, or tools\\n\\n# ... previous code runs based on current state ...\\n\\n# 1. Determine a change or output is needed, construct the event\\n# Example: Updating state\\nupdate_data = {\\'field_1\\': \\'value_2\\'}\\nevent_with_state_change = Event(\\n    author=self.name,\\n    actions=EventActions(state_delta=update_data),\\n    content=types.Content(parts=[types.Part(text=\"State updated.\")])\\n    # ... other event fields ...\\n)\\n\\n# 2. Yield the event to the Runner for processing & commit\\nyield event_with_state_change\\n# <<<<<<<<<<<< EXECUTION PAUSES HERE >>>>>>>>>>>>\\n\\n# <<<<<<<<<<<< RUNNER PROCESSES & COMMITS THE EVENT >>>>>>>>>>>>\\n\\n# 3. Resume execution ONLY after Runner is done processing the above event.\\n# Now, the state committed by the Runner is reliably reflected.\\nval = ctx.session.state[\\'field_1\\']\\n# here `val` is guaranteed to be \"value_2\" (assuming Runner committed successfully)\\nprint(f\"Resumed execution. Value of field_1 is now: {val}\")\\n\\n# ... subsequent code continues ...\\n# Maybe yield another event later...'}, {'sample': 3, 'type': 'code_error', 'message': 'Syntax Error: invalid syntax (<unknown>, line 12)', 'bad_code': '# Core abstractions: wrap a function as a tool.\\nfrom google.adk.tools import FunctionTool, ToolContext\\n\\ndef get_time(tool_context: ToolContext) -> dict:\\n    return {\"time\": \"12:00\"}\\n\\ntime_tool = FunctionTool(func=get_time, name=\"get_time\")\\n\\nfrom google.adk.tools import FunctionTool, ToolContext\\nfrom typing import Dict\\n\\ndef my_authenticated_tool_function(param1: str, ..., tool_context: ToolContext) -> dict:\\n    # ... your logic ...\\n    pass\\n\\nmy_tool = FunctionTool(func=my_authenticated_tool_function)\\n\\nfrom google.adk.tools import FunctionTool, ToolContext\\n\\ndef get_time(tool_context: ToolContext) -> dict:\\n    return {\"time\": \"12:00\"}\\n\\ntime_tool = FunctionTool(func=get_time, name=\"get_time\")'}, {'sample': 4, 'type': 'code_error', 'message': '[\"Instantiation Error on line 27: name \\'get_weather\\' is not defined\", \"Instantiation Error on line 37: name \\'agent\\' is not defined\"]', 'bad_code': 'from google.genai import types\\nfrom google.adk.agents.llm_agent import LlmAgent\\nfrom google.adk.runners import Runner\\nfrom google.adk.sessions import InMemorySessionService\\n\\nAPP_NAME = \"weather_app\"\\nUSER_ID = \"1234\"\\nSESSION_ID = \"session1234\"\\n\\n# Define a simple tool function (optional)\\ndef get_weather(city: str) -> dict:\\n    if city.lower() == \"new york\":\\n        return {\\n            \"status\": \"success\",\\n            \"report\": (\\n                \"The weather in New York is sunny with a temperature of 25 degrees\"\\n                \" Celsius (77 degrees Fahrenheit).\"\\n            ),\\n        }\\n    else:\\n        return {\\n            \"status\": \"error\",\\n            \"error_message\": f\"Weather information for \\'{city}\\' is not available.\",\\n        }\\n\\n# Create the LlmAgent with optional tools\\nagent = LlmAgent(\\n    model=\"gemini-2.5-pro-preview-03-25\",\\n    name=\"weather_and_time_agent\",\\n    instruction=\"You are an agent that returns time and weather\",\\n    tools=[get_weather],  # Optional: list of tool functions\\n)\\n\\n# Setup session service and runner\\nsession_service = InMemorySessionService()\\nsession = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\\nrunner = Runner(agent=agent, app_name=APP_NAME, session_service=session_service)\\n\\n# Function to call the agent with a query and print the response\\ndef call_agent(query: str):\\n    content = types.Content(role=\\'user\\', parts=[types.Part(text=query)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n    for event in events:\\n        if event.is_final_response() and event.content:\\n            final_answer = event.content.parts[0].text.strip()\\n            print(\"\\\\n\ud83d\udfe2 FINAL ANSWER\\\\n\", final_answer, \"\\\\n\")\\n\\n# Example usage\\ncall_agent(\"What is the weather in New York today?\")'}, {'sample': 6, 'type': 'code_error', 'message': '[\"Instantiation Error on line 14: name \\'update_user_preference\\' is not defined\", \"Instantiation Error on line 33: name \\'update_user_preference\\' is not defined\"]', 'bad_code': 'from google.adk.tools import ToolContext, FunctionTool\\n\\ndef update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n    \"\"\"Updates a user-specific preference.\"\"\"\\n    user_prefs_key = \"user:preferences\"\\n    # Get current preferences or initialize if none exist\\n    preferences = tool_context.state.get(user_prefs_key, {})\\n    preferences[preference] = value\\n    # Write the updated dictionary back to the state\\n    tool_context.state[user_prefs_key] = preferences\\n    print(f\"Tool: Updated user preference \\'{preference}\\' to \\'{value}\\'\")\\n    return {\"status\": \"success\", \"updated_preference\": preference}\\n\\npref_tool = FunctionTool(func=update_user_preference)\\n\\n# In an Agent:\\n# my_agent = Agent(..., tools=[pref_tool])\\n\\n# When the LLM calls update_user_preference(preference=\\'theme\\', value=\\'dark\\', ...):\\n# The tool_context.state will be updated, and the change will be part of the\\n# resulting tool response event\\'s actions.state_delta.\\n\\nfrom google.adk.tools import ToolContext, FunctionTool\\n\\ndef update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n    user_prefs_key = \"user:preferences\"\\n    preferences = tool_context.state.get(user_prefs_key, {})\\n    preferences[preference] = value\\n    tool_context.state[user_prefs_key] = preferences\\n    print(f\"Tool: Updated user preference \\'{preference}\\' to \\'{value}\\'\")\\n    return {\"status\": \"success\", \"updated_preference\": preference}\\n\\npref_tool = FunctionTool(func=update_user_preference)'}, {'sample': 7, 'type': 'code_error', 'message': '[\"Import Error on line 4: No module named \\'google.adk.runner\\'\", \"Import Error on line 7: No module named \\'rx\\'\"]', 'bad_code': '# Complete ADK script with SequentialAgent pipeline of three LlmAgents passing data via output_key and running with InMemoryRunner\\n\\nfrom google.adk.agents import LlmAgent, SequentialAgent\\nfrom google.adk.runner import InMemoryRunner\\nfrom google.adk.sessions import Session\\nfrom google.genai.types import Content, Part\\nfrom rx import operators as ops\\n\\n# --- 1. Define Sub-Agents for Each Pipeline Stage ---\\n\\n# Code Writer Agent: generates initial code\\ncode_writer_agent = LlmAgent(\\n    name=\"CodeWriterAgent\",\\n    model=\"gemini-2.0-flash\",\\n    instruction=\"\"\"You are a Python Code Generator.\\nBased *only* on the user\\'s request, write Python code that fulfills the requirement.\\nOutput *only* the complete Python code block, enclosed in triple backticks (\\n\\n).\\nDo not add any other text before or after the code block.\\n\"\"\",\\n    description=\"Writes initial Python code based on a specification.\",\\n    output_key=\"generated_code\"  # Saves output to state[\\'generated_code\\']\\n)\\n\\n# Code Reviewer Agent: reviews the generated code\\ncode_reviewer_agent = LlmAgent(\\n    name=\"CodeReviewerAgent\",\\n    model=\"gemini-2.0-flash\",\\n    instruction=\"\"\"You are an expert Python Code Reviewer.\\nYour task is to provide constructive feedback on the provided code.\\n\\n**Code to Review:**\\n\\n**Review Criteria:**\\n1.  **Correctness:** Does the code work as intended? Are there logic errors?\\n2.  **Readability:** Is the code clear and easy to understand? Follows PEP 8 style guidelines?\\n3.  **Efficiency:** Is the code reasonably efficient? Any obvious performance bottlenecks?\\n4.  **Edge Cases:** Does the code handle potential edge cases or invalid inputs gracefully?\\n5.  **Best Practices:** Does the code follow common Python best practices?\\n\\n**Output:**\\nProvide your feedback as a concise, bulleted list. Focus on the most important points for improvement.\\nIf the code is excellent and requires no changes, simply state: \"No major issues found.\"\\nOutput *only* the review comments or the \"No major issues\" statement.\\n\"\"\",\\n    description=\"Reviews code and provides feedback.\",\\n    output_key=\"review_comments\"  # Saves output to state[\\'review_comments\\']\\n)\\n\\n# Code Refactorer Agent: refactors code based on review comments\\ncode_refactorer_agent = LlmAgent(\\n    name=\"CodeRefactorerAgent\",\\n    model=\"gemini-2.0-flash\",\\n    instruction=\"\"\"You are a Python Code Refactoring AI.\\nYour goal is to improve the given Python code based on the provided review comments.\\n\\n**Original Code:**\\n\\n**Review Comments:**\\n{review_comments}\\n\\n**Task:**\\nCarefully apply the suggestions from the review comments to refactor the original code.\\nIf the review comments state \"No major issues found,\" return the original code unchanged.\\nEnsure the final code is complete, functional, and includes necessary imports and docstrings.\\n\\n**Output:**\\nOutput *only* the final, refactored Python code block, enclosed in triple backticks (\\n\\n).\\nDo not add any other text before or after the code block.\\n\"\"\",\\n    description=\"Refactors code based on review comments.\",\\n    output_key=\"refactored_code\"  # Saves output to state[\\'refactored_code\\']\\n)\\n\\n# --- 2. Create the SequentialAgent pipeline ---\\ncode_pipeline_agent = SequentialAgent(\\n    name=\"CodePipelineAgent\",\\n    sub_agents=[code_writer_agent, code_reviewer_agent, code_refactorer_agent],\\n    description=\"Executes a sequence of code writing, reviewing, and refactoring.\"\\n)\\n\\n# --- 3. Run the pipeline with InMemoryRunner ---\\n\\ndef run_pipeline(prompt: str):\\n    runner = InMemoryRunner(code_pipeline_agent, \"CodePipelineAgent\")\\n    session = runner.session_service().create_session(\"CodePipelineAgent\", \"user_123\").blocking_first()\\n    user_message = Content.from_parts(Part.from_text(prompt))\\n\\n    # Run the agent asynchronously and collect events\\n    event_stream = runner.run_async(\"user_123\", session.id(), user_message)\\n\\n    # Print final response when available\\n    def on_event(event):\\n        if event.final_response():\\n            print(event.stringify_content())\\n\\n    event_stream.subscribe(on_event)\\n\\n# Example usage\\nif __name__ == \"__main__\":\\n    run_pipeline(\"Write a Python function to compute the Fibonacci sequence.\")'}, {'sample': 8, 'type': 'code_error', 'message': '[\"Instantiation Error on line 27: name \\'CheckStatusAndEscalate\\' is not defined\"]', 'bad_code': 'from google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\n# Agent to generate/refine code based on state[\\'current_code\\'] and state[\\'requirements\\']\\ncode_refiner = LlmAgent(\\n    name=\"CodeRefiner\",\\n    instruction=\"Read state[\\'current_code\\'] (if exists) and state[\\'requirements\\']. Generate/refine Python code to meet requirements. Save to state[\\'current_code\\'].\",\\n    output_key=\"current_code\"  # Overwrites previous code in state\\n)\\n\\n# Agent to check if the code meets quality standards\\nquality_checker = LlmAgent(\\n    name=\"QualityChecker\",\\n    instruction=\"Evaluate the code in state[\\'current_code\\'] against state[\\'requirements\\']. Output \\'pass\\' or \\'fail\\'.\",\\n    output_key=\"quality_status\"\\n)\\n\\n# Custom agent to check the status and escalate if \\'pass\\'\\nclass CheckStatusAndEscalate(BaseAgent):\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        status = ctx.session.state.get(\"quality_status\", \"fail\")\\n        should_stop = (status == \"pass\")\\n        yield Event(author=self.name, actions=EventActions(escalate=should_stop))\\n\\nrefinement_loop = LoopAgent(\\n    name=\"CodeRefinementLoop\",\\n    max_iterations=5,\\n    sub_agents=[code_refiner, quality_checker, CheckStatusAndEscalate(name=\"StopChecker\")]\\n)\\n# Loop runs: Refiner -> Checker -> StopChecker\\n# State[\\'current_code\\'] is updated each iteration.\\n# Loop stops if QualityChecker outputs \\'pass\\' (leading to StopChecker escalating) or after 5 iterations.'}]\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 12 lines with 16 lines\nChange 2: Replace 51 lines with 83 lines\nChange 3: Replace 48 lines with 63 lines\n- Metrics: combined_score: 0.6292, num_samples: 10.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 28 lines with 32 lines\nChange 2: Replace 54 lines with 56 lines\nChange 3: Replace 16 lines with 16 lines\nChange 4: Replace 20 lines with 22 lines\nChange 5: Replace 11 lines with 12 lines\n- Metrics: combined_score: 0.6367, num_samples: 10.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 9 lines with 11 lines\nChange 2: Replace 10 lines with 11 lines\nChange 3: Replace 21 lines with 24 lines\nChange 4: Replace 47 lines with 47 lines\nChange 5: Replace 28 lines with 31 lines\nChange 6: Replace 45 lines with 46 lines\n- Metrics: combined_score: 0.6422, num_samples: 10.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.6422)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Append a standardized language tag for chunk tagging synergy\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6422), Performs well on num_samples (10.0000)\n\n### Program 2 (Score: 0.6367)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Normalize label to title case and add language tag explicitly in header for clarity\n            normalized_label = label.title()\n            return f\"\\n\\n#### [{lang_tag}] {normalized_label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            \n            # Improve language detection in reranker by parsing chunk content for LANG tag fallback to metadata\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', d.page_content)\n            lang = lang_tag_search.group(1).lower() if lang_tag_search else d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            \n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Assign a relevance score (0-100) to each chunk for the user query.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            \"- Penalize partial snippets and chunks in other languages.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                # Fallback heuristic: rank by presence of requested language and completeness\n                def score_heuristic(doc):\n                    score = 0\n                    content = doc.page_content.lower()\n                    if requested_lang and f\"[lang={requested_lang}]\" in content:\n                        score += 50\n                    if re.search(r'\\b(class|func|def)\\b', content):\n                        score += 30\n                    if query.lower() in content:\n                        score += 20\n                    return score\n                scored_docs = sorted(docs, key=score_heuristic, reverse=True)\n                return scored_docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            # Use word boundary to avoid partial matches\n            if re.search(r'\\b' + lang + r'\\b', lower_query):\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n\n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Normalize language names for comparison\n            chunk_lang_norm = chunk_lang.lower()\n            requested_lang_norm = requested_lang.lower() if requested_lang else \"\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang_norm == \"python\" and chunk_lang_norm in (\"python\", \"unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang_norm == requested_lang_norm:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                parent_lang_norm = parent_lang.lower()\n                requested_lang_norm = requested_lang.lower() if requested_lang else \"\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang_norm == \"python\" and parent_lang_norm not in (\"python\", \"unknown\"):\n                    continue\n                elif requested_lang_norm != \"python\" and parent_lang_norm != requested_lang_norm:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use LLM invoke with retry to improve robustness\n        try:\n            res = self.llm.invoke(prompt)\n        except Exception as e:\n            print(f\"LLM generation failed: {e}\")\n            return {\"answer\": \"I don't know from the provided documentation\", \"contexts\": final_contexts}\n        finally:\n            gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6367), Performs well on num_samples (10.0000)\n\n### Program 3 (Score: 0.6292)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Normalize label to title case and add language tag explicitly in header for clarity\n            normalized_label = label.title()\n            return f\"\\n\\n#### [{lang_tag}] {normalized_label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            \n            # Mark language if available and if matches requested language\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            lang_flag = \"[LANG_MATCH]\" if requested_lang and lang == requested_lang else \"[LANG_OTHER]\"\n            \n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {lang_flag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Assign a relevance score (0-100) to each chunk for the user query.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            \"- Penalize partial snippets and chunks in other languages.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                # Fallback heuristic: rank by presence of requested language and completeness\n                def score_heuristic(doc):\n                    score = 0\n                    content = doc.page_content.lower()\n                    if requested_lang and f\"[lang={requested_lang}]\" in content:\n                        score += 50\n                    if re.search(r'\\b(class|func|def)\\b', content):\n                        score += 30\n                    if query.lower() in content:\n                        score += 20\n                    return score\n                scored_docs = sorted(docs, key=score_heuristic, reverse=True)\n                return scored_docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n\n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use LLM invoke with retry to improve robustness\n        try:\n            res = self.llm.invoke(prompt)\n        except Exception as e:\n            print(f\"LLM generation failed: {e}\")\n            return {\"answer\": \"I don't know from the provided documentation\", \"contexts\": final_contexts}\n        finally:\n            gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6292), Performs well on num_samples (10.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.6264)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Normalize label to title case and add language tag explicitly in header for clarity\n            normalized_label = label.title()\n            return f\"\\n\\n#### [{lang_tag}] {normalized_label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add language and project-area tags for better filtering and relevance\n            lang_match = re.search(r'```(\\w+)', p.page_content)\n            language = lang_match.group(1).lower() if lang_match else \"unknown\"\n            # Normalize language tags for synergy\n            if language in (\"python\", \"py\"):\n                language = \"python\"\n            elif language in (\"golang\", \"go\"):\n                language = \"go\"\n            elif language == \"java\":\n                language = \"java\"\n            else:\n                language = \"unknown\"\n            p.metadata[\"language\"] = language\n            \n            # Heuristic project-area tagging based on breadcrumbs or source file path, with prioritization\n            breadcrumb_lower = breadcrumb.lower()\n            source_lower = source.lower()\n            if re.search(r'\\b(a2a|auth)\\b', breadcrumb_lower) or re.search(r'\\b(a2a|auth)\\b', source_lower):\n                p.metadata[\"project_area\"] = \"[A2A]\"\n            elif re.search(r'\\bworkflow\\b', breadcrumb_lower) or re.search(r'\\bworkflow\\b', source_lower):\n                p.metadata[\"project_area\"] = \"[WORKFLOW]\"\n            else:\n                p.metadata[\"project_area\"] = \"[GENERAL]\"\n                \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Add explicit language tag into chunk content for reranker synergy\n                lang_tag = f\"[LANG={language.capitalize()}]\" if language != \"unknown\" else \"[LANG=Unknown]\"\n                # Add explicit project_area tag inside chunk content for synergy\n                project_area = p.metadata.get(\"project_area\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = language\n                c.metadata[\"project_area\"] = project_area\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for language, completeness, and snippet type detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        # Detect requested language from query for reranker prompt emphasis\n        requested_lang = None\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n        \n        for i, d in enumerate(docs):\n            # Heuristic completeness detection for full API defs vs partial snippet\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def)\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            \n            # Mark language if available and if matches requested language\n            lang_tag = d.metadata.get(\"language\", \"unknown\")\n            lang_flag = \"[LANG_MATCH]\" if requested_lang and lang_tag == requested_lang else \"[LANG_OTHER]\"\n            \n            meta = d.metadata.get(\"breadcrumb\", \"\") + \" \" + d.metadata.get(\"source\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {lang_flag} Metadata: {meta}\\nContent: {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Assign a relevance score (0-100) to each chunk for the user query.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            \"- Penalize partial snippets and chunks in other languages.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: \n                # Fallback heuristic: score by presence of requested language and completeness\n                def score_heuristic(doc):\n                    score = 0\n                    content = doc.page_content.lower()\n                    if requested_lang and f\"[lang={requested_lang}]\" in content:\n                        score += 50\n                    if re.search(r'\\b(class|func|def)\\b', content):\n                        score += 30\n                    if query.lower() in content:\n                        score += 20\n                    # Bonus: prefer chunks with project area tags if query contains project area keywords\n                    project_area_keywords = {\n                        \"a2a\": \"[A2A]\",\n                        \"auth\": \"[A2A]\",\n                        \"workflow\": \"[WORKFLOW]\"\n                    }\n                    for kw, tag in project_area_keywords.items():\n                        if kw in query.lower() and tag.lower() in content:\n                            score += 10\n                    return score\n                scored_docs = sorted(docs, key=score_heuristic, reverse=True)\n                return scored_docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Cross-language filtering: detect requested language from query and filter candidates\n        requested_lang = None\n        lang_pattern = re.compile(r'\\b(python|go|java)\\b', re.I)\n        lang_match = lang_pattern.search(query_str)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        # Filter candidates by language tag in metadata or content with fallback to preserve recall\n        if requested_lang:\n            filtered_candidates = []\n            for c in candidates:\n                lang = c.metadata.get(\"language\", \"\").lower()\n                # Accept if language matches or is unknown (for Python default)\n                if lang == requested_lang or (requested_lang == \"python\" and lang == \"unknown\"):\n                    filtered_candidates.append(c)\n            # Relax filtering if too few candidates remain to preserve recall\n            if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n                candidates = filtered_candidates\n            else:\n                # Fallback to original set if too few candidates remain\n                candidates = self.retriever.invoke(query_str)\n\n        # 2.5. Expand query if no good candidates found to increase recall, with improved expansion logic\n        if self.expand_query and (not candidates or len(candidates) < 5):\n            def _expand_query(query: str) -> str:\n                base_expansion = \"google adk API usage example\"\n                lower_query = query.lower()\n                if \"google adk\" not in lower_query:\n                    return query + \" \" + base_expansion\n                additions = [\"tutorial\", \"code sample\", \"example\", \"reference\"]\n                for add in additions:\n                    if add not in lower_query:\n                        return query + \" \" + add\n                return query\n\n            expanded_query = _expand_query(query_str)\n            if expanded_query and expanded_query != query_str:\n                candidates = self.retriever.invoke(expanded_query)\n\n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Summarize parent context titles for brevity and clarity\n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent by language similarly to chunk filtering\n                lang = parent.metadata.get(\"language\", \"\").lower()\n                if requested_lang:\n                    if requested_lang == \"python\" and lang not in (\"python\", \"unknown\"):\n                        continue\n                    elif requested_lang != \"python\" and lang != requested_lang:\n                        continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Use XML style tags for clarity and better chunk separation\n                # Summarize parent content to first 1000 chars to reduce prompt size\n                parent_summary = parent.page_content[:1000].rstrip()\n                if len(parent.page_content) > 1000:\n                    parent_summary += \"\\n...[truncated]...\\n\"\n                context_block += (\n                    f\"<source file='{src}' section='{breadcrumb}'>\\n\"\n                    f\"{parent_summary}\\n\"\n                    f\"</source>\\n\\n\"\n                )\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\"\n            \"6. When quoting code snippets, explicitly label the language for clarity.\\n\"\n            \"7. Use XML-style tags to separate and identify sources clearly.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        try:\n            res = self.llm.invoke(prompt)\n        except Exception as e:\n            print(f\"LLM generation failed: {e}\")\n            return {\"answer\": \"I don't know from the provided documentation\", \"contexts\": final_contexts}\n        finally:\n            gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n    def _expand_query(self, query: str) -> str:\n        \"\"\"Optional method to expand or rephrase query for broader recall.\"\"\"\n        # Simple heuristic: append \"API usage example\" or \"Google ADK\" to query\n        # to help capture more documents\n        if \"google adk\" not in query.lower():\n            return query + \" google adk API usage example\"\n        return query\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n### Program D2 (Score: 0.6237)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Append a standardized language tag for chunk tagging synergy\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            # Add language tags from breadcrumb if present (e.g. [Python], [Go], [Java])\n            lang_tag_match = re.search(r'\\[(Python|Go|Java)\\]', breadcrumb)\n            if lang_tag_match:\n                project_tags.append(f\"[LANG={lang_tag_match.group(1)}]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                # Also append completeness tag for reranker synergy if chunk contains full API definition\n                completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', c.page_content, re.I) else \"[PARTIAL]\"\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {completeness_tag} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            # Improve language detection in reranker by parsing chunk content for LANG tag fallback to metadata\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', d.page_content)\n            lang = lang_tag_search.group(1).lower() if lang_tag_search else d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', lower_query):\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Normalize language names for comparison\n            chunk_lang_norm = chunk_lang.lower()\n            requested_lang_norm = requested_lang.lower() if requested_lang else \"\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang_norm == \"python\" and chunk_lang_norm in (\"python\", \"unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang_norm == requested_lang_norm:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.6422, Type: Alternative)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Append a standardized language tag for chunk tagging synergy\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n### Inspiration 2 (Score: 0.6264, Type: Alternative)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Normalize label to title case and add language tag explicitly in header for clarity\n            normalized_label = label.title()\n            return f\"\\n\\n#### [{lang_tag}] {normalized_label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add language and project-area tags for better filtering and relevance\n            lang_match = re.search(r'```(\\w+)', p.page_content)\n            language = lang_match.group(1).lower() if lang_match else \"unknown\"\n            # Normalize language tags for synergy\n            if language in (\"python\", \"py\"):\n                language = \"python\"\n            elif language in (\"golang\", \"go\"):\n                language = \"go\"\n            elif language == \"java\":\n                language = \"java\"\n            else:\n                language = \"unknown\"\n            p.metadata[\"language\"] = language\n            \n            # Heuristic project-area tagging based on breadcrumbs or source file path, with prioritization\n            breadcrumb_lower = breadcrumb.lower()\n            source_lower = source.lower()\n            if re.search(r'\\b(a2a|auth)\\b', breadcrumb_lower) or re.search(r'\\b(a2a|auth)\\b', source_lower):\n                p.metadata[\"project_area\"] = \"[A2A]\"\n            elif re.search(r'\\bworkflow\\b', breadcrumb_lower) or re.search(r'\\bworkflow\\b', source_lower):\n                p.metadata[\"project_area\"] = \"[WORKFLOW]\"\n            else:\n                p.metadata[\"project_area\"] = \"[GENERAL]\"\n                \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Add explicit language tag into chunk content for reranker synergy\n                lang_tag = f\"[LANG={language.capitalize()}]\" if language != \"unknown\" else \"[LANG=Unknown]\"\n                # Add explicit project_area tag inside chunk content for synergy\n                project_area = p.metadata.get(\"project_area\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = language\n                c.metadata[\"project_area\"] = project_area\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for language, completeness, and snippet type detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        # Detect requested language from query for reranker prompt emphasis\n        requested_lang = None\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n        \n        for i, d in enumerate(docs):\n            # Heuristic completeness detection for full API defs vs partial snippet\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def)\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            \n            # Mark language if available and if matches requested language\n            lang_tag = d.metadata.get(\"language\", \"unknown\")\n            lang_flag = \"[LANG_MATCH]\" if requested_lang and lang_tag == requested_lang else \"[LANG_OTHER]\"\n            \n            meta = d.metadata.get(\"breadcrumb\", \"\") + \" \" + d.metadata.get(\"source\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {lang_flag} Metadata: {meta}\\nContent: {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Assign a relevance score (0-100) to each chunk for the user query.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            \"- Penalize partial snippets and chunks in other languages.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: \n                # Fallback heuristic: score by presence of requested language and completeness\n                def score_heuristic(doc):\n                    score = 0\n                    content = doc.page_content.lower()\n                    if requested_lang and f\"[lang={requested_lang}]\" in content:\n                        score += 50\n                    if re.search(r'\\b(class|func|def)\\b', content):\n                        score += 30\n                    if query.lower() in content:\n                        score += 20\n                    # Bonus: prefer chunks with project area tags if query contains project area keywords\n                    project_area_keywords = {\n                        \"a2a\": \"[A2A]\",\n                        \"auth\": \"[A2A]\",\n                        \"workflow\": \"[WORKFLOW]\"\n                    }\n                    for kw, tag in project_area_keywords.items():\n                        if kw in query.lower() and tag.lower() in content:\n                            score += 10\n                    return score\n                scored_docs = sorted(docs, key=score_heuristic, reverse=True)\n                return scored_docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Cross-language filtering: detect requested language from query and filter candidates\n        requested_lang = None\n        lang_pattern = re.compile(r'\\b(python|go|java)\\b', re.I)\n        lang_match = lang_pattern.search(query_str)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        # Filter candidates by language tag in metadata or content with fallback to preserve recall\n        if requested_lang:\n            filtered_candidates = []\n            for c in candidates:\n                lang = c.metadata.get(\"language\", \"\").lower()\n                # Accept if language matches or is unknown (for Python default)\n                if lang == requested_lang or (requested_lang == \"python\" and lang == \"unknown\"):\n                    filtered_candidates.append(c)\n            # Relax filtering if too few candidates remain to preserve recall\n            if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n                candidates = filtered_candidates\n            else:\n                # Fallback to original set if too few candidates remain\n                candidates = self.retriever.invoke(query_str)\n\n        # 2.5. Expand query if no good candidates found to increase recall, with improved expansion logic\n        if self.expand_query and (not candidates or len(candidates) < 5):\n            def _expand_query(query: str) -> str:\n                base_expansion = \"google adk API usage example\"\n                lower_query = query.lower()\n                if \"google adk\" not in lower_query:\n                    return query + \" \" + base_expansion\n                additions = [\"tutorial\", \"code sample\", \"example\", \"reference\"]\n                for add in additions:\n                    if add not in lower_query:\n                        return query + \" \" + add\n                return query\n\n            expanded_query = _expand_query(query_str)\n            if expanded_query and expanded_query != query_str:\n                candidates = self.retriever.invoke(expanded_query)\n\n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Summarize parent context titles for brevity and clarity\n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent by language similarly to chunk filtering\n                lang = parent.metadata.get(\"language\", \"\").lower()\n                if requested_lang:\n                    if requested_lang == \"python\" and lang not in (\"python\", \"unknown\"):\n                        continue\n                    elif requested_lang != \"python\" and lang != requested_lang:\n                        continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Use XML style tags for clarity and better chunk separation\n                # Summarize parent content to first 1000 chars to reduce prompt size\n                parent_summary = parent.page_content[:1000].rstrip()\n                if len(parent.page_content) > 1000:\n                    parent_summary += \"\\n...[truncated]...\\n\"\n                context_block += (\n                    f\"<source file='{src}' section='{breadcrumb}'>\\n\"\n                    f\"{parent_summary}\\n\"\n                    f\"</source>\\n\\n\"\n                )\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\"\n            \"6. When quoting code snippets, explicitly label the language for clarity.\\n\"\n            \"7. Use XML-style tags to separate and identify sources clearly.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        try:\n            res = self.llm.invoke(prompt)\n        except Exception as e:\n            print(f\"LLM generation failed: {e}\")\n            return {\"answer\": \"I don't know from the provided documentation\", \"contexts\": final_contexts}\n        finally:\n            gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n    def _expand_query(self, query: str) -> str:\n        \"\"\"Optional method to expand or rephrase query for broader recall.\"\"\"\n        # Simple heuristic: append \"API usage example\" or \"Google ADK\" to query\n        # to help capture more documents\n        if \"google adk\" not in query.lower():\n            return query + \" google adk API usage example\"\n        return query\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n### Inspiration 3 (Score: 0.6114, Type: Alternative)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Normalize label to title case and add language tag explicitly in header for clarity\n            normalized_label = label.title()\n            return f\"\\n\\n#### [{lang_tag}] {normalized_label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\bA2A\\b', breadcrumb, re.I) or re.search(r'\\ba2a\\b', source, re.I):\n                project_tags.append(\"[A2A]\")\n            if re.search(r'\\bWORKFLOW\\b', breadcrumb, re.I) or re.search(r'workflow', source, re.I):\n                project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"[GENERAL]\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging, fallback to parent's language tag\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    # fallback to parent's detected language tag if available\n                    parent_lang = p.metadata.get(\"language\", \"unknown\").capitalize()\n                    lang_tag = f\"[LANG={parent_lang}]\" if parent_lang != \"Unknown\" else \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for language, completeness, and snippet type detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n        \n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            \n            # Mark language if available and if matches requested language\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            lang_flag = \"[LANG_MATCH]\" if requested_lang and lang == requested_lang else \"[LANG_OTHER]\"\n            \n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {lang_flag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Assign a relevance score (0-100) to each chunk for the user query.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            \"- Penalize partial snippets and chunks in other languages.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                # Fallback heuristic: rank by presence of requested language and completeness\n                def score_heuristic(doc):\n                    score = 0\n                    content = doc.page_content.lower()\n                    if requested_lang and f\"[lang={requested_lang}]\" in content:\n                        score += 50\n                    if re.search(r'\\b(class|func|def)\\b', content):\n                        score += 30\n                    if query.lower() in content:\n                        score += 20\n                    return score\n                scored_docs = sorted(docs, key=score_heuristic, reverse=True)\n                return scored_docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use LLM invoke with retry to improve robustness\n        try:\n            res = self.llm.invoke(prompt)\n        except Exception as e:\n            print(f\"LLM generation failed: {e}\")\n            return {\"answer\": \"I don't know from the provided documentation\", \"contexts\": final_contexts}\n        finally:\n            gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n# Current Program\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Normalize label to title case and add language tag explicitly in header for clarity\n            normalized_label = label.title()\n            return f\"\\n\\n#### [{lang_tag}] {normalized_label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            \n            # Improve language detection in reranker by parsing chunk content for LANG tag fallback to metadata\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', d.page_content)\n            lang = lang_tag_search.group(1).lower() if lang_tag_search else d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            \n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Assign a relevance score (0-100) to each chunk for the user query.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            \"- Penalize partial snippets and chunks in other languages.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                # Fallback heuristic: rank by presence of requested language and completeness\n                def score_heuristic(doc):\n                    score = 0\n                    content = doc.page_content.lower()\n                    if requested_lang and f\"[lang={requested_lang}]\" in content:\n                        score += 50\n                    if re.search(r'\\b(class|func|def)\\b', content):\n                        score += 30\n                    if query.lower() in content:\n                        score += 20\n                    return score\n                scored_docs = sorted(docs, key=score_heuristic, reverse=True)\n                return scored_docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            # Use word boundary to avoid partial matches\n            if re.search(r'\\b' + lang + r'\\b', lower_query):\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n\n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Normalize language names for comparison\n            chunk_lang_norm = chunk_lang.lower()\n            requested_lang_norm = requested_lang.lower() if requested_lang else \"\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang_norm == \"python\" and chunk_lang_norm in (\"python\", \"unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang_norm == requested_lang_norm:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                parent_lang_norm = parent_lang.lower()\n                requested_lang_norm = requested_lang.lower() if requested_lang else \"\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang_norm == \"python\" and parent_lang_norm not in (\"python\", \"unknown\"):\n                    continue\n                elif requested_lang_norm != \"python\" and parent_lang_norm != requested_lang_norm:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use LLM invoke with retry to improve robustness\n        try:\n            res = self.llm.invoke(prompt)\n        except Exception as e:\n            print(f\"LLM generation failed: {e}\")\n            return {\"answer\": \"I don't know from the provided documentation\", \"contexts\": final_contexts}\n        finally:\n            gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n=======\n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            # Normalize breadcrumb for case-insensitive matching\n            breadcrumb_lower = breadcrumb.lower()\n            if re.search(r'\\[a2a\\]', breadcrumb_lower): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[workflow\\]', breadcrumb_lower): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n=======\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {p.metadata.get('tags','')} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            # Use word boundary to avoid partial matches\n            if re.search(r'\\b' + lang + r'\\b', lower_query):\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n\n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Normalize language names for comparison\n            chunk_lang_norm = chunk_lang.lower()\n            requested_lang_norm = requested_lang.lower() if requested_lang else \"\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang_norm == \"python\" and chunk_lang_norm in (\"python\", \"unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang_norm == requested_lang_norm:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n=======\n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            # Use word boundary to avoid partial matches\n            if re.search(r'\\b' + lang + r'\\b', lower_query):\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n\n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Normalize language names for comparison\n            chunk_lang_norm = chunk_lang.lower()\n            requested_lang_norm = requested_lang.lower() if requested_lang else \"\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang_norm == \"python\" and chunk_lang_norm in (\"python\", \"unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang_norm == requested_lang_norm:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # Fallback to original to preserve recall if filtered set too small\n            pass\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Use LLM invoke with retry to improve robustness\n        try:\n            res = self.llm.invoke(prompt)\n        except Exception as e:\n            print(f\"LLM generation failed: {e}\")\n            return {\"answer\": \"I don't know from the provided documentation\", \"contexts\": final_contexts}\n        finally:\n            gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n=======\n        # Use LLM invoke with retry to improve robustness\n        try:\n            res = self.llm.invoke(prompt)\n        except Exception as e:\n            print(f\"LLM generation failed: {e}\")\n            return {\"answer\": \"I don't know from the provided documentation\", \"contexts\": final_contexts}\n        finally:\n            gc.collect()\n        \n        # Explicitly call gc after invoke for memory stability\n        gc.collect()\n        return {\"answer\": res.content, \"contexts\": final_contexts}\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Normalize label to title case and add language tag explicitly in header for clarity\n            normalized_label = label.title()\n            return f\"\\n\\n#### [{lang_tag}] {normalized_label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n=======\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Normalize label to title case and add language tag explicitly in header for clarity\n            normalized_label = label.title()\n            # Add a note for multi-language tabs for reranker synergy\n            return f\"\\n\\n#### [{lang_tag}] {normalized_label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n            # Add tagging for better semantic separation\n            prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n            # Detect language from code fences for tagging\n            lang_tag = \"\"\n            lang_match = re.search(r'```(\\w+)', c.page_content)\n            if lang_match:\n                lang = lang_match.group(1).lower()\n                if lang in (\"python\", \"go\", \"java\"):\n                    lang_tag = f\"[LANG={lang.capitalize()}]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n            else:\n                lang_tag = \"[LANG=Unknown]\"\n            # Add tags and breadcrumb into chunk content for better reranking and generation\n            c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n            c.metadata[\"parent_id\"] = parent_id\n            c.metadata[\"source\"] = source\n            child_chunks.append(c)\n=======\n            # Add tagging for better semantic separation\n            prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n            # Detect language from code fences for tagging\n            lang_tag = \"\"\n            lang_match = re.search(r'```(\\w+)', c.page_content)\n            if lang_match:\n                lang = lang_match.group(1).lower()\n                if lang in (\"python\", \"go\", \"java\"):\n                    lang_tag = f\"[LANG={lang.capitalize()}]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n            else:\n                # fallback to parent's language tag if available\n                parent_lang = p.metadata.get(\"language\", \"unknown\").capitalize()\n                lang_tag = f\"[LANG={parent_lang}]\" if parent_lang != \"Unknown\" else \"[LANG=Unknown]\"\n            # Add tags and breadcrumb into chunk content for better reranking and generation\n            c.page_content = f\"{prefix} {lang_tag} {p.metadata.get('tags','')} {breadcrumb}\\n{c.page_content}\"\n            c.metadata[\"parent_id\"] = parent_id\n            c.metadata[\"source\"] = source\n            child_chunks.append(c)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            \n            # Improve language detection in reranker by parsing chunk content for LANG tag fallback to metadata\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', d.page_content)\n            lang = lang_tag_search.group(1).lower() if lang_tag_search else d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            \n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n=======\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            \n            # Improve language detection in reranker by parsing chunk content for LANG tag fallback to metadata\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', d.page_content)\n            lang = lang_tag_search.group(1).lower() if lang_tag_search else d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            \n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            # Add project area tag if available for synergy\n            project_area = d.metadata.get(\"project_area\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} {project_area} Breadcrumb:{breadcrumb}\\n{snippet}\")\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                # Fallback heuristic: rank by presence of requested language and completeness\n                def score_heuristic(doc):\n                    score = 0\n                    content = doc.page_content.lower()\n                    if requested_lang and f\"[lang={requested_lang}]\" in content:\n                        score += 50\n                    if re.search(r'\\b(class|func|def)\\b', content):\n                        score += 30\n                    if query.lower() in content:\n                        score += 20\n                    return score\n                scored_docs = sorted(docs, key=score_heuristic, reverse=True)\n                return scored_docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n=======\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                # Fallback heuristic: rank by presence of requested language and completeness\n                def score_heuristic(doc):\n                    score = 0\n                    content = doc.page_content.lower()\n                    if requested_lang and f\"[lang={requested_lang}]\" in content:\n                        score += 50\n                    if re.search(r'\\b(class|func|def)\\b', content):\n                        score += 30\n                    if query.lower() in content:\n                        score += 20\n                    # Bonus: prefer chunks with project area tags if query contains project area keywords\n                    project_area_keywords = {\n                        \"a2a\": \"[A2A]\",\n                        \"auth\": \"[A2A]\",\n                        \"workflow\": \"[WORKFLOW]\"\n                    }\n                    for kw, tag in project_area_keywords.items():\n                        if kw in query.lower() and tag.lower() in content:\n                            score += 10\n                    return score\n                scored_docs = sorted(docs, key=score_heuristic, reverse=True)\n                return scored_docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n>>>>>>> REPLACE"]}}, "artifacts_json": "{\"raw_scores\": [0.3125, 0.575, 0.6583333333333334, 0.6785714285714286, 0.5700000000000001, 0.645, 0.6833333333333335, 0.6547619047619048, 0.53125, 0.8333333333333333], \"error_details\": [{\"sample\": 3, \"type\": \"code_error\", \"message\": \"[\\\"Instantiation Error on line 7: name 'get_time' is not defined\\\", \\\"Instantiation Error on line 43: name 'get_time' is not defined\\\", \\\"Instantiation Error on line 57: name 'lookup_order_status' is not defined\\\"]\", \"bad_code\": \"# Core abstractions: wrap a function as a tool.\\nfrom google.adk.tools import FunctionTool, ToolContext\\n\\ndef get_time(tool_context: ToolContext) -> dict:\\n    return {\\\"time\\\": \\\"12:00\\\"}\\n\\ntime_tool = FunctionTool(func=get_time, name=\\\"get_time\\\")\\n\\ndef lookup_order_status(order_id: str) -> dict:\\n    \\\"\\\"\\\"Fetches the current status of a customer's order using its ID.\\n\\n    Use this tool ONLY when a user explicitly asks for the status of\\n    a specific order and provides the order ID. Do not use it for\\n    general inquiries.\\n\\n    Args:\\n        order_id: The unique identifier of the order to look up.\\n\\n    Returns:\\n        A dictionary indicating the outcome.\\n        On success, status is 'success' and includes an 'order' dictionary.\\n        On failure, status is 'error' and includes an 'error_message'.\\n        Example success: {'status': 'success', 'order': {'state': 'shipped', 'tracking_number': '1Z9...'}}\\n        Example error: {'status': 'error', 'error_message': 'Order ID not found.'}\\n    \\\"\\\"\\\"\\n    # ... function implementation to fetch status ...\\n    if status_details := fetch_status_from_backend(order_id):\\n        return {\\n            \\\"status\\\": \\\"success\\\",\\n            \\\"order\\\": {\\n                \\\"state\\\": status_details.state,\\n                \\\"tracking_number\\\": status_details.tracking,\\n            },\\n        }\\n    else:\\n        return {\\\"status\\\": \\\"error\\\", \\\"error_message\\\": f\\\"Order ID {order_id} not found.\\\"}\\n\\nfrom google.adk.tools import FunctionTool, ToolContext\\n\\ndef get_time(tool_context: ToolContext) -> dict:\\n    return {\\\"time\\\": \\\"12:00\\\"}\\n\\ntime_tool = FunctionTool(func=get_time, name=\\\"get_time\\\")\\n\\ndef lookup_order_status(order_id: str) -> dict:\\n    \\\"\\\"\\\"Fetches the current status of a customer's order using its ID.\\n\\n    Args:\\n        order_id: The unique identifier of the order to look up.\\n\\n    Returns:\\n        A dictionary with 'status' and either 'order' details or 'error_message'.\\n    \\\"\\\"\\\"\\n    # Implementation here...\\n    pass\\n\\nlookup_order_status_tool = FunctionTool(func=lookup_order_status)\"}, {\"sample\": 4, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 44)\", \"bad_code\": \"import asyncio\\nfrom google.adk.agents import LlmAgent\\nfrom google.adk.runners import Runner\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.genai import types\\n\\n# --- Define constants ---\\nAPP_NAME = \\\"simple_qa_app\\\"\\nUSER_ID = \\\"user_123\\\"\\nSESSION_ID = \\\"session_qa_001\\\"\\nMODEL_NAME = \\\"gemini-2.0-flash\\\"\\n\\n# --- Create the LlmAgent ---\\nagent = LlmAgent(\\n    model=MODEL_NAME,\\n    name=\\\"simple_qa_agent\\\",\\n    instruction=\\\"You are a helpful assistant that answers questions clearly and concisely.\\\"\\n)\\n\\n# --- Set up session service and runner ---\\nsession_service = InMemorySessionService()\\n\\nasync def main():\\n    # Create a session for the user\\n    await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\\n\\n    # Create a runner to run the agent\\n    runner = Runner(agent=agent, app_name=APP_NAME, session_service=session_service)\\n\\n    # Prepare the user query\\n    user_query = \\\"What is the capital of France?\\\"\\n    user_content = types.Content(role='user', parts=[types.Part(text=user_query)])\\n\\n    print(f\\\">>> Sending query to agent: {user_query}\\\")\\n\\n    # Run the agent asynchronously and print the final response\\n    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=user_content):\\n        if event.is_final_response() and event.content and event.content.parts:\\n            print(f\\\"<<< Agent response: {event.content.parts[0].text}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    asyncio.run(main())\\n\\n> from google.adk.agents import LlmAgent\\n> from google.adk.runners import Runner\\n> from google.adk.sessions import InMemorySessionService\\n> from google.genai import types\\n>\\n\\n> agent = LlmAgent(\\n>     model=MODEL_NAME,\\n>     name=\\\"simple_qa_agent\\\",\\n>     instruction=\\\"You are a helpful assistant that answers questions clearly and concisely.\\\"\\n> )\\n>\\n\\n> session_service = InMemorySessionService()\\n>\\n\\n> runner = Runner(agent=agent, app_name=APP_NAME, session_service=session_service)\\n>\\n\\n> async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=user_content):\\n>     if event.is_final_response() and event.content and event.content.parts:\\n>         print(f\\\"<<< Agent response: {event.content.parts[0].text}\\\")\\n>\"}, {\"sample\": 5, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 42)\", \"bad_code\": \"from typing import AsyncGenerator\\nfrom google.adk.agents import BaseAgent, Agent\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom google.adk.events import Event\\n\\nclass RoutingAgent(BaseAgent):\\n    \\\"\\\"\\\"\\n    A custom agent that routes execution to different sub-agents\\n    based on a value stored in the session state.\\n    \\\"\\\"\\\"\\n\\n    # Define sub-agents as instance attributes, e.g., passed during init\\n    def __init__(self, name: str, sub_agent_a: Agent, sub_agent_b: Agent):\\n        super().__init__(name=name, sub_agents=[sub_agent_a, sub_agent_b])\\n        self.sub_agent_a = sub_agent_a\\n        self.sub_agent_b = sub_agent_b\\n\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        # Read routing key from session state\\n        route_key = ctx.session.state.get(\\\"route_key\\\")\\n\\n        if route_key == \\\"A\\\":\\n            # Route to sub_agent_a\\n            async for event in self.sub_agent_a.run_async(ctx):\\n                yield event\\n        elif route_key == \\\"B\\\":\\n            # Route to sub_agent_b\\n            async for event in self.sub_agent_b.run_async(ctx):\\n                yield event\\n        else:\\n            # Default or fallback behavior\\n            # Optionally yield an event or do nothing\\n            # For example, yield a simple event indicating no route matched\\n            from google.adk.events import Event\\n            yield Event(\\n                invocation_id=\\\"routing_agent_no_route\\\",\\n                author=self.name,\\n                content=None,\\n                actions=None,\\n            )\\n\\n> async for event in self.some_sub_agent.run_async(ctx):\\n>     yield event\\n>\\n\\n> previous_result = ctx.session.state.get(\\\"some_key\\\")\\n> if previous_result == \\\"some_value\\\":\\n>     # call a specific sub-agent\\n> else:\\n>     # call another sub-agent\\n>\"}, {\"sample\": 6, \"type\": \"code_error\", \"message\": \"[\\\"Instantiation Error on line 14: name 'update_user_preference' is not defined\\\", \\\"Instantiation Error on line 33: name 'update_user_preference' is not defined\\\"]\", \"bad_code\": \"from google.adk.tools import ToolContext, FunctionTool\\n\\ndef update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n    \\\"\\\"\\\"Updates a user-specific preference.\\\"\\\"\\\"\\n    user_prefs_key = \\\"user:preferences\\\"\\n    # Get current preferences or initialize if none exist\\n    preferences = tool_context.state.get(user_prefs_key, {})\\n    preferences[preference] = value\\n    # Write the updated dictionary back to the state\\n    tool_context.state[user_prefs_key] = preferences\\n    print(f\\\"Tool: Updated user preference '{preference}' to '{value}'\\\")\\n    return {\\\"status\\\": \\\"success\\\", \\\"updated_preference\\\": preference}\\n\\npref_tool = FunctionTool(func=update_user_preference)\\n\\n# In an Agent:\\n# my_agent = Agent(..., tools=[pref_tool])\\n\\n# When the LLM calls update_user_preference(preference='theme', value='dark', ...):\\n# The tool_context.state will be updated, and the change will be part of the\\n# resulting tool response event's actions.state_delta.\\n\\nfrom google.adk.tools import ToolContext, FunctionTool\\n\\ndef update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n    user_prefs_key = \\\"user:preferences\\\"\\n    preferences = tool_context.state.get(user_prefs_key, {})\\n    preferences[preference] = value\\n    tool_context.state[user_prefs_key] = preferences\\n    print(f\\\"Tool: Updated user preference '{preference}' to '{value}'\\\")\\n    return {\\\"status\\\": \\\"success\\\", \\\"updated_preference\\\": preference}\\n\\npref_tool = FunctionTool(func=update_user_preference)\"}, {\"sample\": 7, \"type\": \"code_error\", \"message\": \"[\\\"Import Error on line 108: No module named 'google.adk.runner'\\\", \\\"Import Error on line 134: No module named 'google.adk.runner'\\\", \\\"Instantiation Error on line 30: name 'GEMINI_MODEL' is not defined\\\", \\\"Instantiation Error on line 45: name 'GEMINI_MODEL' is not defined\\\", \\\"Instantiation Error on line 70: name 'GEMINI_MODEL' is not defined\\\", \\\"Instantiation Error on line 97: name 'code_writer_agent' is not defined\\\", \\\"Instantiation Error on line 117: name 'InMemoryRunner' is not defined\\\", \\\"Instantiation Error on line 217: name 'InMemoryRunner' is not defined\\\"]\", \"bad_code\": \"# Conceptual Code: Sequential Data Pipeline\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\n\\nvalidator = LlmAgent(name=\\\"ValidateInput\\\", instruction=\\\"Validate the input.\\\", output_key=\\\"validation_status\\\")\\nprocessor = LlmAgent(name=\\\"ProcessData\\\", instruction=\\\"Process data if {validation_status} is 'valid'.\\\", output_key=\\\"result\\\")\\nreporter = LlmAgent(name=\\\"ReportResult\\\", instruction=\\\"Report the result from {result}.\\\")\\n\\ndata_pipeline = SequentialAgent(\\n    name=\\\"DataPipeline\\\",\\n    sub_agents=[validator, processor, reporter]\\n)\\n# validator runs -> saves to state['validation_status']\\n# processor runs -> reads state['validation_status'], saves to state['result']\\n# reporter runs -> reads state['result']\\n\\n# Conceptual Example: Sequential Pipeline\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\n\\nstep1 = LlmAgent(name=\\\"Step1_Fetch\\\", output_key=\\\"data\\\") # Saves output to state['data']\\nstep2 = LlmAgent(name=\\\"Step2_Process\\\", instruction=\\\"Process data from {data}.\\\")\\n\\npipeline = SequentialAgent(name=\\\"MyPipeline\\\", sub_agents=[step1, step2])\\n# When pipeline runs, Step2 can access the state['data'] set by Step1.\\n\\n# Part of agent.py --> Follow https://google.github.io/adk-docs/get-started/quickstart/ to learn the setup\\n\\n# --- 1. Define Sub-Agents for Each Pipeline Stage ---\\n\\n# Code Writer Agent\\ncode_writer_agent = LlmAgent(\\n    name=\\\"CodeWriterAgent\\\",\\n    model=GEMINI_MODEL,\\n    instruction=\\\"\\\"\\\"You are a Python Code Generator.\\nBased *only* on the user's request, write Python code that fulfills the requirement.\\nOutput *only* the complete Python code block, enclosed in triple backticks (\\n\\n).\\nDo not add any other text before or after the code block.\\n\\\"\\\"\\\",\\n    description=\\\"Writes initial Python code based on a specification.\\\",\\n    output_key=\\\"generated_code\\\" # Stores output in state['generated_code']\\n)\\n\\n# Code Reviewer Agent\\ncode_reviewer_agent = LlmAgent(\\n    name=\\\"CodeReviewerAgent\\\",\\n    model=GEMINI_MODEL,\\n    instruction=\\\"\\\"\\\"You are an expert Python Code Reviewer.\\nYour task is to provide constructive feedback on the provided code.\\n\\n**Code to Review:**\\n\\n**Review Criteria:**\\n1.  **Correctness:** Does the code work as intended? Are there logic errors?\\n2.  **Readability:** Is the code clear and easy to understand? Follows PEP 8 style guidelines?\\n3.  **Efficiency:** Is the code reasonably efficient? Any obvious performance bottlenecks?\\n4.  **Edge Cases:** Does the code handle potential edge cases or invalid inputs gracefully?\\n5.  **Best Practices:** Does the code follow common Python best practices?\\n\\n**Output:**\\nProvide your feedback as a concise, bulleted list. Focus on the most important points for improvement.\\nIf the code is excellent and requires no changes, simply state: \\\"No major issues found.\\\"\\nOutput *only* the review comments or the \\\"No major issues\\\" statement.\\n\\\"\\\"\\\",\\n    description=\\\"Reviews code and provides feedback.\\\",\\n    output_key=\\\"review_comments\\\", # Stores output in state['review_comments']\\n)\\n\\n# Code Refactorer Agent\\ncode_refactorer_agent = LlmAgent(\\n    name=\\\"CodeRefactorerAgent\\\",\\n    model=GEMINI_MODEL,\\n    instruction=\\\"\\\"\\\"You are a Python Code Refactoring AI.\\nYour goal is to improve the given Python code based on the provided review comments.\\n\\n**Original Code:**\\n\\n**Review Comments:**\\n{review_comments}\\n\\n**Task:**\\nCarefully apply the suggestions from the review comments to refactor the original code.\\nIf the review comments state \\\"No major issues found,\\\" return the original code unchanged.\\nEnsure the final code is complete, functional, and includes necessary imports and docstrings.\\n\\n**Output:**\\nOutput *only* the final, refactored Python code block, enclosed in triple backticks (\\n\\n).\\nDo not add any other text before or after the code block.\\n\\\"\\\"\\\",\\n    description=\\\"Refactors code based on review comments.\\\",\\n    output_key=\\\"refactored_code\\\", # Stores output in state['refactored_code']\\n)\\n\\n# --- 2. Create the SequentialAgent ---\\ncode_pipeline_agent = SequentialAgent(\\n    name=\\\"CodePipelineAgent\\\",\\n    sub_agents=[code_writer_agent, code_reviewer_agent, code_refactorer_agent],\\n    description=\\\"Executes a sequence of code writing, reviewing, and refactoring.\\\",\\n)\\n\\n# For ADK tools compatibility, the root agent must be named `root_agent`\\nroot_agent = code_pipeline_agent\\n\\n# Running the SequentialAgent pipeline with InMemoryRunner\\n\\nfrom google.adk.runner import InMemoryRunner\\nfrom google.adk.sessions import Session\\nfrom google.genai.types import Content, Part\\nfrom google.adk.events import Event\\n\\nUSER_ID = \\\"user_123\\\"\\nAPP_NAME = \\\"CodePipelineAgent\\\"\\n\\n# Create the runner with the root agent\\nrunner = InMemoryRunner(root_agent, APP_NAME)\\n\\n# Create a session for the user\\nsession = runner.session_service().create_session(APP_NAME, USER_ID).blocking_get()\\n\\n# Prepare user input as Content\\nuser_input = Content.from_parts(Part.from_text(\\\"Write a Python function to compute Fibonacci numbers.\\\"))\\n\\n# Run the agent asynchronously\\nevent_stream = runner.run_async(USER_ID, session.id(), user_input)\\n\\n# Collect and print final response from the event stream\\nfor event in event_stream.blocking_iter():\\n    if event.final_response():\\n        print(event.stringify_content())\\n\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\nfrom google.adk.runner import InMemoryRunner\\nfrom google.adk.sessions import Session\\nfrom google.genai.types import Content, Part\\n\\n# --- Define Sub-Agents ---\\n\\ncode_writer_agent = LlmAgent(\\n    name=\\\"CodeWriterAgent\\\",\\n    model=\\\"GEMINI_MODEL\\\",  # Replace with actual model instance or identifier\\n    instruction=\\\"\\\"\\\"You are a Python Code Generator.\\nBased *only* on the user's request, write Python code that fulfills the requirement.\\nOutput *only* the complete Python code block, enclosed in triple backticks (\\n\\n).\\nDo not add any other text before or after the code block.\\n\\\"\\\"\\\",\\n    description=\\\"Writes initial Python code based on a specification.\\\",\\n    output_key=\\\"generated_code\\\"\\n)\\n\\ncode_reviewer_agent = LlmAgent(\\n    name=\\\"CodeReviewerAgent\\\",\\n    model=\\\"GEMINI_MODEL\\\",\\n    instruction=\\\"\\\"\\\"You are an expert Python Code Reviewer.\\nYour task is to provide constructive feedback on the provided code.\\n\\n**Code to Review:**\\n\\n**Review Criteria:**\\n1.  **Correctness:** Does the code work as intended? Are there logic errors?\\n2.  **Readability:** Is the code clear and easy to understand? Follows PEP 8 style guidelines?\\n3.  **Efficiency:** Is the code reasonably efficient? Any obvious performance bottlenecks?\\n4.  **Edge Cases:** Does the code handle potential edge cases or invalid inputs gracefully?\\n5.  **Best Practices:** Does the code follow common Python best practices?\\n\\n**Output:**\\nProvide your feedback as a concise, bulleted list. Focus on the most important points for improvement.\\nIf the code is excellent and requires no changes, simply state: \\\"No major issues found.\\\"\\nOutput *only* the review comments or the \\\"No major issues\\\" statement.\\n\\\"\\\"\\\",\\n    description=\\\"Reviews code and provides feedback.\\\",\\n    output_key=\\\"review_comments\\\"\\n)\\n\\ncode_refactorer_agent = LlmAgent(\\n    name=\\\"CodeRefactorerAgent\\\",\\n    model=\\\"GEMINI_MODEL\\\",\\n    instruction=\\\"\\\"\\\"You are a Python Code Refactoring AI.\\nYour goal is to improve the given Python code based on the provided review comments.\\n\\n**Original Code:**\\n\\n**Review Comments:**\\n{review_comments}\\n\\n**Task:**\\nCarefully apply the suggestions from the review comments to refactor the original code.\\nIf the review comments state \\\"No major issues found,\\\" return the original code unchanged.\\nEnsure the final code is complete, functional, and includes necessary imports and docstrings.\\n\\n**Output:**\\nOutput *only* the final, refactored Python code block, enclosed in triple backticks (\\n\\n).\\nDo not add any other text before or after the code block.\\n\\\"\\\"\\\",\\n    description=\\\"Refactors code based on review comments.\\\",\\n    output_key=\\\"refactored_code\\\"\\n)\\n\\n# --- Create SequentialAgent Pipeline ---\\n\\ncode_pipeline_agent = SequentialAgent(\\n    name=\\\"CodePipelineAgent\\\",\\n    sub_agents=[code_writer_agent, code_reviewer_agent, code_refactorer_agent],\\n    description=\\\"Executes a sequence of code writing, reviewing, and refactoring.\\\"\\n)\\n\\n# --- Run the Pipeline with InMemoryRunner ---\\n\\nUSER_ID = \\\"user_123\\\"\\nAPP_NAME = \\\"CodePipelineAgent\\\"\\n\\nrunner = InMemoryRunner(code_pipeline_agent, APP_NAME)\\n\\nsession = runner.session_service().create_session(APP_NAME, USER_ID).blocking_get()\\n\\nuser_input = Content.from_parts(Part.from_text(\\\"Write a Python function to compute Fibonacci numbers.\\\"))\\n\\nevent_stream = runner.run_async(USER_ID, session.id(), user_input)\\n\\nfor event in event_stream.blocking_iter():\\n    if event.final_response():\\n        print(event.stringify_content())\"}, {\"sample\": 8, \"type\": \"code_error\", \"message\": \"[\\\"Instantiation Error on line 23: name 'CheckConditionAgent' is not defined\\\"]\", \"bad_code\": \"from google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\n# Custom agent to check a condition and escalate to exit the loop early\\nclass CheckConditionAgent(BaseAgent):\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        # Example condition: check if state['status'] == 'completed'\\n        status = ctx.session.state.get(\\\"status\\\", \\\"pending\\\")\\n        should_stop = (status == \\\"completed\\\")\\n        # Emit an event with escalate=True to exit the LoopAgent early if condition met\\n        yield Event(author=self.name, actions=EventActions(escalate=should_stop))\\n\\n# Agent that performs some processing and updates state['status']\\nprocessing_agent = LlmAgent(\\n    name=\\\"ProcessingAgent\\\",\\n    instruction=\\\"Perform a step and update state['status'] accordingly.\\\",\\n    output_key=\\\"status\\\"\\n)\\n\\n# LoopAgent that runs processing_agent and then the condition checker repeatedly\\nretry_loop = LoopAgent(\\n    name=\\\"RetryLoop\\\",\\n    max_iterations=10,\\n    sub_agents=[processing_agent, CheckConditionAgent(name=\\\"ConditionChecker\\\")]\\n)\\n\\n# When retry_loop runs, it executes processing_agent then ConditionChecker repeatedly\\n# The loop exits early if ConditionChecker yields an event with escalate=True\\n# or after max_iterations is reached.\"}]}", "artifact_dir": null, "embedding": null}