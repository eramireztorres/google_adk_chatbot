{"id": "07b4e123-de99-4069-b68f-e13e522d6463", "code": "import os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if found in breadcrumb or source filename\n            tags = []\n            if re.search(r'\\bA2A\\b', breadcrumb, re.I) or 'a2a' in source.lower():\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bWORKFLOW\\b', breadcrumb, re.I) or 'workflow' in source.lower():\n                tags.append(\"[WORKFLOW]\")\n            if tags:\n                p.metadata[\"tags\"] = \" \".join(tags)\n            else:\n                p.metadata[\"tags\"] = \"[GENERAL]\"\n            \n            # Add language metadata for parent doc if code fence exists\n            lang_match = re.search(r'```(\\w+)', p.page_content)\n            if lang_match:\n                p.metadata[\"language\"] = lang_match.group(1).lower()\n            else:\n                p.metadata[\"language\"] = \"unknown\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation and language tags if present\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                \n                # Add language tag from breadcrumb or parent metadata if detected\n                lang_tag = \"\"\n                lang_matches = re.findall(r'#### (\\w+) Implementation', c.page_content)\n                if lang_matches:\n                    lang_tag = f\"[{lang_matches[0].upper()}]\"\n                elif \"language\" in p.metadata and p.metadata[\"language\"] != \"unknown\":\n                    lang_tag = f\"[{p.metadata['language'].upper()}]\"\n                \n                # Add tags from parent metadata\n                tag_str = p.metadata.get(\"tags\", \"\")\n                \n                c.page_content = f\"{prefix} {lang_tag} {tag_str} {breadcrumb}\\n{c.page_content}\".strip()\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = p.metadata[\"language\"]\n                c.metadata[\"tags\"] = tag_str\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with improved prompt for multi-language and completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            # Indicate if chunk likely contains a full API def or partial snippet heuristically\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'(class|func|def)\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            lang_tag = \"\"\n            # Extract language tag from metadata if available\n            if \"language\" in d.metadata and d.metadata[\"language\"] != \"unknown\":\n                lang_tag = f\"[{d.metadata['language'].upper()}]\"\n            packed.append(f\"[{i}] {completeness_tag} {lang_tag} {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Task: Given a user query and a list of documentation chunks, assign a relevance score (0-100) to each chunk.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Output JSON with format: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Cross-language filtering: detect requested language from query and filter candidates\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', query_lower):\n                requested_lang = lang\n                break\n        \n        if requested_lang:\n            filtered_candidates = [c for c in candidates if c.metadata.get(\"language\", \"unknown\") == requested_lang]\n            # If filtered too strictly, fallback to original candidates to preserve recall\n            if len(filtered_candidates) >= 3:\n                candidates = filtered_candidates\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting with XML-style tags and language filtering\n        final_contexts = []\n        seen_parents = set()\n        context_blocks = []\n        \n        def chunk_language_filter(chunk_text: str) -> bool:\n            # Simple heuristic: check for language tags or code fences\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            # Also accept if chunk mentions requested language in prefix or breadcrumb\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # If chunk is code and no explicit language, accept by default\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n        \n        for c in top_chunks:\n            if not chunk_language_filter(c.page_content):\n                continue\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent by language as well\n                if not chunk_language_filter(parent.page_content):\n                    continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_blocks.append(f\"<source file='{src}' section='{breadcrumb}'>\\n{parent.page_content}\\n</source>\")\n        \n        context_block = \"\\n\\n\".join(context_blocks)\n        \n        # 5. Generation (Strict Grounding) with improved prompt encouraging Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "8ae3bfc7-36b0-45af-9ba6-681af9284a70", "generation": 2, "timestamp": 1769718207.6626992, "iteration_found": 7, "metrics": {"combined_score": 0.41022619047619047, "num_samples": 10}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 9 lines with 15 lines\nChange 2: Replace 31 lines with 42 lines\nChange 3: Replace 35 lines with 39 lines\nChange 4: Replace 69 lines with 76 lines", "parent_metrics": {"combined_score": 0.4947261904761905, "num_samples": 10}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer for Google ADK documentation. Improve the code to maximize RAG-pipeline performance.\n\nThe current architecture uses a **Parent-Child Retrieval** system with **Hybrid Search** and **LLM Reranking**.\n\n### High-Leverage Evolution Points:\n\n1. **Hyperparameter Synergy**: Optimize the balance between `chunk_size` (for retrieval precision), `top_k_vector/bm25` (for recall), and `weight_vector/bm25`. The documentation has many exact API names which may favor BM25, while conceptual \"how-to\" queries favor Vector.\n2. **Reranker Prompt Engineering**: The `_llm_rerank` prompt is critical. Evolve it to better identify relevance, handle multi-language snippets, or detect if a chunk contains a complete API definition vs. a partial snippet.\n3. **Metadata-Rich Ingestion**: Enhance `_chunk_document` to include more context in `breadcrumb` and `tagging`. Consider adding language-specific tags or project-area identifiers (e.g. `[A2A]`, `[WORKFLOW]`) to help the reranker and generator.\n4. **Context Assembly & Prompting**: Optimize the `context_block` formatting in `query`. Experiment with summarizing parent context or using XML-style tags to separate sources. Refine the final generation prompt for better \"Quote then Explain\" adherence.\n5. **Cross-Language Filtering**: Implement logic to detect the user's requested language and prioritize or filter retrieved chunks to reduce cross-language noise.\n\n**STRICT RULES**:\n- **Fork-Safety**: Avoid any top-level imports of `langchain`, `agno`, or `openai`. Use lazy imports inside methods.\n- **Compatibility**: Ensure `evaluate_rag(docs_path, query)` maintains its signature.\n- **Cleanup**: Always call `gc.collect()` after LLM calls or retrieval to maintain memory stability.\n\n**Example Improvement (Reranker Logic)**:\n```python\n# You can evolve the reranker prompt to be more specific:\nprompt = (\n    \"Role: ADK Expert Reranker. \"\n    \"Goal: Select chunks that provide EXECUTABLE code for the query. \"\n    f\"Query: {query}\\n\"\n    \"Chunks: ... \"\n)\n```\n- Maintain `evaluate_rag(docs_path, query)` signature and return keys: `answer`, `contexts`.\n- Keep `RAGSystem` class structure and lazy-loading in `__init__`.\n", "user": "# Current Program Information\n- Fitness: 0.4947\n- Feature coordinates: combined_score=0.49\n- Focus areas: - Fitness unchanged at 0.4947\n- Exploring combined_score=0.49 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### raw_scores\n```\n[0.29583333333333334, 0.51, 0.5850000000000001, 0.5, 0.5, 0.5, 0.475, 0.5314285714285715, 0.5, 0.55]\n```\n\n### error_details\n```\n[{'sample': 4, 'type': 'code_error', 'message': '[\"Import Error on line 1: No module named \\'google.ai\\'\", \"Import Error on line 2: No module named \\'google.ai\\'\"]', 'bad_code': 'from google.ai.generative import TextGenerationClient\\nfrom google.ai.generative.agent import LlmAgent\\n\\ndef main():\\n    # Initialize the TextGenerationClient\\n    client = TextGenerationClient()\\n\\n    # Create an LlmAgent instance with the client\\n    agent = LlmAgent(client=client)\\n\\n    # Define the question to ask the agent\\n    question = \"What is the capital of France?\"\\n\\n    # Run the agent to get the answer\\n    answer = agent.run(question)\\n\\n    # Print the answer\\n    print(\"Answer:\", answer)\\n\\nif __name__ == \"__main__\":\\n    main()'}, {'sample': 7, 'type': 'code_error', 'message': 'Syntax Error: invalid syntax (<unknown>, line 33)', 'bad_code': 'from google.adk.agents import SequentialAgent, LlmAgent\\nfrom google.adk.runtime import Runner\\n\\n# Define three LlmAgents in sequence, each passing data via output_key\\nvalidator = LlmAgent(\\n    name=\"ValidateInput\",\\n    instruction=\"Validate the input.\",\\n    output_key=\"validation_status\"\\n)  # Saves output to state[\\'validation_status\\']\\n\\nprocessor = LlmAgent(\\n    name=\"ProcessData\",\\n    instruction=\"Process data if {validation_status} is \\'valid\\'.\",\\n    output_key=\"result\"\\n)  # Reads state[\\'validation_status\\'], saves to state[\\'result\\']\\n\\nreporter = LlmAgent(\\n    name=\"ReportResult\",\\n    instruction=\"Report the result from {result}.\"\\n)  # Reads state[\\'result\\']\\n\\n# Create the SequentialAgent pipeline with the three sub_agents\\ndata_pipeline = SequentialAgent(\\n    name=\"DataPipeline\",\\n    sub_agents=[validator, processor, reporter]\\n)\\n\\n# Run the pipeline using the Runner\\nif __name__ == \"__main__\":\\n    runner = Runner(agent=data_pipeline)\\n    runner.run()\\n\\n> validator = LlmAgent(name=\"ValidateInput\", instruction=\"Validate the input.\", output_key=\"validation_status\")\\n> processor = LlmAgent(name=\"ProcessData\", instruction=\"Process data if {validation_status} is \\'valid\\'.\", output_key=\"result\")\\n> reporter = LlmAgent(name=\"ReportResult\", instruction=\"Report the result from {result}.\")\\n>  \\n> data_pipeline = SequentialAgent(\\n> name=\"DataPipeline\",\\n> sub_agents=[validator, processor, reporter]\\n> )\\n> # validator runs -> saves to state[\\'validation_status\\']\\n> # processor runs -> reads state[\\'validation_status\\'], saves to state[\\'result\\']\\n> # reporter runs -> reads state[\\'result\\']\\n>'}, {'sample': 9, 'type': 'code_error', 'message': '[\"Import Error on line 1: No module named \\'google.ai\\'\", \"Import Error on line 2: No module named \\'google.ai\\'\", \"Import Error on line 3: No module named \\'google.ai\\'\", \"Import Error on line 4: No module named \\'google.ai\\'\"]', 'bad_code': 'from google.ai.generativeai import GenerativeAIClient\\nfrom google.ai.generativeai.types import GenerateTextRequest, TextPrompt\\nfrom google.ai.generativeai.types import AnalyzeToneRequest\\nfrom google.ai.generativeai.types import Tone\\n\\nclass CustomStoryAgent:\\n    def __init__(self):\\n        self.client = GenerativeAIClient()\\n\\n    async def _run_async_impl(self, prompt: str) -> str:\\n        # Step 1: Generate story\\n        generate_request = GenerateTextRequest(\\n            prompt=TextPrompt(text=prompt)\\n        )\\n        response = await self.client.generate_text(request=generate_request)\\n        story = response.text\\n\\n        # Step 2: Check tone\\n        tone_request = AnalyzeToneRequest(text=story)\\n        tone_response = await self.client.analyze_tone(request=tone_request)\\n        tone = tone_response.tone\\n\\n        # Step 3: Regenerate if tone is negative\\n        if tone == Tone.NEGATIVE:\\n            regenerate_request = GenerateTextRequest(\\n                prompt=TextPrompt(text=prompt + \" Please make the tone positive.\")\\n            )\\n            regenerate_response = await self.client.generate_text(request=regenerate_request)\\n            story = regenerate_response.text\\n\\n        return story'}]\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 9 lines with 16 lines\nChange 2: Replace 34 lines with 39 lines\nChange 3: Replace 32 lines with 63 lines\nChange 4: Replace 37 lines with 59 lines\n- Metrics: combined_score: 0.4947, num_samples: 10.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 9 lines with 15 lines\nChange 2: Replace 34 lines with 40 lines\nChange 3: Replace 37 lines with 53 lines\nChange 4: Replace 38 lines with 58 lines\n- Metrics: combined_score: 0.5395, num_samples: 10.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.5461, num_samples: 10.0000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.5461)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            content = d.page_content[:1500]\n            packed.append(f\"[{i}] {content}\")\n        \n        prompt = (\n            \"You are a reranker for Google ADK documentation. \"\n            \"Given a user query and a list of chunks, score each chunk from 0 to 100 based on its relevance. \"\n            \"Prioritize chunks that contain exact API names or code examples for the requested task. \"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_block += f\"--- SOURCE: {src} | SECTION: {breadcrumb} ---\\n{parent.page_content}\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5461), Performs well on num_samples (10.0000)\n\n### Program 2 (Score: 0.5395)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add language and project-area tags for better filtering and relevance\n            lang_match = re.search(r'```(\\w+)', p.page_content)\n            language = lang_match.group(1).lower() if lang_match else \"unknown\"\n            p.metadata[\"language\"] = language\n            \n            # Heuristic project-area tagging based on breadcrumbs or source file path\n            if re.search(r'a2a|auth', breadcrumb, re.I) or re.search(r'a2a|auth', source, re.I):\n                p.metadata[\"project_area\"] = \"[A2A]\"\n            elif re.search(r'workflow', breadcrumb, re.I) or re.search(r'workflow', source, re.I):\n                p.metadata[\"project_area\"] = \"[WORKFLOW]\"\n            else:\n                p.metadata[\"project_area\"] = \"[GENERAL]\"\n                \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb} {p.metadata['project_area']}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = language\n                c.metadata[\"project_area\"] = p.metadata[\"project_area\"]\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags and breadcrumb in reranking context for better relevance detection\n            meta = d.metadata.get(\"breadcrumb\", \"\") + \" \" + d.metadata.get(\"source\", \"\")\n            content = d.page_content[:1400]\n            packed.append(f\"[{i}] Metadata: {meta}\\nContent: {content}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk 0-100 on relevance to the query.\\n\"\n            \"Prioritize:\\n\"\n            \"- chunks containing exact API names matching the query,\\n\"\n            \"- chunks with executable code snippets,\\n\"\n            \"- chunks containing complete API definitions rather than partial fragments.\\n\"\n            \"- consider multi-language code snippets and mark those matching the user's requested language.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Cross-language filtering: detect requested language from query and filter candidates\n        requested_lang = None\n        lang_pattern = re.compile(r'\\b(python|go|java)\\b', re.I)\n        lang_match = lang_pattern.search(query_str)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n        \n        if requested_lang:\n            candidates = [c for c in candidates if c.metadata.get(\"language\", \"\") == requested_lang]\n            # If filtered too strictly, fallback to original set\n            if len(candidates) < 3:\n                candidates = self.retriever.invoke(query_str)\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Summarize parent context titles for brevity and clarity\n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Use XML style tags for clarity and better chunk separation\n                context_block += (\n                    f\"<source file='{src}' section='{breadcrumb}'>\\n\"\n                    f\"{parent.page_content}\\n\"\n                    f\"</source>\\n\\n\"\n                )\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5395), Performs well on num_samples (10.0000)\n\n### Program 3 (Score: 0.4947)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if found in breadcrumb or source filename\n            tags = []\n            if re.search(r'\\bA2A\\b', breadcrumb, re.I) or 'a2a' in source.lower():\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bWORKFLOW\\b', breadcrumb, re.I) or 'workflow' in source.lower():\n                tags.append(\"[WORKFLOW]\")\n            if tags:\n                p.metadata[\"tags\"] = \" \".join(tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation and language tags if present\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                \n                # Add language tag from breadcrumb if detected\n                lang_tag = \"\"\n                lang_matches = re.findall(r'#### (\\w+) Implementation', c.page_content)\n                if lang_matches:\n                    lang_tag = f\"[{lang_matches[0].upper()}]\"\n                \n                # Add tags from parent metadata\n                tag_str = p.metadata.get(\"tags\", \"\")\n                \n                c.page_content = f\"{prefix} {lang_tag} {tag_str} {breadcrumb}\\n{c.page_content}\".strip()\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with improved prompt for multi-language and completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            # Indicate if chunk likely contains a full API def or partial snippet heuristically\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'class\\s+\\w+|func\\s+\\w+|def\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            packed.append(f\"[{i}] {completeness_tag} {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Task: Given a user query and a list of documentation chunks, assign a relevance score (0-100) to each chunk.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Output JSON with format: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting with XML-style tags and language filtering\n        final_contexts = []\n        seen_parents = set()\n        context_blocks = []\n        \n        # Detect requested language from query (case-insensitive)\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', query_lower):\n                requested_lang = lang\n                break\n        \n        def chunk_language_filter(chunk_text: str) -> bool:\n            # Simple heuristic: check for language tags or code fences\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            # Also accept if chunk mentions requested language in prefix or breadcrumb\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # If chunk is code and no explicit language, accept by default\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n        \n        for c in top_chunks:\n            if not chunk_language_filter(c.page_content):\n                continue\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent by language as well\n                if not chunk_language_filter(parent.page_content):\n                    continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_blocks.append(f\"<source file='{src}' section='{breadcrumb}'>\\n{parent.page_content}\\n</source>\")\n        \n        context_block = \"\\n\\n\".join(context_blocks)\n        \n        # 4. Generation (Strict Grounding) with improved prompt encouraging Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.4947), Performs well on num_samples (10.0000)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.5461, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            content = d.page_content[:1500]\n            packed.append(f\"[{i}] {content}\")\n        \n        prompt = (\n            \"You are a reranker for Google ADK documentation. \"\n            \"Given a user query and a list of chunks, score each chunk from 0 to 100 based on its relevance. \"\n            \"Prioritize chunks that contain exact API names or code examples for the requested task. \"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_block += f\"--- SOURCE: {src} | SECTION: {breadcrumb} ---\\n{parent.page_content}\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n# Current Program\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if found in breadcrumb or source filename\n            tags = []\n            if re.search(r'\\bA2A\\b', breadcrumb, re.I) or 'a2a' in source.lower():\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bWORKFLOW\\b', breadcrumb, re.I) or 'workflow' in source.lower():\n                tags.append(\"[WORKFLOW]\")\n            if tags:\n                p.metadata[\"tags\"] = \" \".join(tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation and language tags if present\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                \n                # Add language tag from breadcrumb if detected\n                lang_tag = \"\"\n                lang_matches = re.findall(r'#### (\\w+) Implementation', c.page_content)\n                if lang_matches:\n                    lang_tag = f\"[{lang_matches[0].upper()}]\"\n                \n                # Add tags from parent metadata\n                tag_str = p.metadata.get(\"tags\", \"\")\n                \n                c.page_content = f\"{prefix} {lang_tag} {tag_str} {breadcrumb}\\n{c.page_content}\".strip()\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with improved prompt for multi-language and completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            # Indicate if chunk likely contains a full API def or partial snippet heuristically\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'class\\s+\\w+|func\\s+\\w+|def\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            packed.append(f\"[{i}] {completeness_tag} {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Task: Given a user query and a list of documentation chunks, assign a relevance score (0-100) to each chunk.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Output JSON with format: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting with XML-style tags and language filtering\n        final_contexts = []\n        seen_parents = set()\n        context_blocks = []\n        \n        # Detect requested language from query (case-insensitive)\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', query_lower):\n                requested_lang = lang\n                break\n        \n        def chunk_language_filter(chunk_text: str) -> bool:\n            # Simple heuristic: check for language tags or code fences\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            # Also accept if chunk mentions requested language in prefix or breadcrumb\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # If chunk is code and no explicit language, accept by default\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n        \n        for c in top_chunks:\n            if not chunk_language_filter(c.page_content):\n                continue\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent by language as well\n                if not chunk_language_filter(parent.page_content):\n                    continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_blocks.append(f\"<source file='{src}' section='{breadcrumb}'>\\n{parent.page_content}\\n</source>\")\n        \n        context_block = \"\\n\\n\".join(context_blocks)\n        \n        # 4. Generation (Strict Grounding) with improved prompt encouraging Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n=======\n        # Adjust chunk size smaller for more precise but keep overlap moderate\n        self.chunk_size = 400\n        self.chunk_overlap = 120\n        \n        # Increase top_k for vector to boost recall on conceptual queries, keep BM25 same\n        self.top_k_vector = 30\n        self.top_k_bm25 = 20\n        \n        # Adjust weights to favor BM25 slightly due to many exact API names in docs\n        self.weight_vector = 0.4\n        self.weight_bm25 = 0.6\n        \n        self.rerank_top_n = 10  # rerank more candidates for better final precision\n        self.temperature = 0.0\n        self.expand_query = False \n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n            # Add project-area tags if found in breadcrumb or source filename\n            tags = []\n            if re.search(r'\\bA2A\\b', breadcrumb, re.I) or 'a2a' in source.lower():\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bWORKFLOW\\b', breadcrumb, re.I) or 'workflow' in source.lower():\n                tags.append(\"[WORKFLOW]\")\n            if tags:\n                p.metadata[\"tags\"] = \" \".join(tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation and language tags if present\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                \n                # Add language tag from breadcrumb if detected\n                lang_tag = \"\"\n                lang_matches = re.findall(r'#### (\\w+) Implementation', c.page_content)\n                if lang_matches:\n                    lang_tag = f\"[{lang_matches[0].upper()}]\"\n                \n                # Add tags from parent metadata\n                tag_str = p.metadata.get(\"tags\", \"\")\n                \n                c.page_content = f\"{prefix} {lang_tag} {tag_str} {breadcrumb}\\n{c.page_content}\".strip()\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n=======\n            # Add project-area tags if found in breadcrumb or source filename\n            tags = []\n            if re.search(r'\\bA2A\\b', breadcrumb, re.I) or 'a2a' in source.lower():\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bWORKFLOW\\b', breadcrumb, re.I) or 'workflow' in source.lower():\n                tags.append(\"[WORKFLOW]\")\n            if tags:\n                p.metadata[\"tags\"] = \" \".join(tags)\n            else:\n                p.metadata[\"tags\"] = \"[GENERAL]\"\n            \n            # Add language metadata for parent doc if code fence exists\n            lang_match = re.search(r'```(\\w+)', p.page_content)\n            if lang_match:\n                p.metadata[\"language\"] = lang_match.group(1).lower()\n            else:\n                p.metadata[\"language\"] = \"unknown\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation and language tags if present\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                \n                # Add language tag from breadcrumb or parent metadata if detected\n                lang_tag = \"\"\n                lang_matches = re.findall(r'#### (\\w+) Implementation', c.page_content)\n                if lang_matches:\n                    lang_tag = f\"[{lang_matches[0].upper()}]\"\n                elif \"language\" in p.metadata and p.metadata[\"language\"] != \"unknown\":\n                    lang_tag = f\"[{p.metadata['language'].upper()}]\"\n                \n                # Add tags from parent metadata\n                tag_str = p.metadata.get(\"tags\", \"\")\n                \n                c.page_content = f\"{prefix} {lang_tag} {tag_str} {breadcrumb}\\n{c.page_content}\".strip()\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = p.metadata[\"language\"]\n                c.metadata[\"tags\"] = tag_str\n                child_chunks.append(c)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            # Indicate if chunk likely contains a full API def or partial snippet heuristically\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'class\\s+\\w+|func\\s+\\w+|def\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            packed.append(f\"[{i}] {completeness_tag} {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Task: Given a user query and a list of documentation chunks, assign a relevance score (0-100) to each chunk.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Output JSON with format: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n=======\n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            # Indicate if chunk likely contains a full API def or partial snippet heuristically\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'(class|func|def)\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            lang_tag = \"\"\n            # Extract language tag from metadata if available\n            if \"language\" in d.metadata and d.metadata[\"language\"] != \"unknown\":\n                lang_tag = f\"[{d.metadata['language'].upper()}]\"\n            packed.append(f\"[{i}] {completeness_tag} {lang_tag} {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Task: Given a user query and a list of documentation chunks, assign a relevance score (0-100) to each chunk.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Output JSON with format: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting with XML-style tags and language filtering\n        final_contexts = []\n        seen_parents = set()\n        context_blocks = []\n        \n        # Detect requested language from query (case-insensitive)\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', query_lower):\n                requested_lang = lang\n                break\n        \n        def chunk_language_filter(chunk_text: str) -> bool:\n            # Simple heuristic: check for language tags or code fences\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            # Also accept if chunk mentions requested language in prefix or breadcrumb\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # If chunk is code and no explicit language, accept by default\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n        \n        for c in top_chunks:\n            if not chunk_language_filter(c.page_content):\n                continue\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent by language as well\n                if not chunk_language_filter(parent.page_content):\n                    continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_blocks.append(f\"<source file='{src}' section='{breadcrumb}'>\\n{parent.page_content}\\n</source>\")\n        \n        context_block = \"\\n\\n\".join(context_blocks)\n        \n        # 4. Generation (Strict Grounding) with improved prompt encouraging Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n=======\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Cross-language filtering: detect requested language from query and filter candidates\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', query_lower):\n                requested_lang = lang\n                break\n        \n        if requested_lang:\n            filtered_candidates = [c for c in candidates if c.metadata.get(\"language\", \"unknown\") == requested_lang]\n            # If filtered too strictly, fallback to original candidates to preserve recall\n            if len(filtered_candidates) >= 3:\n                candidates = filtered_candidates\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting with XML-style tags and language filtering\n        final_contexts = []\n        seen_parents = set()\n        context_blocks = []\n        \n        def chunk_language_filter(chunk_text: str) -> bool:\n            # Simple heuristic: check for language tags or code fences\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            # Also accept if chunk mentions requested language in prefix or breadcrumb\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # If chunk is code and no explicit language, accept by default\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n        \n        for c in top_chunks:\n            if not chunk_language_filter(c.page_content):\n                continue\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent by language as well\n                if not chunk_language_filter(parent.page_content):\n                    continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_blocks.append(f\"<source file='{src}' section='{breadcrumb}'>\\n{parent.page_content}\\n</source>\")\n        \n        context_block = \"\\n\\n\".join(context_blocks)\n        \n        # 5. Generation (Strict Grounding) with improved prompt encouraging Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": "{\"raw_scores\": [0.305, 0.5041666666666667, 0.655, 0.5, 0.25, 0.5, 0.475, 0.4130952380952381, 0.5, 0.0], \"error_details\": [{\"sample\": 4, \"type\": \"code_error\", \"message\": \"[\\\"Import Error on line 1: No module named 'google.ai'\\\"]\", \"bad_code\": \"from google.ai.generativeai import LlmAgent\\n\\ndef main():\\n    # Initialize the LlmAgent\\n    agent = LlmAgent()\\n\\n    # Define the question to ask\\n    question = \\\"What is the capital of France?\\\"\\n\\n    # Run the agent to get the answer\\n    answer = agent.run(question)\\n\\n    # Print the answer\\n    print(\\\"Answer:\\\", answer)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\"}, {\"sample\": 7, \"type\": \"code_error\", \"message\": \"Syntax Error: unterminated string literal (detected at line 77) (<unknown>, line 77)\", \"bad_code\": \"> # Conceptual Code: Sequential Data Pipeline\\n> from google.adk.agents import SequentialAgent, LlmAgent\\n> \\n> validator = LlmAgent(name=\\\"ValidateInput\\\", instruction=\\\"Validate the input.\\\", output_key=\\\"validation_status\\\")\\n> processor = LlmAgent(name=\\\"ProcessData\\\", instruction=\\\"Process data if {validation_status} is 'valid'.\\\", output_key=\\\"result\\\")\\n> reporter = LlmAgent(name=\\\"ReportResult\\\", instruction=\\\"Report the result from {result}.\\\")\\n> \\n> data_pipeline = SequentialAgent(\\n> name=\\\"DataPipeline\\\",\\n> sub_agents=[validator, processor, reporter]\\n> )\\n> # validator runs -> saves to state['validation_status']\\n> # processor runs -> reads state['validation_status'], saves to state['result']\\n> # reporter runs -> reads state['result']\\n>\\n\\n> # Full Example: Code Development Pipeline\\n> from google.adk.agents import SequentialAgent, LlmAgent\\n> from google.adk.runner import InMemoryRunner\\n> from google.adk.sessions import Session\\n> from google.genai.types import Content, Part\\n> from google.adk.events import Event\\n> \\n> # Define sub-agents with output_key to pass data via shared state\\n> code_writer_agent = LlmAgent(\\n>     name=\\\"CodeWriterAgent\\\",\\n>     instruction=\\\"Write Python code for the given specification.\\\",\\n>     output_key=\\\"generated_code\\\"\\n> )\\n> \\n> code_reviewer_agent = LlmAgent(\\n>     name=\\\"CodeReviewerAgent\\\",\\n>     instruction=\\\"Review the code:\\\\n{generated_code}\\\",\\n>     output_key=\\\"review_comments\\\"\\n> )\\n> \\n> code_refactorer_agent = LlmAgent(\\n>     name=\\\"CodeRefactorerAgent\\\",\\n>     instruction=\\\"Refactor the code based on review comments:\\\\nCode:\\\\n{generated_code}\\\\nComments:\\\\n{review_comments}\\\",\\n>     output_key=\\\"refactored_code\\\"\\n> )\\n> \\n> # Create the SequentialAgent pipeline\\n> code_pipeline_agent = SequentialAgent(\\n>     name=\\\"CodePipelineAgent\\\",\\n>     sub_agents=[code_writer_agent, code_reviewer_agent, code_refactorer_agent]\\n> )\\n> \\n> # Run the pipeline with InMemoryRunner\\n> runner = InMemoryRunner(code_pipeline_agent, \\\"CodePipelineAgent\\\")\\n> session = runner.session_service().create_session(\\\"CodePipelineAgent\\\", \\\"user_123\\\").blocking_get()\\n> \\n> # User input as Content\\n> user_message = Content.from_parts(Part.from_text(\\\"Write a Python function to compute factorial.\\\"))\\n> \\n> # Run the agent asynchronously and print final response\\n> event_stream = runner.run_async(\\\"user_123\\\", session.id(), user_message)\\n> \\n> for event in event_stream.blocking_iter():\\n>     if event.final_response():\\n>         print(event.stringify_content())\\n>\\n\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\nfrom google.adk.runner import InMemoryRunner\\nfrom google.adk.sessions import Session\\nfrom google.genai.types import Content, Part\\nfrom google.adk.events import Event\\n\\ndef main():\\n    # Define sub-agents with output_key to pass data via shared state\\n    code_writer_agent = LlmAgent(\\n        name=\\\"CodeWriterAgent\\\",\\n        instruction=(\\n            \\\"You are a Python Code Generator.\\\\n\\\"\\n            \\\"Based *only* on the user's request, write Python code that fulfills the requirement.\\\\n\\\"\\n            \\\"Output *only* the complete Python code block, enclosed in triple backticks (\\n\\n).\\\\n\\\"\\n            \\\"Do not add any other text before or after the code block.\\\"\\n        ),\\n        output_key=\\\"generated_code\\\"\\n    )\\n\\n    code_reviewer_agent = LlmAgent(\\n        name=\\\"CodeReviewerAgent\\\",\\n        instruction=(\\n            \\\"You are an expert Python Code Reviewer.\\\\n\\\"\\n            \\\"Your task is to provide constructive feedback on the provided code.\\\\n\\\\n\\\"\\n            \\\"**Code to Review:**\\\\n\\\"\\n            \\\"\\n\\n\\\\n\\\\n\\\"\\n            \\\"**Output:**\\\\n\\\"\\n            \\\"Provide your feedback as a concise, bulleted list. If no issues, say 'No major issues found.'\\\\n\\\"\\n            \\\"Output *only* the review comments or the 'No major issues' statement.\\\"\\n        ),\\n        output_key=\\\"review_comments\\\"\\n    )\\n\\n    code_refactorer_agent = LlmAgent(\\n        name=\\\"CodeRefactorerAgent\\\",\\n        instruction=(\\n            \\\"You are a Python Code Refactoring AI.\\\\n\\\"\\n            \\\"Your goal is to improve the given Python code based on the provided review comments.\\\\n\\\\n\\\"\\n            \\\"**Original Code:**\\\\n\\\"\\n            \\\"\\n\\n\\\\n\\\\n\\\"\\n            \\\"**Review Comments:**\\\\n\\\"\\n            \\\"{review_comments}\\\\n\\\\n\\\"\\n            \\\"**Output:**\\\\n\\\"\\n            \\\"Output *only* the final, refactored Python code block, enclosed in triple backticks (\\n\\n).\\\\n\\\"\\n            \\\"Do not add any other text before or after the code block.\\\"\\n        ),\\n        output_key=\\\"refactored_code\\\"\\n    )\\n\\n    # Create the SequentialAgent pipeline\\n    code_pipeline_agent = SequentialAgent(\\n        name=\\\"CodePipelineAgent\\\",\\n        sub_agents=[code_writer_agent, code_reviewer_agent, code_refactorer_agent]\\n    )\\n\\n    # Create runner and session\\n    runner = InMemoryRunner(code_pipeline_agent, \\\"CodePipelineAgent\\\")\\n    session = runner.session_service().create_session(\\\"CodePipelineAgent\\\", \\\"user_123\\\").blocking_get()\\n\\n    # User input as Content\\n    user_message = Content.from_parts(Part.from_text(\\\"Write a Python function to compute factorial.\\\"))\\n\\n    # Run the agent asynchronously and print final response\\n    event_stream = runner.run_async(\\\"user_123\\\", session.id(), user_message)\\n\\n    for event in event_stream.blocking_iter():\\n        if event.final_response():\\n            print(event.stringify_content())\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\"}]}", "artifact_dir": null, "embedding": null}