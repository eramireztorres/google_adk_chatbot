{"id": "05dc0e3d-9d11-4fe7-b4d4-e4293b101db4", "code": "import os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags based on breadcrumb or filename\n            tags = []\n            if re.search(r'\\bA2A\\b', breadcrumb, re.I) or 'a2a' in source.lower():\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bWORKFLOW\\b', breadcrumb, re.I) or 'workflow' in source.lower():\n                tags.append(\"[WORKFLOW]\")\n            p.metadata[\"tags\"] = \" \".join(tags) if tags else \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation and language tags if present\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                \n                # Add language tag from breadcrumb if detected (e.g. #### Python Implementation)\n                lang_tag = \"\"\n                lang_matches = re.findall(r'#### (\\w+) Implementation', c.page_content)\n                if lang_matches:\n                    lang_tag = f\"[{lang_matches[0].upper()}]\"\n                \n                # Include tags from parent metadata\n                tag_str = p.metadata.get(\"tags\", \"\")\n                \n                c.page_content = f\"{prefix} {lang_tag} {tag_str} {breadcrumb}\\n{c.page_content}\".strip()\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with improved prompt for multi-language and completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            # Tag chunk as complete API or partial snippet heuristically\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def)\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            packed.append(f\"[{i}] {completeness_tag} {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Task: Given a user query and a list of documentation chunks, assign a relevance score (0-100) to each chunk.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Output JSON with format: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Detect requested language from query (case-insensitive)\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        # Support alias and synonyms for language detection\n        lang_aliases = {\n            'python': ['python', 'py'],\n            'go': ['go', 'golang'],\n            'java': ['java']\n        }\n        for lang, aliases in lang_aliases.items():\n            if any(re.search(r'\\b' + alias + r'\\b', query_lower) for alias in aliases):\n                requested_lang = lang\n                break\n\n        # 2. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n\n        # 3. Cross-language filtering: filter candidates by requested language to reduce noise\n        def chunk_language_filter(chunk_text: str) -> bool:\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            # Accept if chunk contains a header for the requested language (support [LANG=XXX] tags)\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            if re.search(r'\\[LANG=' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # Accept if chunk prefix or tags mention requested language\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # Accept if code chunk with no explicit language tag (fallback)\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n\n        filtered_candidates = [c for c in candidates if chunk_language_filter(c.page_content)]\n        # Relax filtering if too few candidates remain to preserve recall\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n\n        # 4. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n\n        # 5. Parent Retrieval & Formatting with XML-style tags and cross-language filtering\n        final_contexts = []\n        seen_parents = set()\n        context_blocks = []\n\n        for c in top_chunks:\n            if not chunk_language_filter(c.page_content):\n                continue\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Also filter parent text by language\n                if not chunk_language_filter(parent.page_content):\n                    continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_blocks.append(f\"<source file='{src}' section='{breadcrumb}'>\\n{parent.page_content}\\n</source>\")\n\n        context_block = \"\\n\\n\".join(context_blocks)\n\n        # 6. Generation (Strict Grounding) with improved prompt enforcing Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\"\n            \"5. When quoting code, explicitly indicate the programming language of the snippet.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n\n        res = self.llm.invoke(prompt)\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "18287e49-c59d-400d-994d-91fa994ebbfb", "generation": 3, "timestamp": 1769723435.6059942, "iteration_found": 25, "metrics": {"combined_score": 0.41380952380952385, "num_samples": 10}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 8 lines with 14 lines\nChange 2: Replace 15 lines with 17 lines\nChange 3: Replace 12 lines with 13 lines\nChange 4: Replace 5 lines with 5 lines", "parent_metrics": {"combined_score": 0.4806666666666666, "num_samples": 10}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer for Google ADK documentation. Improve the code to maximize RAG-pipeline performance.\n\nThe current architecture uses a **Parent-Child Retrieval** system with **Hybrid Search** and **LLM Reranking**.\n\n### High-Leverage Evolution Points:\n\n1. **Hyperparameter Synergy**: Optimize the balance between `chunk_size` (for retrieval precision), `top_k_vector/bm25` (for recall), and `weight_vector/bm25`. The documentation has many exact API names which may favor BM25, while conceptual \"how-to\" queries favor Vector.\n2. **Reranker Prompt Engineering**: The `_llm_rerank` prompt is critical. Evolve it to better identify relevance, handle multi-language snippets, or detect if a chunk contains a complete API definition vs. a partial snippet.\n3. **Metadata-Rich Ingestion**: Enhance `_chunk_document` to include more context in `breadcrumb` and `tagging`. Consider adding language-specific tags or project-area identifiers (e.g. `[A2A]`, `[WORKFLOW]`) to help the reranker and generator.\n4. **Context Assembly & Prompting**: Optimize the `context_block` formatting in `query`. Experiment with summarizing parent context or using XML-style tags to separate sources. Refine the final generation prompt for better \"Quote then Explain\" adherence.\n5. **Cross-Language Filtering**: Implement logic to detect the user's requested language and prioritize or filter retrieved chunks to reduce cross-language noise.\n\n**STRICT RULES**:\n- **Fork-Safety**: Avoid any top-level imports of `langchain`, `agno`, or `openai`. Use lazy imports inside methods.\n- **Compatibility**: Ensure `evaluate_rag(docs_path, query)` maintains its signature.\n- **Cleanup**: Always call `gc.collect()` after LLM calls or retrieval to maintain memory stability.\n\n**Example Improvement (Reranker Logic)**:\n```python\n# You can evolve the reranker prompt to be more specific:\nprompt = (\n    \"Role: ADK Expert Reranker. \"\n    \"Goal: Select chunks that provide EXECUTABLE code for the query. \"\n    f\"Query: {query}\\n\"\n    \"Chunks: ... \"\n)\n```\n- Maintain `evaluate_rag(docs_path, query)` signature and return keys: `answer`, `contexts`.\n- Keep `RAGSystem` class structure and lazy-loading in `__init__`.\n", "user": "# Current Program Information\n- Fitness: 0.4807\n- Feature coordinates: combined_score=0.48\n- Focus areas: - Fitness declined: 0.5529 \u2192 0.4807. Consider revising recent changes.\n- Exploring combined_score=0.48 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### raw_scores\n```\n[0.30833333333333335, 0.5700000000000001, 0.5485714285714286, 0.5, 0.5, 0.5, 0.5, 0.6047619047619048, 0.475, 0.30000000000000004]\n```\n\n### error_details\n```\n[{'sample': 4, 'type': 'code_error', 'message': '[\"Import Error on line 1: No module named \\'google.ai\\'\", \"Import Error on line 2: No module named \\'google.ai\\'\", \"Import Error on line 7: No module named \\'google.ai\\'\", \"Import Error on line 10: No module named \\'google.ai\\'\", \"Import Error on line 11: No module named \\'google.ai\\'\", \"Import Error on line 12: No module named \\'google.ai\\'\", \"Import Error on line 14: No module named \\'google.ai\\'\"]', 'bad_code': 'from google.ai.generativelanguage import TextServiceClient\\nfrom google.ai.generativelanguage_v1beta2 import (\\n    ModelName,\\n    TextPrompt,\\n    GenerateTextRequest,\\n)\\nfrom google.ai.generativelanguage_v1beta2.types import (\\n    TextCompletion,\\n)\\nfrom google.ai.generativelanguage_v1beta2.services.text_service import TextServiceClient\\nfrom google.ai.generativelanguage_v1beta2.types import GenerateTextRequest\\nfrom google.ai.generativelanguage_v1beta2.types import TextPrompt\\n\\nfrom google.ai.generativelanguage_v1beta2.experimental import LlmAgent, LlmAgentConfig\\n\\ndef main():\\n    # Initialize the TextServiceClient\\n    client = TextServiceClient()\\n\\n    # Configure the LlmAgent with the model and client\\n    agent_config = LlmAgentConfig(\\n        model=ModelName(\"models/chat-bison-001\"),\\n        client=client,\\n    )\\n    agent = LlmAgent(agent_config)\\n\\n    # Define the question to ask\\n    question = \"What is the capital of France?\"\\n\\n    # Run the agent to get the answer\\n    response = agent.run(question)\\n\\n    # Print the answer\\n    print(\"Answer:\", response)\\n\\nif __name__ == \"__main__\":\\n    main()'}, {'sample': 7, 'type': 'code_error', 'message': 'Syntax Error: invalid syntax (<unknown>, line 32)', 'bad_code': 'from google.adk.agents import SequentialAgent, LlmAgent\\nfrom google.adk.runtime import Runner\\n\\n# Define three LlmAgents in sequence, each passing data via output_key\\nvalidator = LlmAgent(\\n    name=\"ValidateInput\",\\n    instruction=\"Validate the input.\",\\n    output_key=\"validation_status\"\\n)  # Saves output to state[\\'validation_status\\']\\n\\nprocessor = LlmAgent(\\n    name=\"ProcessData\",\\n    instruction=\"Process data if {validation_status} is \\'valid\\'.\",\\n    output_key=\"result\"\\n)  # Reads state[\\'validation_status\\'], saves to state[\\'result\\']\\n\\nreporter = LlmAgent(\\n    name=\"ReportResult\",\\n    instruction=\"Report the result from {result}.\"\\n)  # Reads state[\\'result\\']\\n\\n# Create the SequentialAgent pipeline with the three sub_agents\\ndata_pipeline = SequentialAgent(\\n    name=\"DataPipeline\",\\n    sub_agents=[validator, processor, reporter]\\n)\\n\\n# Run the pipeline using the Runner\\nrunner = Runner(agent=data_pipeline)\\nrunner.run()\\n\\n> from google.adk.agents import SequentialAgent, LlmAgent\\n> \\n> validator = LlmAgent(name=\"ValidateInput\", instruction=\"Validate the input.\", output_key=\"validation_status\")\\n> processor = LlmAgent(name=\"ProcessData\", instruction=\"Process data if {validation_status} is \\'valid\\'.\", output_key=\"result\")\\n> reporter = LlmAgent(name=\"ReportResult\", instruction=\"Report the result from {result}.\")\\n> \\n> data_pipeline = SequentialAgent(\\n>     name=\"DataPipeline\",\\n>     sub_agents=[validator, processor, reporter]\\n> )\\n> # validator runs -> saves to state[\\'validation_status\\']\\n> # processor runs -> reads state[\\'validation_status\\'], saves to state[\\'result\\']\\n> # reporter runs -> reads state[\\'result\\']\\n>'}, {'sample': 9, 'type': 'code_error', 'message': '[\"Import Error on line 1: No module named \\'google.ai\\'\", \"Import Error on line 2: No module named \\'google.ai\\'\", \"Import Error on line 3: No module named \\'google.ai\\'\"]', 'bad_code': 'from google.ai.generativeai import TextGenerationClient\\nfrom google.ai.generativeai.types import GenerateTextRequest\\nfrom google.ai.generativeai.types import TextPrompt\\n\\nclass CustomStoryAgent:\\n    def __init__(self):\\n        self.client = TextGenerationClient()\\n\\n    async def _run_async_impl(self, prompt: str) -> str:\\n        # Step 1: Generate story\\n        generate_request = GenerateTextRequest(\\n            model=\"models/story-generator\",\\n            prompt=TextPrompt(text=prompt),\\n            max_tokens=500,\\n        )\\n        response = await self.client.generate_text(request=generate_request)\\n        story = response.text\\n\\n        # Step 2: Check tone\\n        tone_check_request = GenerateTextRequest(\\n            model=\"models/tone-checker\",\\n            prompt=TextPrompt(text=f\"Check the tone of this story: {story}\"),\\n            max_tokens=10,\\n        )\\n        tone_response = await self.client.generate_text(request=tone_check_request)\\n        tone = tone_response.text.strip().lower()\\n\\n        # Step 3: Regenerate if tone is negative\\n        if \"negative\" in tone:\\n            regenerate_request = GenerateTextRequest(\\n                model=\"models/story-generator\",\\n                prompt=TextPrompt(text=prompt + \" Generate a positive tone story.\"),\\n                max_tokens=500,\\n            )\\n            regenerate_response = await self.client.generate_text(request=regenerate_request)\\n            story = regenerate_response.text\\n\\n        return story'}]\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 9 lines with 9 lines\nChange 2: Replace 18 lines with 24 lines\nChange 3: Replace 47 lines with 50 lines\nChange 4: Replace 51 lines with 73 lines\n- Metrics: combined_score: 0.5529, num_samples: 10.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 13 lines with 14 lines\nChange 2: Replace 14 lines with 14 lines\nChange 3: Replace 6 lines with 19 lines\nChange 4: Replace 37 lines with 45 lines\nChange 5: Replace 16 lines with 20 lines\nChange 6: Replace 15 lines with 15 lines\nChange 7: Replace 11 lines with 11 lines\n- Metrics: combined_score: 0.5879, num_samples: 10.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 39 lines with 48 lines\nChange 2: Replace 37 lines with 53 lines\nChange 3: Replace 40 lines with 54 lines\nChange 4: Replace 65 lines with 67 lines\n- Metrics: combined_score: 0.5918, num_samples: 10.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.5918)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    # Only ingest .md files, skip hidden and non-API docs\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n\n                        # Add language and project tags in chunk metadata by filename heuristics and content\n                        lang_tag = None\n                        # Detect language from filename and also from code fences in content for synergy\n                        if re.search(r'\\bpython\\b', file.lower()):\n                            lang_tag = \"[LANG_PYTHON]\"\n                        elif re.search(r'\\bgo\\b', file.lower()):\n                            lang_tag = \"[LANG_GO]\"\n                        elif re.search(r'\\bjava\\b', file.lower()):\n                            lang_tag = \"[LANG_JAVA]\"\n                        else:\n                            # Fallback: detect language from first code fence in text content\n                            code_lang_match = re.search(r'```(\\w+)', text_content)\n                            if code_lang_match:\n                                lang = code_lang_match.group(1).lower()\n                                if lang in (\"python\", \"go\", \"java\"):\n                                    lang_tag = f\"[LANG_{lang.upper()}]\"\n\n                        # Project area tags example: detect A2A, WORKFLOW from path or filename heuristics\n                        project_tag = None\n                        if \"a2a\" in root.lower() or \"a2a\" in file.lower():\n                            project_tag = \"[A2A]\"\n                        elif \"workflow\" in root.lower() or \"workflow\" in file.lower():\n                            project_tag = \"[WORKFLOW]\"\n\n                        chunks = self._chunk_document(text_content, file_path)\n                        # Inject tags into each chunk content for better reranking and generation\n                        for c in chunks:\n                            tags = []\n                            if lang_tag: tags.append(lang_tag)\n                            if project_tag: tags.append(project_tag)\n                            if tags:\n                                c.page_content = \" \".join(tags) + \" \" + c.page_content\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n\n            # Detect language tag from first code fence in parent content\n            lang_match = re.search(r'```(\\w+)', p.page_content)\n            language = lang_match.group(1).lower() if lang_match else \"unknown\"\n            p.metadata[\"language\"] = language\n\n            # Heuristic project-area tagging based on breadcrumb or source path\n            if re.search(r'a2a|auth', breadcrumb, re.I) or re.search(r'a2a|auth', source, re.I):\n                p.metadata[\"project_area\"] = \"[A2A]\"\n            elif re.search(r'workflow', breadcrumb, re.I) or re.search(r'workflow', source, re.I):\n                p.metadata[\"project_area\"] = \"[WORKFLOW]\"\n            else:\n                p.metadata[\"project_area\"] = \"[GENERAL]\"\n\n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb} {p.metadata['project_area']}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = language\n                c.metadata[\"project_area\"] = p.metadata[\"project_area\"]\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for code, language, and completeness.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1500].replace('\\n', ' ')  # flatten newlines for prompt\n            # Indicate if chunk looks like a full API def or partial snippet\n            is_complete_api = \"Complete API definition\" if re.search(r'^\\s*(class|func|interface|def|type)\\s', snippet, re.I|re.M) else \"Partial snippet\"\n            # Include language tag if present in metadata or content\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                lang_search = re.search(r'\\[LANG=([A-Za-z]+)\\]', d.page_content)\n                if lang_search:\n                    lang_tag = f\"[LANG={lang_search.group(1)}]\"\n                else:\n                    # Fallback heuristic by code fences\n                    if re.search(r'```python', d.page_content, re.I):\n                        lang_tag = \"[LANG=Python]\"\n                    elif re.search(r'```go', d.page_content, re.I):\n                        lang_tag = \"[LANG=Go]\"\n                    elif re.search(r'```java', d.page_content, re.I):\n                        lang_tag = \"[LANG=Java]\"\n            packed.append(f\"[{i}] {lang_tag} ({is_complete_api}) {snippet}\")\n        \n        prompt = (\n            \"Role: Google ADK Expert Reranker.\\n\"\n            \"Goal: Given the user query and a list of documentation chunks, assign a relevance score 0-100. \"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query,\\n\"\n            \"- Complete API definitions over partial snippets,\\n\"\n            \"- Relevant code examples,\\n\"\n            \"- Matching requested programming language if specified.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return a JSON object with format: {'scores': {chunk_id: score, ...}} with scores as integers.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Sort indices by score descending\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Detect requested language from query (if any)\n        requested_lang = None\n        lang_patterns = {\n            \"python\": r\"\\bpython\\b\",\n            \"go\": r\"\\bgo\\b\",\n            \"java\": r\"\\bjava\\b\",\n        }\n        for lang, pattern in lang_patterns.items():\n            if re.search(pattern, query_str, re.I):\n                requested_lang = lang.upper()\n                break\n\n        # 2. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 3. Filter candidates by language tag if requested to reduce noise\n        if requested_lang:\n            filtered = []\n            lang_tag = f\"[LANG_{requested_lang}]\"\n            for c in candidates:\n                if lang_tag in c.page_content:\n                    filtered.append(c)\n            # Relax filtering if too few candidates remain to preserve recall\n            if filtered and len(filtered) >= max(3, self.rerank_top_n):\n                candidates = filtered\n\n        # 4. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 5. Parent Retrieval & Formatting with XML-style tags and summarized parents\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Summarize parent to first 1000 chars for conciseness\n                parent_summary = parent.page_content[:1000].strip() + (\"...\" if len(parent.page_content) > 1000 else \"\")\n                context_block += (\n                    f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent_summary}\\n</source>\\n\\n\"\n                )\n\n        # 6. Generation (Strict Grounding) with improved prompt to emphasize quote/explain and language\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using ONLY the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not found in the context, explicitly respond: 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc or code snippet, then provide your explanation.\\n\"\n            \"4. Only provide code or examples in the requested programming language if specified by the user. \"\n            \"If no language is specified, default to Python but mention that other languages exist.\\n\"\n            \"5. When quoting code, explicitly indicate the programming language of the snippet.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5918), Performs well on num_samples (10.0000)\n\n### Program 2 (Score: 0.5879)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks with enhanced flattening and language tag normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            # Normalize language tag: map common aliases and lowercase\n            lang_map = {\"py\": \"PYTHON\", \"python\": \"PYTHON\", \"go\": \"GO\", \"golang\": \"GO\", \"java\": \"JAVA\"}\n            lang_tag = lang_map.get(label.lower(), label.upper())\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n\n        # Remove tabs wrappers but keep tabs content flattened with language tags\n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            # Add project-area tags heuristically based on breadcrumb content for better reranking and generation\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            tags = []\n            if re.search(r'\\bworkflow\\b', breadcrumb, re.I) or re.search(r'workflow', source, re.I):\n                tags.append(\"[WORKFLOW]\")\n            if re.search(r'\\ba2a\\b', breadcrumb, re.I) or re.search(r'a2a', source, re.I):\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bapi\\b', breadcrumb, re.I):\n                tags.append(\"[API]\")\n            tag_str = \" \".join(tags) if tags else \"[GENERAL]\"\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            p.metadata[\"tags\"] = tag_str\n            self.parent_docs[parent_id] = p\n\n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Incorporate tags and language info into chunk content for reranker and better filtering\n                # Try detect language from markdown tabs or code fences for explicit tagging\n                lang_tag = \"\"\n                lang_search = re.search(r'\\[([A-Z]+)\\]\\s+\\w+\\s+Implementation', c.page_content)\n                if lang_search:\n                    lang_tag = f\"[LANG={lang_search.group(1)}]\"\n                else:\n                    # fallback: detect language from code fences in chunk\n                    fence_lang_search = re.search(r'```(\\w+)', c.page_content)\n                    if fence_lang_search:\n                        lang_tag = f\"[LANG={fence_lang_search.group(1).upper()}]\"\n                    else:\n                        lang_tag = \"[LANG=UNKNOWN]\"\n                c.page_content = f\"{prefix} {lang_tag} {tag_str} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"tags\"] = tag_str\n                c.metadata[\"language\"] = lang_tag.strip(\"[]\").replace(\"LANG=\", \"\").lower()\n                child_chunks.append(c)\n\n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n\n        packed = []\n        requested_lang = None\n        # Detect requested language from query for reranker prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            lang = d.metadata.get(\"language\", \"unknown\")\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type)\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n\n        # Determine requested language from query for filtering\n        requested_lang = None\n        lang_match = re.search(r'\\b(python|go|java)\\b', query_str, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        # Filter candidates by language tag if language requested, with fallback to preserve recall\n        if requested_lang:\n            filtered_candidates = []\n            for d in candidates:\n                tags = d.metadata.get(\"tags\", \"\")\n                lang = d.metadata.get(\"language\", \"\").lower()\n                # Match requested language tag in content or metadata tags or language field\n                if (f\"[{requested_lang.upper()}]\" in d.page_content or\n                    f\"[{requested_lang.upper()}]\" in tags or\n                    lang == requested_lang):\n                    filtered_candidates.append(d)\n            # Use filtered candidates only if sufficient count, else fallback to full set\n            if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n                candidates = filtered_candidates\n\n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n\n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n\n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Use XML-like tags for clearer source separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. Only provide code or explanations in the user's requested language if specified; otherwise, default to Python but mention that other languages are available.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n\n        res = self.llm.invoke(prompt)\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5879), Performs well on num_samples (10.0000)\n\n### Program 3 (Score: 0.5529)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags to help reranker identify language and completeness\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                # Try detect language from breadcrumb or content heuristics\n                if re.search(r'```python', d.page_content, re.I):\n                    lang_tag = \"[LANG=Python]\"\n                elif re.search(r'```go', d.page_content, re.I):\n                    lang_tag = \"[LANG=Go]\"\n                elif re.search(r'```java', d.page_content, re.I):\n                    lang_tag = \"[LANG=Java]\"\n            # Detect if chunk likely contains a complete API definition or partial snippet\n            completeness_tag = \"[COMPLETE]\" if re.search(\n                r'^\\s*(class|func|interface|def|type|package|func\\s+main)\\s', d.page_content, re.I|re.M) else \"[PARTIAL]\"\n            \n            content_snippet = d.page_content[:1300].replace('\\n', ' ')\n            packed.append(f\"[{i}] {lang_tag} {completeness_tag} {content_snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 on relevance to the query, prioritizing:\\n\"\n            \"1) Exact API names or full definitions over partial snippets.\\n\"\n            \"2) Code examples executable in the requested language.\\n\"\n            \"3) Language requested by user query (detect language tags).\\n\"\n            \"4) Prefer chunks with complete API definitions or full function/class blocks.\\n\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON ONLY with format: {'scores': {chunk_id: score, ...}}\\n\"\n            \"Scores must be integers 0 to 100.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5529), Performs well on num_samples (10.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.5237)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags to help reranker identify language and completeness\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                # Try detect language from breadcrumb or content heuristics\n                if re.search(r'```python', d.page_content, re.I):\n                    lang_tag = \"[LANG=Python]\"\n                elif re.search(r'```go', d.page_content, re.I):\n                    lang_tag = \"[LANG=Go]\"\n                elif re.search(r'```java', d.page_content, re.I):\n                    lang_tag = \"[LANG=Java]\"\n            completeness_tag = \"[COMPLETE]\" if re.search(r'^\\s*(class|func|interface|def|type)\\s', d.page_content, re.I|re.M) else \"[PARTIAL]\"\n            \n            content_snippet = d.page_content[:1300].replace('\\n', ' ')\n            packed.append(f\"[{i}] {lang_tag} {completeness_tag} {content_snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 on relevance to the query, prioritizing:\\n\"\n            \"1) Exact API names or full definitions over partial snippets.\\n\"\n            \"2) Code examples executable in the requested language.\\n\"\n            \"3) Language requested by user query (detect language tags).\\n\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON ONLY with format: {'scores': {chunk_id: score, ...}}\\n\"\n            \"Scores must be integers 0 to 100.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n### Program D2 (Score: 0.5461)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            content = d.page_content[:1500]\n            packed.append(f\"[{i}] {content}\")\n        \n        prompt = (\n            \"You are a reranker for Google ADK documentation. \"\n            \"Given a user query and a list of chunks, score each chunk from 0 to 100 based on its relevance. \"\n            \"Prioritize chunks that contain exact API names or code examples for the requested task. \"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_block += f\"--- SOURCE: {src} | SECTION: {breadcrumb} ---\\n{parent.page_content}\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.5918, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    # Only ingest .md files, skip hidden and non-API docs\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n\n                        # Add language and project tags in chunk metadata by filename heuristics and content\n                        lang_tag = None\n                        # Detect language from filename and also from code fences in content for synergy\n                        if re.search(r'\\bpython\\b', file.lower()):\n                            lang_tag = \"[LANG_PYTHON]\"\n                        elif re.search(r'\\bgo\\b', file.lower()):\n                            lang_tag = \"[LANG_GO]\"\n                        elif re.search(r'\\bjava\\b', file.lower()):\n                            lang_tag = \"[LANG_JAVA]\"\n                        else:\n                            # Fallback: detect language from first code fence in text content\n                            code_lang_match = re.search(r'```(\\w+)', text_content)\n                            if code_lang_match:\n                                lang = code_lang_match.group(1).lower()\n                                if lang in (\"python\", \"go\", \"java\"):\n                                    lang_tag = f\"[LANG_{lang.upper()}]\"\n\n                        # Project area tags example: detect A2A, WORKFLOW from path or filename heuristics\n                        project_tag = None\n                        if \"a2a\" in root.lower() or \"a2a\" in file.lower():\n                            project_tag = \"[A2A]\"\n                        elif \"workflow\" in root.lower() or \"workflow\" in file.lower():\n                            project_tag = \"[WORKFLOW]\"\n\n                        chunks = self._chunk_document(text_content, file_path)\n                        # Inject tags into each chunk content for better reranking and generation\n                        for c in chunks:\n                            tags = []\n                            if lang_tag: tags.append(lang_tag)\n                            if project_tag: tags.append(project_tag)\n                            if tags:\n                                c.page_content = \" \".join(tags) + \" \" + c.page_content\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n\n            # Detect language tag from first code fence in parent content\n            lang_match = re.search(r'```(\\w+)', p.page_content)\n            language = lang_match.group(1).lower() if lang_match else \"unknown\"\n            p.metadata[\"language\"] = language\n\n            # Heuristic project-area tagging based on breadcrumb or source path\n            if re.search(r'a2a|auth', breadcrumb, re.I) or re.search(r'a2a|auth', source, re.I):\n                p.metadata[\"project_area\"] = \"[A2A]\"\n            elif re.search(r'workflow', breadcrumb, re.I) or re.search(r'workflow', source, re.I):\n                p.metadata[\"project_area\"] = \"[WORKFLOW]\"\n            else:\n                p.metadata[\"project_area\"] = \"[GENERAL]\"\n\n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb} {p.metadata['project_area']}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = language\n                c.metadata[\"project_area\"] = p.metadata[\"project_area\"]\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for code, language, and completeness.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1500].replace('\\n', ' ')  # flatten newlines for prompt\n            # Indicate if chunk looks like a full API def or partial snippet\n            is_complete_api = \"Complete API definition\" if re.search(r'^\\s*(class|func|interface|def|type)\\s', snippet, re.I|re.M) else \"Partial snippet\"\n            # Include language tag if present in metadata or content\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                lang_search = re.search(r'\\[LANG=([A-Za-z]+)\\]', d.page_content)\n                if lang_search:\n                    lang_tag = f\"[LANG={lang_search.group(1)}]\"\n                else:\n                    # Fallback heuristic by code fences\n                    if re.search(r'```python', d.page_content, re.I):\n                        lang_tag = \"[LANG=Python]\"\n                    elif re.search(r'```go', d.page_content, re.I):\n                        lang_tag = \"[LANG=Go]\"\n                    elif re.search(r'```java', d.page_content, re.I):\n                        lang_tag = \"[LANG=Java]\"\n            packed.append(f\"[{i}] {lang_tag} ({is_complete_api}) {snippet}\")\n        \n        prompt = (\n            \"Role: Google ADK Expert Reranker.\\n\"\n            \"Goal: Given the user query and a list of documentation chunks, assign a relevance score 0-100. \"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query,\\n\"\n            \"- Complete API definitions over partial snippets,\\n\"\n            \"- Relevant code examples,\\n\"\n            \"- Matching requested programming language if specified.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return a JSON object with format: {'scores': {chunk_id: score, ...}} with scores as integers.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Sort indices by score descending\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Detect requested language from query (if any)\n        requested_lang = None\n        lang_patterns = {\n            \"python\": r\"\\bpython\\b\",\n            \"go\": r\"\\bgo\\b\",\n            \"java\": r\"\\bjava\\b\",\n        }\n        for lang, pattern in lang_patterns.items():\n            if re.search(pattern, query_str, re.I):\n                requested_lang = lang.upper()\n                break\n\n        # 2. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 3. Filter candidates by language tag if requested to reduce noise\n        if requested_lang:\n            filtered = []\n            lang_tag = f\"[LANG_{requested_lang}]\"\n            for c in candidates:\n                if lang_tag in c.page_content:\n                    filtered.append(c)\n            # Relax filtering if too few candidates remain to preserve recall\n            if filtered and len(filtered) >= max(3, self.rerank_top_n):\n                candidates = filtered\n\n        # 4. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 5. Parent Retrieval & Formatting with XML-style tags and summarized parents\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Summarize parent to first 1000 chars for conciseness\n                parent_summary = parent.page_content[:1000].strip() + (\"...\" if len(parent.page_content) > 1000 else \"\")\n                context_block += (\n                    f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent_summary}\\n</source>\\n\\n\"\n                )\n\n        # 6. Generation (Strict Grounding) with improved prompt to emphasize quote/explain and language\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using ONLY the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not found in the context, explicitly respond: 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc or code snippet, then provide your explanation.\\n\"\n            \"4. Only provide code or examples in the requested programming language if specified by the user. \"\n            \"If no language is specified, default to Python but mention that other languages exist.\\n\"\n            \"5. When quoting code, explicitly indicate the programming language of the snippet.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n### Inspiration 2 (Score: 0.5237, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags to help reranker identify language and completeness\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                # Try detect language from breadcrumb or content heuristics\n                if re.search(r'```python', d.page_content, re.I):\n                    lang_tag = \"[LANG=Python]\"\n                elif re.search(r'```go', d.page_content, re.I):\n                    lang_tag = \"[LANG=Go]\"\n                elif re.search(r'```java', d.page_content, re.I):\n                    lang_tag = \"[LANG=Java]\"\n            completeness_tag = \"[COMPLETE]\" if re.search(r'^\\s*(class|func|interface|def|type)\\s', d.page_content, re.I|re.M) else \"[PARTIAL]\"\n            \n            content_snippet = d.page_content[:1300].replace('\\n', ' ')\n            packed.append(f\"[{i}] {lang_tag} {completeness_tag} {content_snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 on relevance to the query, prioritizing:\\n\"\n            \"1) Exact API names or full definitions over partial snippets.\\n\"\n            \"2) Code examples executable in the requested language.\\n\"\n            \"3) Language requested by user query (detect language tags).\\n\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON ONLY with format: {'scores': {chunk_id: score, ...}}\\n\"\n            \"Scores must be integers 0 to 100.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n### Inspiration 3 (Score: 0.4625, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            # Add language tags inline for reranker to detect language per snippet easily\n            return f\"\\n\\n#### {label} Implementation\\n[LANG={label.upper()}]\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            import re\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                \n                # Detect language tag from markdown tabs or code fences\n                lang_tag = \"\"\n                lang_search = re.search(r'#### \\[?(\\w+)\\]? Implementation', c.page_content)\n                if lang_search:\n                    lang_tag = f\"[{lang_search.group(1).upper()}]\"\n                else:\n                    # fallback: try to detect language from code fences in chunk\n                    fence_lang_search = re.search(r'```(\\w+)', c.page_content)\n                    if fence_lang_search:\n                        lang_tag = f\"[{fence_lang_search.group(1).upper()}]\"\n                    else:\n                        lang_tag = \"[UNKNOWN]\"\n                \n                # Add project area tag heuristics based on breadcrumb or source\n                project_area = \"[GENERAL]\"\n                if re.search(r'a2a|auth', breadcrumb, re.I) or re.search(r'a2a|auth', source, re.I):\n                    project_area = \"[A2A]\"\n                elif re.search(r'workflow', breadcrumb, re.I) or re.search(r'workflow', source, re.I):\n                    project_area = \"[WORKFLOW]\"\n                \n                # Compose prefix with tags\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = lang_tag.strip(\"[]\").lower()\n                c.metadata[\"project_area\"] = project_area\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language, completeness and project area.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            # Include metadata tags and breadcrumb in reranking context for better relevance detection\n            meta = d.metadata.get(\"breadcrumb\", \"\") + \" \" + d.metadata.get(\"source\", \"\")\n            lang = d.metadata.get(\"language\", \"unknown\")\n            proj = d.metadata.get(\"project_area\", \"[GENERAL]\")\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            # Heuristic completeness detection\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'class\\s+\\w+|func\\s+\\w+|def\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            lang_tag = f\"[{lang.upper()}]\"\n            proj_tag = proj\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            packed.append(f\"[{i}] {completeness_tag} {lang_tag} {proj_tag} {requested_lang_tag} Meta: {meta}\\nContent: {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Task: Given a user query and a list of documentation chunks, assign a relevance score (0-100) to each chunk.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            \"- Chunks related to project areas mentioned or inferred from query if possible.\\n\"\n            \"- Penalize partial or incomplete snippets and those in unrelated languages.\\n\"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Output JSON with format: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting with XML-style tags and language/project area filtering\n        final_contexts = []\n        seen_parents = set()\n        context_blocks = []\n\n        # Detect requested language from query (case-insensitive)\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', query_lower):\n                requested_lang = lang\n                break\n\n        for c in top_chunks:\n            # Filter chunks by requested language to reduce noise but allow unknown if code\n            if requested_lang:\n                lang = c.metadata.get(\"language\", \"\").lower()\n                if lang != requested_lang:\n                    if not c.page_content.startswith(\"[DOC_CODE]\"):\n                        continue\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent by language similarly\n                parent_lang = parent.metadata.get(\"language\", \"\").lower()\n                if requested_lang and parent_lang != requested_lang:\n                    continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_blocks.append(f\"<source file='{src}' section='{breadcrumb}'>\\n{parent.page_content}\\n</source>\")\n\n        context_block = \"\\n\\n\".join(context_blocks)\n\n        # 4. Generation (Strict Grounding) with improved prompt encouraging Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n\n        res = self.llm.invoke(prompt)\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n# Current Program\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags based on breadcrumb or filename\n            tags = []\n            if re.search(r'\\bA2A\\b', breadcrumb, re.I) or 'a2a' in source.lower():\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bWORKFLOW\\b', breadcrumb, re.I) or 'workflow' in source.lower():\n                tags.append(\"[WORKFLOW]\")\n            p.metadata[\"tags\"] = \" \".join(tags) if tags else \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation and language tags if present\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                \n                # Add language tag from breadcrumb if detected (e.g. #### Python Implementation)\n                lang_tag = \"\"\n                lang_matches = re.findall(r'#### (\\w+) Implementation', c.page_content)\n                if lang_matches:\n                    lang_tag = f\"[{lang_matches[0].upper()}]\"\n                \n                # Include tags from parent metadata\n                tag_str = p.metadata.get(\"tags\", \"\")\n                \n                c.page_content = f\"{prefix} {lang_tag} {tag_str} {breadcrumb}\\n{c.page_content}\".strip()\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with improved prompt for multi-language and completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            # Tag chunk as complete API or partial snippet heuristically\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def)\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            packed.append(f\"[{i}] {completeness_tag} {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Task: Given a user query and a list of documentation chunks, assign a relevance score (0-100) to each chunk.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Output JSON with format: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Detect requested language from query (case-insensitive)\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', query_lower):\n                requested_lang = lang\n                break\n\n        # 2. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n\n        # 3. Cross-language filtering: filter candidates by requested language to reduce noise\n        def chunk_language_filter(chunk_text: str) -> bool:\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            # Accept if chunk contains a header for the requested language\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            # Accept if chunk prefix or tags mention requested language\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # Accept if code chunk with no explicit language tag (fallback)\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n\n        filtered_candidates = [c for c in candidates if chunk_language_filter(c.page_content)]\n        # Relax filtering if too few candidates remain to preserve recall\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n\n        # 4. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n\n        # 5. Parent Retrieval & Formatting with XML-style tags and cross-language filtering\n        final_contexts = []\n        seen_parents = set()\n        context_blocks = []\n\n        for c in top_chunks:\n            if not chunk_language_filter(c.page_content):\n                continue\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Also filter parent text by language\n                if not chunk_language_filter(parent.page_content):\n                    continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_blocks.append(f\"<source file='{src}' section='{breadcrumb}'>\\n{parent.page_content}\\n</source>\")\n\n        context_block = \"\\n\\n\".join(context_blocks)\n\n        # 6. Generation (Strict Grounding) with improved prompt enforcing Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n\n        res = self.llm.invoke(prompt)\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n        # 1. Detect requested language from query (case-insensitive)\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', query_lower):\n                requested_lang = lang\n                break\n=======\n        # 1. Detect requested language from query (case-insensitive)\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        # Support alias and synonyms for language detection\n        lang_aliases = {\n            'python': ['python', 'py'],\n            'go': ['go', 'golang'],\n            'java': ['java']\n        }\n        for lang, aliases in lang_aliases.items():\n            if any(re.search(r'\\b' + alias + r'\\b', query_lower) for alias in aliases):\n                requested_lang = lang\n                break\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # 3. Cross-language filtering: filter candidates by requested language to reduce noise\n        def chunk_language_filter(chunk_text: str) -> bool:\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            # Accept if chunk contains a header for the requested language\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            # Accept if chunk prefix or tags mention requested language\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # Accept if code chunk with no explicit language tag (fallback)\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n=======\n        # 3. Cross-language filtering: filter candidates by requested language to reduce noise\n        def chunk_language_filter(chunk_text: str) -> bool:\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            # Accept if chunk contains a header for the requested language (support [LANG=XXX] tags)\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            if re.search(r'\\[LANG=' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # Accept if chunk prefix or tags mention requested language\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # Accept if code chunk with no explicit language tag (fallback)\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # 6. Generation (Strict Grounding) with improved prompt enforcing Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n=======\n        # 6. Generation (Strict Grounding) with improved prompt enforcing Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\"\n            \"5. When quoting code, explicitly indicate the programming language of the snippet.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n            res = self.llm.invoke(prompt)\n            gc.collect()\n\n            return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n=======\n            res = self.llm.invoke(prompt)\n            gc.collect()\n\n            return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n>>>>>>> REPLACE"]}}, "artifacts_json": "{\"raw_scores\": [0.2916666666666667, 0.515, 0.52, 0.5, 0.25, 0.5, 0.475, 0.5364285714285715, 0.25, 0.30000000000000004], \"error_details\": [{\"sample\": 4, \"type\": \"code_error\", \"message\": \"[\\\"Import Error on line 1: No module named 'google.ai'\\\"]\", \"bad_code\": \"from google.ai.generativeai import LlmAgent\\n\\ndef main():\\n    # Initialize the LlmAgent\\n    agent = LlmAgent()\\n\\n    # Define the question to ask\\n    question = \\\"What is the capital of France?\\\"\\n\\n    # Run the agent to get the answer\\n    answer = agent.run(question)\\n\\n    # Print the answer\\n    print(\\\"Answer:\\\", answer)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\"}, {\"sample\": 5, \"type\": \"code_error\", \"message\": \"[\\\"Import Error on line 1: No module named 'google.actions'\\\"]\", \"bad_code\": \"from google.actions.sdk.v2.agent import CustomAgent, SubAgent, SessionState\\n\\nclass MyCustomAgent(CustomAgent):\\n    def route(self, session_state: SessionState):\\n        # Quote:\\n        # \\\"CustomAgent\\\" is the base class for creating custom routing logic.\\n        # \\\"route\\\" method can be overridden to implement routing based on session state.\\n        # \\\"SessionState\\\" contains the current session data, which can be inspected to decide routing.\\n\\n        # Example routing logic based on a value in session state\\n        sub_agent_key = session_state.data.get('sub_agent_key')\\n\\n        if sub_agent_key == 'agent_a':\\n            return SubAgent('agent_a')\\n        elif sub_agent_key == 'agent_b':\\n            return SubAgent('agent_b')\\n        else:\\n            return SubAgent('default_agent')\"}, {\"sample\": 7, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 33)\", \"bad_code\": \"# Python\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\nfrom google.adk.runtime import Runner\\n\\n# Define three LlmAgents in sequence, each passing data via output_key\\nvalidator = LlmAgent(\\n    name=\\\"ValidateInput\\\",\\n    instruction=\\\"Validate the input.\\\",\\n    output_key=\\\"validation_status\\\"\\n)  # Saves output to state['validation_status']\\n\\nprocessor = LlmAgent(\\n    name=\\\"ProcessData\\\",\\n    instruction=\\\"Process data if {validation_status} is 'valid'.\\\",\\n    output_key=\\\"result\\\"\\n)  # Reads state['validation_status'], saves to state['result']\\n\\nreporter = LlmAgent(\\n    name=\\\"ReportResult\\\",\\n    instruction=\\\"Report the result from {result}.\\\"\\n)  # Reads state['result']\\n\\n# Create the SequentialAgent pipeline with the three sub_agents\\ndata_pipeline = SequentialAgent(\\n    name=\\\"DataPipeline\\\",\\n    sub_agents=[validator, processor, reporter]\\n)\\n\\n# Run the pipeline using the Runner\\nrunner = Runner(agent=data_pipeline)\\nrunner.run()\\n\\n> validator = LlmAgent(name=\\\"ValidateInput\\\", instruction=\\\"Validate the input.\\\", output_key=\\\"validation_status\\\")\\n> processor = LlmAgent(name=\\\"ProcessData\\\", instruction=\\\"Process data if {validation_status} is 'valid'.\\\", output_key=\\\"result\\\")\\n> reporter = LlmAgent(name=\\\"ReportResult\\\", instruction=\\\"Report the result from {result}.\\\")\\n>  \\n> data_pipeline = SequentialAgent(\\n> name=\\\"DataPipeline\\\",\\n> sub_agents=[validator, processor, reporter]\\n> )\\n> # validator runs -> saves to state['validation_status']\\n> # processor runs -> reads state['validation_status'], saves to state['result']\\n> # reporter runs -> reads state['result']\\n>\"}, {\"sample\": 8, \"type\": \"code_error\", \"message\": \"[\\\"Instantiation Error on line 32: name 'LoopAgent' is not defined\\\"]\", \"bad_code\": \"class LoopAgent:\\n    def escalate(self):\\n        # This method is used to exit the loop early when a condition is met\\n        pass\\n\\ndef run_loop(agent: LoopAgent):\\n    while True:\\n        # perform some work\\n        if some_condition:\\n            agent.escalate()  # exit loop early\\n        # continue looping otherwise\\n\\nclass LoopAgent:\\n    def escalate(self):\\n        # Implementation to exit the loop early\\n        raise StopIteration(\\\"Exiting loop early\\\")\\n\\ndef run_loop(agent: LoopAgent):\\n    try:\\n        while True:\\n            # perform some work\\n            if some_condition():\\n                agent.escalate()  # exit loop early\\n            # continue looping otherwise\\n    except StopIteration:\\n        print(\\\"Loop exited early due to escalate()\\\")\\n\\ndef some_condition():\\n    # Replace with actual condition logic\\n    return True\\n\\nagent = LoopAgent()\\nrun_loop(agent)\"}, {\"sample\": 9, \"type\": \"code_error\", \"message\": \"[\\\"Import Error on line 1: No module named 'google.ai'\\\", \\\"Import Error on line 6: No module named 'google.ai'\\\", \\\"Import Error on line 7: No module named 'google.ai'\\\", \\\"Import Error on line 8: No module named 'google.ai'\\\", \\\"Import Error on line 9: No module named 'google.ai'\\\"]\", \"bad_code\": \"from google.ai.generative.language_v1beta2 import (\\n    TextServiceAsyncClient,\\n    GenerateTextRequest,\\n    TextPrompt,\\n)\\nfrom google.ai.generative.language_v1beta2.types import SafetySettings\\nfrom google.ai.generative.language_v1beta2.types import SafetySetting\\nfrom google.ai.generative.language_v1beta2.types import SafetyCategory\\nfrom google.ai.generative.language_v1beta2.types import SafetyRating\\nimport asyncio\\n\\nclass CustomStoryAgent:\\n    def __init__(self, model_name: str = \\\"models/story-generator\\\"):\\n        self.client = TextServiceAsyncClient()\\n        self.model_name = model_name\\n\\n    async def _run_async_impl(self, prompt: str) -> str:\\n        # Generate story\\n        generate_request = GenerateTextRequest(\\n            model=self.model_name,\\n            prompt=TextPrompt(text=prompt),\\n            safety_settings=[\\n                SafetySetting(\\n                    category=SafetyCategory.TOXICITY,\\n                    threshold=SafetyRating.HIGH,\\n                )\\n            ],\\n        )\\n        response = await self.client.generate_text(request=generate_request)\\n        story = response.candidates[0].output\\n\\n        # Check tone (simplified as checking for negative words)\\n        if self._is_tone_negative(story):\\n            # Regenerate story if tone is negative\\n            regenerate_response = await self.client.generate_text(request=generate_request)\\n            story = regenerate_response.candidates[0].output\\n\\n        return story\\n\\n    def _is_tone_negative(self, text: str) -> bool:\\n        negative_keywords = [\\\"sad\\\", \\\"angry\\\", \\\"upset\\\", \\\"negative\\\", \\\"bad\\\"]\\n        return any(word in text.lower() for word in negative_keywords)\\n\\n# Example usage:\\n# async def main():\\n#     agent = CustomStoryAgent()\\n#     story = await agent._run_async_impl(\\\"Write a happy story about a dog.\\\")\\n#     print(story)\\n#\\n# asyncio.run(main())\"}]}", "artifact_dir": null, "embedding": null}