{"id": "6d67c591-ee07-4362-addd-bca584a4296b", "code": "import os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags to help reranker identify language and completeness\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                # Try detect language from breadcrumb or content heuristics\n                if re.search(r'```python', d.page_content, re.I):\n                    lang_tag = \"[LANG=Python]\"\n                elif re.search(r'```go', d.page_content, re.I):\n                    lang_tag = \"[LANG=Go]\"\n                elif re.search(r'```java', d.page_content, re.I):\n                    lang_tag = \"[LANG=Java]\"\n            # Detect if chunk likely contains a complete API definition or partial snippet\n            completeness_tag = \"[COMPLETE]\" if re.search(\n                r'^\\s*(class|func|interface|def|type|package|func\\s+main)\\s', d.page_content, re.I|re.M) else \"[PARTIAL]\"\n            \n            content_snippet = d.page_content[:1300].replace('\\n', ' ')\n            packed.append(f\"[{i}] {lang_tag} {completeness_tag} {content_snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 on relevance to the query, prioritizing:\\n\"\n            \"1) Exact API names or full definitions over partial snippets.\\n\"\n            \"2) Code examples executable in the requested language.\\n\"\n            \"3) Language requested by user query (detect language tags).\\n\"\n            \"4) Prefer chunks with complete API definitions or full function/class blocks.\\n\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON ONLY with format: {'scores': {chunk_id: score, ...}}\\n\"\n            \"Scores must be integers 0 to 100.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "1472eb02-cbc7-43bf-809a-8a6734ab8593", "generation": 2, "timestamp": 1769720085.0860934, "iteration_found": 12, "metrics": {"combined_score": 0.552904761904762, "num_samples": 10}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 9 lines with 9 lines\nChange 2: Replace 18 lines with 24 lines\nChange 3: Replace 47 lines with 50 lines\nChange 4: Replace 51 lines with 73 lines", "parent_metrics": {"combined_score": 0.5236785714285714, "num_samples": 10}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer for Google ADK documentation. Improve the code to maximize RAG-pipeline performance.\n\nThe current architecture uses a **Parent-Child Retrieval** system with **Hybrid Search** and **LLM Reranking**.\n\n### High-Leverage Evolution Points:\n\n1. **Hyperparameter Synergy**: Optimize the balance between `chunk_size` (for retrieval precision), `top_k_vector/bm25` (for recall), and `weight_vector/bm25`. The documentation has many exact API names which may favor BM25, while conceptual \"how-to\" queries favor Vector.\n2. **Reranker Prompt Engineering**: The `_llm_rerank` prompt is critical. Evolve it to better identify relevance, handle multi-language snippets, or detect if a chunk contains a complete API definition vs. a partial snippet.\n3. **Metadata-Rich Ingestion**: Enhance `_chunk_document` to include more context in `breadcrumb` and `tagging`. Consider adding language-specific tags or project-area identifiers (e.g. `[A2A]`, `[WORKFLOW]`) to help the reranker and generator.\n4. **Context Assembly & Prompting**: Optimize the `context_block` formatting in `query`. Experiment with summarizing parent context or using XML-style tags to separate sources. Refine the final generation prompt for better \"Quote then Explain\" adherence.\n5. **Cross-Language Filtering**: Implement logic to detect the user's requested language and prioritize or filter retrieved chunks to reduce cross-language noise.\n\n**STRICT RULES**:\n- **Fork-Safety**: Avoid any top-level imports of `langchain`, `agno`, or `openai`. Use lazy imports inside methods.\n- **Compatibility**: Ensure `evaluate_rag(docs_path, query)` maintains its signature.\n- **Cleanup**: Always call `gc.collect()` after LLM calls or retrieval to maintain memory stability.\n\n**Example Improvement (Reranker Logic)**:\n```python\n# You can evolve the reranker prompt to be more specific:\nprompt = (\n    \"Role: ADK Expert Reranker. \"\n    \"Goal: Select chunks that provide EXECUTABLE code for the query. \"\n    f\"Query: {query}\\n\"\n    \"Chunks: ... \"\n)\n```\n- Maintain `evaluate_rag(docs_path, query)` signature and return keys: `answer`, `contexts`.\n- Keep `RAGSystem` class structure and lazy-loading in `__init__`.\n", "user": "# Current Program Information\n- Fitness: 0.5237\n- Feature coordinates: combined_score=0.52\n- Focus areas: - Fitness unchanged at 0.5237\n- Exploring combined_score=0.52 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### raw_scores\n```\n[0.40166666666666667, 0.5041666666666667, 0.6391666666666667, 0.5928571428571429, 0.3375, 0.44000000000000006, 0.5750000000000001, 0.6630952380952382, 0.4916666666666667, 0.5916666666666667]\n```\n\n### error_details\n```\n[{'sample': 3, 'type': 'code_error', 'message': 'Syntax Error: invalid syntax (<unknown>, line 1)', 'bad_code': '> def get_weather(city: str) -> dict:  \\n>     \"\"\"Retrieves weather for a city.\"\"\"  \\n>     return {\"status\": \"success\", \"temperature\": \"25\u00b0C\"}  \\n>\\n\\nfrom google.adk.tools import FunctionTool\\n\\ndef get_weather(city: str) -> dict:\\n    \"\"\"Retrieves weather for a city.\"\"\"\\n    # Your logic here, e.g., call a weather API\\n    return {\"status\": \"success\", \"temperature\": \"25\u00b0C\"}\\n\\n# Wrap the function as a FunctionTool\\nweather_tool = FunctionTool(func=get_weather)'}, {'sample': 4, 'type': 'code_error', 'message': 'Syntax Error: invalid syntax (<unknown>, line 41)', 'bad_code': 'import asyncio\\n\\nfrom google.genai import types\\nfrom google.adk.agents.llm_agent import LlmAgent\\nfrom google.adk.runners import Runner\\nfrom google.adk.sessions import InMemorySessionService\\n\\n# --- Step 1: Define the agent ---\\nagent = LlmAgent(\\n    model=\"gemini-2.0-flash\",  # Specify your model name here\\n    name=\"simple_qa_agent\",\\n    instruction=\"You are a helpful assistant that answers questions.\"\\n)\\n\\n# --- Step 2: Set up session service and runner ---\\nAPP_NAME = \"simple_qa_app\"\\nUSER_ID = \"user_001\"\\nSESSION_ID = \"session_001\"\\n\\nsession_service = InMemorySessionService()\\nsession_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\\n\\nrunner = Runner(agent=agent, app_name=APP_NAME, session_service=session_service)\\n\\n# --- Step 3: Define a function to call the agent ---\\ndef call_agent(query: str):\\n    content = types.Content(role=\\'user\\', parts=[types.Part(text=query)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        # Print debug info for each event\\n        print(f\"\\\\nDEBUG EVENT: {event}\\\\n\")\\n        if event.is_final_response() and event.content:\\n            final_answer = event.content.parts[0].text.strip()\\n            print(\"\\\\n\ud83d\udfe2 FINAL ANSWER\\\\n\", final_answer, \"\\\\n\")\\n\\n# --- Step 4: Run the agent with a sample question ---\\nif __name__ == \"__main__\":\\n    call_agent(\"What is the capital of France?\")\\n\\n> from google.adk.agents.llm_agent import LlmAgent\\n> from google.adk.runners import Runner\\n> from google.adk.sessions import InMemorySessionService\\n> from google.genai import types\\n> \\n> agent = LlmAgent(\\n>     model=\"gemini-2.0-flash\",\\n>     name=\"simple_qa_agent\",\\n>     instruction=\"You are a helpful assistant that answers questions.\"\\n> )\\n> \\n> session_service = InMemorySessionService()\\n> session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\\n> \\n> runner = Runner(agent=agent, app_name=APP_NAME, session_service=session_service)\\n> \\n> def call_agent(query: str):\\n>     content = types.Content(role=\\'user\\', parts=[types.Part(text=query)])\\n>     events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n> \\n>     for event in events:\\n>         if event.is_final_response() and event.content:\\n>             final_answer = event.content.parts[0].text.strip()\\n>             print(final_answer)\\n> \\n> call_agent(\"What is the capital of France?\")\\n>'}, {'sample': 5, 'type': 'code_error', 'message': 'Syntax Error: invalid syntax (<unknown>, line 1)', 'bad_code': '> async for event in self.some_sub_agent.run_async(ctx):\\n>     yield event\\n>\\n\\n> previous_result = ctx.session.state.get(\"some_key\")\\n> if previous_result == \"some_value\":\\n>     # call a specific sub-agent\\n> else:\\n>     # call another sub-agent\\n>\\n\\nfrom typing import AsyncGenerator\\nfrom google.adk.agents import BaseAgent, LlmAgent\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom google.adk.events import Event\\n\\nclass RoutingAgent(BaseAgent):\\n    # Define sub-agents as attributes\\n    agent_a: LlmAgent\\n    agent_b: LlmAgent\\n\\n    model_config = {\"arbitrary_types_allowed\": True}\\n\\n    def __init__(self, name: str, agent_a: LlmAgent, agent_b: LlmAgent):\\n        sub_agents_list = [agent_a, agent_b]\\n        super().__init__(name=name, agent_a=agent_a, agent_b=agent_b, sub_agents=sub_agents_list)\\n\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        # Read routing key from session state\\n        route_key = ctx.session.state.get(\"route_key\")\\n\\n        if route_key == \"use_agent_a\":\\n            # Route to agent_a\\n            async for event in self.agent_a.run_async(ctx):\\n                yield event\\n        elif route_key == \"use_agent_b\":\\n            # Route to agent_b\\n            async for event in self.agent_b.run_async(ctx):\\n                yield event\\n        else:\\n            # Default or fallback behavior\\n            # For example, yield a simple event or raise an error\\n            from google.adk.events import Event\\n            yield Event(\\n                author=\"RoutingAgent\",\\n                content=None,\\n                text=f\"No valid route_key found in session state: {route_key}\"\\n            )'}, {'sample': 6, 'type': 'code_error', 'message': 'Syntax Error: invalid syntax (<unknown>, line 1)', 'bad_code': '> from google.adk.tools import ToolContext, FunctionTool\\n> \\n> def update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n>     \"\"\"Updates a user-specific preference.\"\"\"\\n>     user_prefs_key = \"user:preferences\"\\n>     # Get current preferences or initialize if none exist\\n>     preferences = tool_context.state.get(user_prefs_key, {})\\n>     preferences[preference] = value\\n>     # Write the updated dictionary back to the state\\n>     tool_context.state[user_prefs_key] = preferences\\n>     print(f\"Tool: Updated user preference \\'{preference}\\' to \\'{value}\\'\")\\n>     return {\"status\": \"success\", \"updated_preference\": preference}\\n> \\n> pref_tool = FunctionTool(func=update_user_preference)\\n>'}, {'sample': 7, 'type': 'code_error', 'message': 'Syntax Error: invalid syntax (<unknown>, line 1)', 'bad_code': '> from google.adk.agents import SequentialAgent, LlmAgent\\n> \\n> validator = LlmAgent(name=\"ValidateInput\", instruction=\"Validate the input.\", output_key=\"validation_status\")\\n> processor = LlmAgent(name=\"ProcessData\", instruction=\"Process data if {validation_status} is \\'valid\\'.\", output_key=\"result\")\\n> reporter = LlmAgent(name=\"ReportResult\", instruction=\"Report the result from {result}.\")\\n> \\n> data_pipeline = SequentialAgent(\\n>     name=\"DataPipeline\",\\n>     sub_agents=[validator, processor, reporter]\\n> )\\n> # validator runs -> saves to state[\\'validation_status\\']\\n> # processor runs -> reads state[\\'validation_status\\'], saves to state[\\'result\\']\\n> # reporter runs -> reads state[\\'result\\']\\n>\\n\\n> from google.adk.agents import SequentialAgent, LlmAgent\\n> from google.adk.runner import InMemoryRunner\\n> from google.adk.sessions import Session\\n> from google.genai.types import Content, Part\\n> \\n> # Define sub-agents with output_key to pass data via state\\n> agent1 = LlmAgent(\\n>     name=\"Agent1\",\\n>     instruction=\"Generate initial data.\",\\n>     output_key=\"data1\"\\n> )\\n> agent2 = LlmAgent(\\n>     name=\"Agent2\",\\n>     instruction=\"Process the data: {data1}\",\\n>     output_key=\"data2\"\\n> )\\n> agent3 = LlmAgent(\\n>     name=\"Agent3\",\\n>     instruction=\"Summarize the processed data: {data2}\"\\n> )\\n> \\n> pipeline = SequentialAgent(\\n>     name=\"MySequentialPipeline\",\\n>     sub_agents=[agent1, agent2, agent3]\\n> )\\n> \\n> # Create runner and session\\n> runner = InMemoryRunner(pipeline, \"MySequentialPipeline\")\\n> session = runner.sessionService().createSession(\"MySequentialPipeline\", \"user_123\").blockingGet()\\n> \\n> # Create user message content (empty or initial prompt)\\n> user_message = Content.fromParts(Part.fromText(\"Start the pipeline\"))\\n> \\n> # Run the pipeline\\n> event_stream = runner.runAsync(\"user_123\", session.id(), user_message)\\n> \\n> # Print final response\\n> for event in event_stream.blockingIterable():\\n>     if event.finalResponse():\\n>         print(event.stringifyContent())\\n>\\n\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\nfrom google.adk.runner import InMemoryRunner\\nfrom google.adk.sessions import Session\\nfrom google.genai.types import Content, Part\\n\\ndef main():\\n    # Define three LlmAgents in sequence, passing data via output_key\\n    agent1 = LlmAgent(\\n        name=\"Agent1\",\\n        instruction=\"Generate initial data.\",\\n        output_key=\"data1\"\\n    )\\n    agent2 = LlmAgent(\\n        name=\"Agent2\",\\n        instruction=\"Process the data: {data1}\",\\n        output_key=\"data2\"\\n    )\\n    agent3 = LlmAgent(\\n        name=\"Agent3\",\\n        instruction=\"Summarize the processed data: {data2}\"\\n    )\\n\\n    # Create the SequentialAgent pipeline\\n    pipeline = SequentialAgent(\\n        name=\"MySequentialPipeline\",\\n        sub_agents=[agent1, agent2, agent3]\\n    )\\n\\n    # Create an InMemoryRunner to run the pipeline\\n    runner = InMemoryRunner(pipeline, \"MySequentialPipeline\")\\n\\n    # Create a session for a user\\n    session = runner.sessionService().createSession(\"MySequentialPipeline\", \"user_123\").blockingGet()\\n\\n    # Create initial user message content to start the pipeline\\n    user_message = Content.fromParts(Part.fromText(\"Start the pipeline\"))\\n\\n    # Run the pipeline asynchronously\\n    event_stream = runner.runAsync(\"user_123\", session.id(), user_message)\\n\\n    # Iterate over events and print the final response content\\n    for event in event_stream.blockingIterable():\\n        if event.finalResponse():\\n            print(event.stringifyContent())\\n\\nif __name__ == \"__main__\":\\n    main()'}, {'sample': 8, 'type': 'code_error', 'message': '[\"Instantiation Error on line 28: name \\'exit_loop\\' is not defined\", \"Instantiation Error on line 40: name \\'refiner_agent\\' is not defined\"]', 'bad_code': 'from google.adk.agents import LoopAgent, LlmAgent\\nfrom google.adk.tools.tool_context import ToolContext\\n\\n# Define a tool function that signals the loop to exit by escalating\\ndef exit_loop(tool_context: ToolContext):\\n    print(f\"[Tool Call] exit_loop triggered by {tool_context.agent_name}\")\\n    tool_context.actions.escalate = True\\n    return {}\\n\\n# Define sub-agents\\nwriter_agent = LlmAgent(\\n    name=\"WriterAgent\",\\n    model=\"gemini-2.0-flash\",\\n    instruction=\"Write initial content.\",\\n    output_key=\"document\"\\n)\\n\\ncritic_agent = LlmAgent(\\n    name=\"CriticAgent\",\\n    model=\"gemini-2.0-flash\",\\n    instruction=\"\"\"\\n    Review the document and if it is good enough, respond with \"STOP\".\\n    Otherwise, provide improvement suggestions.\\n    \"\"\",\\n    output_key=\"critique\"\\n)\\n\\nrefiner_agent = LlmAgent(\\n    name=\"RefinerAgent\",\\n    model=\"gemini-2.0-flash\",\\n    instruction=\"\"\"\\n    If critique is exactly \"STOP\", call the exit_loop tool.\\n    Else, refine the document based on critique.\\n    \"\"\",\\n    tools=[exit_loop],\\n    output_key=\"document\"\\n)\\n\\n# Create the LoopAgent with max_iterations and sub-agents\\nloop_agent = LoopAgent(\\n    name=\"RetryLoopAgent\",\\n    sub_agents=[critic_agent, refiner_agent],\\n    max_iterations=5  # Optional max iteration limit\\n)'}]\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 9 lines with 19 lines\nChange 2: Replace 34 lines with 48 lines\nChange 3: Replace 37 lines with 59 lines\nChange 4: Replace 32 lines with 45 lines\n- Metrics: combined_score: 0.5237, num_samples: 10.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 9 lines with 15 lines\nChange 2: Replace 34 lines with 40 lines\nChange 3: Replace 37 lines with 53 lines\nChange 4: Replace 38 lines with 58 lines\n- Metrics: combined_score: 0.5395, num_samples: 10.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.5461, num_samples: 10.0000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.5461)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            content = d.page_content[:1500]\n            packed.append(f\"[{i}] {content}\")\n        \n        prompt = (\n            \"You are a reranker for Google ADK documentation. \"\n            \"Given a user query and a list of chunks, score each chunk from 0 to 100 based on its relevance. \"\n            \"Prioritize chunks that contain exact API names or code examples for the requested task. \"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_block += f\"--- SOURCE: {src} | SECTION: {breadcrumb} ---\\n{parent.page_content}\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5461), Performs well on num_samples (10.0000)\n\n### Program 2 (Score: 0.5395)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add language and project-area tags for better filtering and relevance\n            lang_match = re.search(r'```(\\w+)', p.page_content)\n            language = lang_match.group(1).lower() if lang_match else \"unknown\"\n            p.metadata[\"language\"] = language\n            \n            # Heuristic project-area tagging based on breadcrumbs or source file path\n            if re.search(r'a2a|auth', breadcrumb, re.I) or re.search(r'a2a|auth', source, re.I):\n                p.metadata[\"project_area\"] = \"[A2A]\"\n            elif re.search(r'workflow', breadcrumb, re.I) or re.search(r'workflow', source, re.I):\n                p.metadata[\"project_area\"] = \"[WORKFLOW]\"\n            else:\n                p.metadata[\"project_area\"] = \"[GENERAL]\"\n                \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb} {p.metadata['project_area']}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = language\n                c.metadata[\"project_area\"] = p.metadata[\"project_area\"]\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags and breadcrumb in reranking context for better relevance detection\n            meta = d.metadata.get(\"breadcrumb\", \"\") + \" \" + d.metadata.get(\"source\", \"\")\n            content = d.page_content[:1400]\n            packed.append(f\"[{i}] Metadata: {meta}\\nContent: {content}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk 0-100 on relevance to the query.\\n\"\n            \"Prioritize:\\n\"\n            \"- chunks containing exact API names matching the query,\\n\"\n            \"- chunks with executable code snippets,\\n\"\n            \"- chunks containing complete API definitions rather than partial fragments.\\n\"\n            \"- consider multi-language code snippets and mark those matching the user's requested language.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Cross-language filtering: detect requested language from query and filter candidates\n        requested_lang = None\n        lang_pattern = re.compile(r'\\b(python|go|java)\\b', re.I)\n        lang_match = lang_pattern.search(query_str)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n        \n        if requested_lang:\n            candidates = [c for c in candidates if c.metadata.get(\"language\", \"\") == requested_lang]\n            # If filtered too strictly, fallback to original set\n            if len(candidates) < 3:\n                candidates = self.retriever.invoke(query_str)\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Summarize parent context titles for brevity and clarity\n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Use XML style tags for clarity and better chunk separation\n                context_block += (\n                    f\"<source file='{src}' section='{breadcrumb}'>\\n\"\n                    f\"{parent.page_content}\\n\"\n                    f\"</source>\\n\\n\"\n                )\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5395), Performs well on num_samples (10.0000)\n\n### Program 3 (Score: 0.5237)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags to help reranker identify language and completeness\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                # Try detect language from breadcrumb or content heuristics\n                if re.search(r'```python', d.page_content, re.I):\n                    lang_tag = \"[LANG=Python]\"\n                elif re.search(r'```go', d.page_content, re.I):\n                    lang_tag = \"[LANG=Go]\"\n                elif re.search(r'```java', d.page_content, re.I):\n                    lang_tag = \"[LANG=Java]\"\n            completeness_tag = \"[COMPLETE]\" if re.search(r'^\\s*(class|func|interface|def|type)\\s', d.page_content, re.I|re.M) else \"[PARTIAL]\"\n            \n            content_snippet = d.page_content[:1300].replace('\\n', ' ')\n            packed.append(f\"[{i}] {lang_tag} {completeness_tag} {content_snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 on relevance to the query, prioritizing:\\n\"\n            \"1) Exact API names or full definitions over partial snippets.\\n\"\n            \"2) Code examples executable in the requested language.\\n\"\n            \"3) Language requested by user query (detect language tags).\\n\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON ONLY with format: {'scores': {chunk_id: score, ...}}\\n\"\n            \"Scores must be integers 0 to 100.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5237), Performs well on num_samples (10.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4457)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            # Add language tag in header for reranker and generator\n            lang_tag = label.upper()\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n\n        # Remove tabs wrappers but keep tabs content flattened with language tags\n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            # Add project-area tags heuristically based on breadcrumb content for better reranking\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            tags = []\n            if re.search(r'\\bworkflow\\b', breadcrumb, re.I):\n                tags.append(\"[WORKFLOW]\")\n            if re.search(r'\\ba2a\\b', breadcrumb, re.I):\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bapi\\b', breadcrumb, re.I):\n                tags.append(\"[API]\")\n            tag_str = \" \".join(tags)\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            p.metadata[\"tags\"] = tag_str\n            self.parent_docs[parent_id] = p\n\n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Incorporate tags into chunk content for reranker awareness\n                c.page_content = f\"{prefix} {tag_str} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"tags\"] = tag_str\n                child_chunks.append(c)\n\n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n\n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1500]\n            # Add explicit tag info if available to help reranker detect language and completeness\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker. \"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query. \"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n\n        # Determine requested language from query for filtering\n        requested_lang = None\n        lang_match = re.search(r'\\b(Python|Go|Java)\\b', query_str, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).capitalize()\n\n        # Filter candidates by language tag if language requested\n        if requested_lang:\n            filtered_candidates = []\n            for d in candidates:\n                tags = d.metadata.get(\"tags\", \"\")\n                # If chunk has explicit language tag from _flatten_markdown_tabs, keep it\n                if f\"[{requested_lang.upper()}]\" in d.page_content or f\"[{requested_lang.upper()}]\" in tags:\n                    filtered_candidates.append(d)\n            if filtered_candidates:\n                candidates = filtered_candidates\n\n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n\n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n\n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Use XML-like tags for clearer source separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. Only provide code or explanations in the user's requested language if specified; otherwise, default to Python but mention that other languages are available.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n\n        res = self.llm.invoke(prompt)\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n### Program D2 (Score: 0.4947)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if found in breadcrumb or source filename\n            tags = []\n            if re.search(r'\\bA2A\\b', breadcrumb, re.I) or 'a2a' in source.lower():\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bWORKFLOW\\b', breadcrumb, re.I) or 'workflow' in source.lower():\n                tags.append(\"[WORKFLOW]\")\n            if tags:\n                p.metadata[\"tags\"] = \" \".join(tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation and language tags if present\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                \n                # Add language tag from breadcrumb if detected\n                lang_tag = \"\"\n                lang_matches = re.findall(r'#### (\\w+) Implementation', c.page_content)\n                if lang_matches:\n                    lang_tag = f\"[{lang_matches[0].upper()}]\"\n                \n                # Add tags from parent metadata\n                tag_str = p.metadata.get(\"tags\", \"\")\n                \n                c.page_content = f\"{prefix} {lang_tag} {tag_str} {breadcrumb}\\n{c.page_content}\".strip()\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with improved prompt for multi-language and completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            # Indicate if chunk likely contains a full API def or partial snippet heuristically\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'class\\s+\\w+|func\\s+\\w+|def\\s+\\w+', snippet, re.I) else \"[PARTIAL]\"\n            packed.append(f\"[{i}] {completeness_tag} {snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Task: Given a user query and a list of documentation chunks, assign a relevance score (0-100) to each chunk.\\n\"\n            \"Prioritize:\\n\"\n            \"- Chunks containing complete API definitions or well-formed code examples.\\n\"\n            \"- Chunks with exact API names matching the query.\\n\"\n            \"- Chunks in the user's requested programming language if specified (Python, Go, Java).\\n\"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Output JSON with format: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting with XML-style tags and language filtering\n        final_contexts = []\n        seen_parents = set()\n        context_blocks = []\n        \n        # Detect requested language from query (case-insensitive)\n        requested_lang = None\n        lang_candidates = ['python', 'go', 'java']\n        query_lower = query_str.lower()\n        for lang in lang_candidates:\n            if re.search(r'\\b' + lang + r'\\b', query_lower):\n                requested_lang = lang\n                break\n        \n        def chunk_language_filter(chunk_text: str) -> bool:\n            # Simple heuristic: check for language tags or code fences\n            if requested_lang is None:\n                return True\n            lang_tag = requested_lang.capitalize()\n            if f\"#### {lang_tag} Implementation\" in chunk_text:\n                return True\n            # Also accept if chunk mentions requested language in prefix or breadcrumb\n            if re.search(r'\\[' + requested_lang.upper() + r'\\]', chunk_text, re.I):\n                return True\n            # If chunk is code and no explicit language, accept by default\n            if chunk_text.startswith(\"[DOC_CODE]\"):\n                return True\n            return False\n        \n        for c in top_chunks:\n            if not chunk_language_filter(c.page_content):\n                continue\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent by language as well\n                if not chunk_language_filter(parent.page_content):\n                    continue\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_blocks.append(f\"<source file='{src}' section='{breadcrumb}'>\\n{parent.page_content}\\n</source>\")\n        \n        context_block = \"\\n\\n\".join(context_blocks)\n        \n        # 4. Generation (Strict Grounding) with improved prompt encouraging Quote then Explain and language adherence\n        prompt = (\n            \"You are a Google ADK technical expert assistant. Answer the user's question strictly using the provided context.\\n\"\n            \"Rules:\\n\"\n            \"1. If the answer is not contained within the context, respond exactly: 'I don't know from the provided documentation'.\\n\"\n            \"2. Always mention API classes or methods using the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' approach: first quote the relevant passage or code, then provide your explanation.\\n\"\n            \"4. Only provide code snippets or examples in the user-requested programming language (Python, Go, Java). If no language is requested, default to Python but acknowledge other languages exist in the docs.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to num_samples\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.5461, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            content = d.page_content[:1500]\n            packed.append(f\"[{i}] {content}\")\n        \n        prompt = (\n            \"You are a reranker for Google ADK documentation. \"\n            \"Given a user query and a list of chunks, score each chunk from 0 to 100 based on its relevance. \"\n            \"Prioritize chunks that contain exact API names or code examples for the requested task. \"\n            f\"Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON: {'scores': {chunk_id: score, ...}}\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                context_block += f\"--- SOURCE: {src} | SECTION: {breadcrumb} ---\\n{parent.page_content}\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n### Inspiration 2 (Score: 0.5395, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add language and project-area tags for better filtering and relevance\n            lang_match = re.search(r'```(\\w+)', p.page_content)\n            language = lang_match.group(1).lower() if lang_match else \"unknown\"\n            p.metadata[\"language\"] = language\n            \n            # Heuristic project-area tagging based on breadcrumbs or source file path\n            if re.search(r'a2a|auth', breadcrumb, re.I) or re.search(r'a2a|auth', source, re.I):\n                p.metadata[\"project_area\"] = \"[A2A]\"\n            elif re.search(r'workflow', breadcrumb, re.I) or re.search(r'workflow', source, re.I):\n                p.metadata[\"project_area\"] = \"[WORKFLOW]\"\n            else:\n                p.metadata[\"project_area\"] = \"[GENERAL]\"\n                \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                c.page_content = f\"{prefix} {breadcrumb} {p.metadata['project_area']}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"language\"] = language\n                c.metadata[\"project_area\"] = p.metadata[\"project_area\"]\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags and breadcrumb in reranking context for better relevance detection\n            meta = d.metadata.get(\"breadcrumb\", \"\") + \" \" + d.metadata.get(\"source\", \"\")\n            content = d.page_content[:1400]\n            packed.append(f\"[{i}] Metadata: {meta}\\nContent: {content}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk 0-100 on relevance to the query.\\n\"\n            \"Prioritize:\\n\"\n            \"- chunks containing exact API names matching the query,\\n\"\n            \"- chunks with executable code snippets,\\n\"\n            \"- chunks containing complete API definitions rather than partial fragments.\\n\"\n            \"- consider multi-language code snippets and mark those matching the user's requested language.\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON with 'scores' mapping chunk_id to score (0-100).\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            # Rank indices by score\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Cross-language filtering: detect requested language from query and filter candidates\n        requested_lang = None\n        lang_pattern = re.compile(r'\\b(python|go|java)\\b', re.I)\n        lang_match = lang_pattern.search(query_str)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n        \n        if requested_lang:\n            candidates = [c for c in candidates if c.metadata.get(\"language\", \"\") == requested_lang]\n            # If filtered too strictly, fallback to original set\n            if len(candidates) < 3:\n                candidates = self.retriever.invoke(query_str)\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Summarize parent context titles for brevity and clarity\n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Use XML style tags for clarity and better chunk separation\n                context_block += (\n                    f\"<source file='{src}' section='{breadcrumb}'>\\n\"\n                    f\"{parent.page_content}\\n\"\n                    f\"</source>\\n\\n\"\n                )\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. If multiple languages are available (Python, Go, Java), only provide the one requested by the user. If none requested, default to Python but mention others exist.\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n### Inspiration 3 (Score: 0.4457, Type: Experimental)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            # Add language tag in header for reranker and generator\n            lang_tag = label.upper()\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n\n        # Remove tabs wrappers but keep tabs content flattened with language tags\n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            # Add project-area tags heuristically based on breadcrumb content for better reranking\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            tags = []\n            if re.search(r'\\bworkflow\\b', breadcrumb, re.I):\n                tags.append(\"[WORKFLOW]\")\n            if re.search(r'\\ba2a\\b', breadcrumb, re.I):\n                tags.append(\"[A2A]\")\n            if re.search(r'\\bapi\\b', breadcrumb, re.I):\n                tags.append(\"[API]\")\n            tag_str = \" \".join(tags)\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            p.metadata[\"tags\"] = tag_str\n            self.parent_docs[parent_id] = p\n\n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Incorporate tags into chunk content for reranker awareness\n                c.page_content = f\"{prefix} {tag_str} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"tags\"] = tag_str\n                child_chunks.append(c)\n\n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n\n        packed = []\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1500]\n            # Add explicit tag info if available to help reranker detect language and completeness\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker. \"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query. \"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Extract JSON robustly\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n\n        # Determine requested language from query for filtering\n        requested_lang = None\n        lang_match = re.search(r'\\b(Python|Go|Java)\\b', query_str, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).capitalize()\n\n        # Filter candidates by language tag if language requested\n        if requested_lang:\n            filtered_candidates = []\n            for d in candidates:\n                tags = d.metadata.get(\"tags\", \"\")\n                # If chunk has explicit language tag from _flatten_markdown_tabs, keep it\n                if f\"[{requested_lang.upper()}]\" in d.page_content or f\"[{requested_lang.upper()}]\" in tags:\n                    filtered_candidates.append(d)\n            if filtered_candidates:\n                candidates = filtered_candidates\n\n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n\n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n\n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Use XML-like tags for clearer source separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            \"4. Only provide code or explanations in the user's requested language if specified; otherwise, default to Python but mention that other languages are available.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n\n        res = self.llm.invoke(prompt)\n        gc.collect()\n\n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n# Current Program\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers.\"\"\"\n        # Simple regex for {% tab label=\"Python\" %} blocks\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1)\n            content = match.group(2)\n            return f\"\\n\\n#### {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags to help reranker identify language and completeness\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                # Try detect language from breadcrumb or content heuristics\n                if re.search(r'```python', d.page_content, re.I):\n                    lang_tag = \"[LANG=Python]\"\n                elif re.search(r'```go', d.page_content, re.I):\n                    lang_tag = \"[LANG=Go]\"\n                elif re.search(r'```java', d.page_content, re.I):\n                    lang_tag = \"[LANG=Java]\"\n            completeness_tag = \"[COMPLETE]\" if re.search(r'^\\s*(class|func|interface|def|type)\\s', d.page_content, re.I|re.M) else \"[PARTIAL]\"\n            \n            content_snippet = d.page_content[:1300].replace('\\n', ' ')\n            packed.append(f\"[{i}] {lang_tag} {completeness_tag} {content_snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 on relevance to the query, prioritizing:\\n\"\n            \"1) Exact API names or full definitions over partial snippets.\\n\"\n            \"2) Code examples executable in the requested language.\\n\"\n            \"3) Language requested by user query (detect language tags).\\n\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON ONLY with format: {'scores': {chunk_id: score, ...}}\\n\"\n            \"Scores must be integers 0 to 100.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n=======\n        self.chunk_size = 450  # Slightly smaller chunks for finer precision and better API snippet isolation\n        self.chunk_overlap = 120  # Increased overlap to preserve context across chunk boundaries\n        self.top_k_vector = 25  # Increase vector recall to capture conceptual queries better\n        self.top_k_bm25 = 15    # Slightly reduce bm25 to prioritize precision on exact matches\n        self.weight_vector = 0.6  # Favor semantic vector search for conceptual queries\n        self.weight_bm25 = 0.4    # Still keep BM25 weight for exact API name recall\n        self.rerank_top_n = 10    # Increase rerank candidates for better coverage\n        self.temperature = 0.0    # Keep generation deterministic for reproducibility\n        self.expand_query = False \n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Add tagging for better semantic separation\n        prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n        # Detect language from code fences for tagging\n        lang_tag = \"\"\n        lang_match = re.search(r'```(\\w+)', c.page_content)\n        if lang_match:\n            lang = lang_match.group(1).lower()\n            if lang in (\"python\", \"go\", \"java\"):\n                lang_tag = f\"[LANG={lang.capitalize()}]\"\n            else:\n                lang_tag = \"[LANG=Unknown]\"\n        else:\n            lang_tag = \"[LANG=Unknown]\"\n        # Add tags and breadcrumb into chunk content for better reranking and generation\n        c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n        c.metadata[\"parent_id\"] = parent_id\n        c.metadata[\"source\"] = source\n        child_chunks.append(c)\n=======\n        # Add tagging for better semantic separation\n        prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n        # Detect language from code fences for tagging and fallback with heuristic from parent\n        lang_tag = \"\"\n        lang_match = re.search(r'```(\\w+)', c.page_content)\n        if lang_match:\n            lang = lang_match.group(1).lower()\n            if lang in (\"python\", \"go\", \"java\"):\n                lang_tag = f\"[LANG={lang.capitalize()}]\"\n            else:\n                lang_tag = \"[LANG=Unknown]\"\n        else:\n            # Heuristic: if parent breadcrumb or tags mention language, assign that\n            parent_lang = None\n            for l in (\"Python\", \"Go\", \"Java\"):\n                if re.search(fr'\\b{l}\\b', breadcrumb, re.I) or re.search(fr'\\[{l.upper()}\\]', p.metadata.get(\"tags\", \"\"), re.I):\n                    parent_lang = l\n                    break\n            lang_tag = f\"[LANG={parent_lang}]\" if parent_lang else \"[LANG=Unknown]\"\n        # Add tags and breadcrumb into chunk content for better reranking and generation\n        c.page_content = f\"{prefix} {lang_tag} {breadcrumb} {p.metadata.get('tags','')}\\n{c.page_content}\"\n        c.metadata[\"parent_id\"] = parent_id\n        c.metadata[\"source\"] = source\n        child_chunks.append(c)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags to help reranker identify language and completeness\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                # Try detect language from breadcrumb or content heuristics\n                if re.search(r'```python', d.page_content, re.I):\n                    lang_tag = \"[LANG=Python]\"\n                elif re.search(r'```go', d.page_content, re.I):\n                    lang_tag = \"[LANG=Go]\"\n                elif re.search(r'```java', d.page_content, re.I):\n                    lang_tag = \"[LANG=Java]\"\n            completeness_tag = \"[COMPLETE]\" if re.search(r'^\\s*(class|func|interface|def|type)\\s', d.page_content, re.I|re.M) else \"[PARTIAL]\"\n            \n            content_snippet = d.page_content[:1300].replace('\\n', ' ')\n            packed.append(f\"[{i}] {lang_tag} {completeness_tag} {content_snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 on relevance to the query, prioritizing:\\n\"\n            \"1) Exact API names or full definitions over partial snippets.\\n\"\n            \"2) Code examples executable in the requested language.\\n\"\n            \"3) Language requested by user query (detect language tags).\\n\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON ONLY with format: {'scores': {chunk_id: score, ...}}\\n\"\n            \"Scores must be integers 0 to 100.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n=======\n        \"\"\"Rerank candidates using a small LLM.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        for i, d in enumerate(docs):\n            # Include metadata tags to help reranker identify language and completeness\n            lang_tag = \"[LANG=Unknown]\"\n            if \"[DOC_CODE]\" in d.page_content:\n                # Try detect language from breadcrumb or content heuristics\n                if re.search(r'```python', d.page_content, re.I):\n                    lang_tag = \"[LANG=Python]\"\n                elif re.search(r'```go', d.page_content, re.I):\n                    lang_tag = \"[LANG=Go]\"\n                elif re.search(r'```java', d.page_content, re.I):\n                    lang_tag = \"[LANG=Java]\"\n            # Detect if chunk likely contains a complete API definition or partial snippet\n            completeness_tag = \"[COMPLETE]\" if re.search(\n                r'^\\s*(class|func|interface|def|type|package|func\\s+main)\\s', d.page_content, re.I|re.M) else \"[PARTIAL]\"\n            \n            content_snippet = d.page_content[:1300].replace('\\n', ' ')\n            packed.append(f\"[{i}] {lang_tag} {completeness_tag} {content_snippet}\")\n        \n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 on relevance to the query, prioritizing:\\n\"\n            \"1) Exact API names or full definitions over partial snippets.\\n\"\n            \"2) Code examples executable in the requested language.\\n\"\n            \"3) Language requested by user query (detect language tags).\\n\"\n            \"4) Prefer chunks with complete API definitions or full function/class blocks.\\n\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON ONLY with format: {'scores': {chunk_id: score, ...}}\\n\"\n            \"Scores must be integers 0 to 100.\"\n        )\n        \n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            # Use regex to find JSON if model adds fluff\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match: return docs[:self.rerank_top_n]\n            \n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)), \n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))), \n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # 2. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 3. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 4. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n=======\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": "{\"raw_scores\": [0.5, 0.4541666666666667, 0.5, 0.5892857142857143, 0.6050000000000001, 0.68, 0.5875, 0.6630952380952382, 0.525, 0.42500000000000004], \"error_details\": [{\"sample\": 1, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 1)\", \"bad_code\": \"def run(new_query, ...):\\n    # 1. Append user query to session history\\n    session_service.append_event(session, Event(author='user', content=new_query))\\n\\n    # 2. Start agent event generation\\n    agent_event_generator = agent_to_run.run_async(context)\\n\\n    async for event in agent_event_generator:\\n        # 3. Process event and commit changes\\n        session_service.append_event(session, event)\\n\\n        # 4. Yield event upstream (e.g., to UI)\\n        yield event\\n        # 5. Agent resumes after yield, loop continues\"}, {\"sample\": 3, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 1)\", \"bad_code\": \"> def get_weather(city: str) -> dict:\\n>     \\\"\\\"\\\"Retrieves weather for a city.\\\"\\\"\\\"\\n>     return {\\\"status\\\": \\\"success\\\", \\\"temperature\\\": \\\"25\\u00b0C\\\"}\\n>\\n\\nfrom google.adk.tools import FunctionTool\\n\\ndef list_user_files_py(tool_context: ToolContext) -> str:\\n    \\\"\\\"\\\"Tool to list available artifacts for the user.\\\"\\\"\\\"\\n    # function implementation here\\n    ...\\n\\nlist_files_tool = FunctionTool(func=list_user_files_py)\\n\\nfrom google.adk.tools import FunctionTool\\n\\ndef get_weather(city: str) -> dict:\\n    \\\"\\\"\\\"Retrieves weather for a city.\\\"\\\"\\\"\\n    # Your logic here, e.g., call a weather API\\n    return {\\\"status\\\": \\\"success\\\", \\\"temperature\\\": \\\"25\\u00b0C\\\"}\\n\\nweather_tool = FunctionTool(func=get_weather)\"}, {\"sample\": 4, \"type\": \"code_error\", \"message\": \"[\\\"Instantiation Error on line 17: name 'MODEL_NAME' is not defined\\\"]\", \"bad_code\": \"# Complete Python example to run a simple LlmAgent that answers questions\\n\\nimport asyncio\\nfrom google.adk.agents import LlmAgent\\nfrom google.adk.runners import Runner\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.genai import types\\nfrom google.genai.types import Content, Part\\n\\n# --- Constants ---\\nAPP_NAME = \\\"simple_qa_app\\\"\\nUSER_ID = \\\"user1234\\\"\\nSESSION_ID = \\\"session_qa_001\\\"\\nMODEL_NAME = \\\"gemini-2.0-flash\\\"\\n\\n# --- Define the LlmAgent ---\\nqa_agent = LlmAgent(\\n    name=\\\"simple_qa_agent\\\",\\n    model=MODEL_NAME,\\n    instruction=(\\n        \\\"You are a helpful assistant. Answer the user's questions clearly and concisely.\\\"\\n    ),\\n    description=\\\"A simple question-answering agent.\\\",\\n)\\n\\n# --- Setup session service and runner ---\\nsession_service = InMemorySessionService()\\n\\nasync def main():\\n    # Create a session for the user\\n    await session_service.create_session(\\n        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\\n    )\\n\\n    # Create a runner to run the agent\\n    runner = Runner(agent=qa_agent, app_name=APP_NAME, session_service=session_service)\\n\\n    # Prepare user query as Content\\n    user_query = \\\"What is the capital of France?\\\"\\n    content = Content(role=\\\"user\\\", parts=[Part(text=user_query)])\\n\\n    print(f\\\"\\\\n--- Sending query to agent: {user_query} ---\\\")\\n\\n    # Run the agent asynchronously and print the final response\\n    final_response = \\\"No response received.\\\"\\n    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\\n        if event.is_final_response() and event.content and event.content.parts:\\n            final_response = event.content.parts[0].text.strip()\\n            print(f\\\"\\\\n\\ud83d\\udfe2 Agent final response:\\\\n{final_response}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    asyncio.run(main())\"}, {\"sample\": 5, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 1)\", \"bad_code\": \"> async for event in self.some_sub_agent.run_async(ctx):\\n>     # Optionally inspect or log the event\\n>     yield event # Pass the event up\\n>\\n\\n> # Read data set by a previous agent\\n> previous_result = ctx.session.state.get(\\\"some_key\\\")\\n> \\n> # Make a decision based on state\\n> if previous_result == \\\"some_value\\\":\\n>     # ... call a specific sub-agent ...\\n> else:\\n>     # ... call another sub-agent ...\\n> \\n> # Store a result for a later step (often done via a sub-agent's output_key)\\n> # ctx.session.state[\\\"my_custom_result\\\"] = \\\"calculated_value\\\"\\n>\\n\\nfrom typing import AsyncGenerator\\nfrom google.adk.agents import BaseAgent, LlmAgent\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom google.adk.events import Event\\n\\nclass RoutingAgent(BaseAgent):\\n    # Define sub-agents as instance attributes\\n    agent_a: LlmAgent\\n    agent_b: LlmAgent\\n\\n    model_config = {\\\"arbitrary_types_allowed\\\": True}\\n\\n    def __init__(self, name: str, agent_a: LlmAgent, agent_b: LlmAgent):\\n        sub_agents_list = [agent_a, agent_b]\\n        super().__init__(name=name, agent_a=agent_a, agent_b=agent_b, sub_agents=sub_agents_list)\\n\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        # Read routing key from session state\\n        route_key = ctx.session.state.get(\\\"route_key\\\")\\n\\n        if route_key == \\\"use_agent_a\\\":\\n            # Route to agent_a\\n            async for event in self.agent_a.run_async(ctx):\\n                yield event\\n        elif route_key == \\\"use_agent_b\\\":\\n            # Route to agent_b\\n            async for event in self.agent_b.run_async(ctx):\\n                yield event\\n        else:\\n            # Default or fallback behavior\\n            # For example, yield an event indicating no route matched\\n            from google.adk.events import Event\\n            yield Event(\\n                author=\\\"RoutingAgent\\\",\\n                content=None,\\n                metadata={\\\"message\\\": \\\"No valid route_key found in session state.\\\"}\\n            )\"}, {\"sample\": 6, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 1)\", \"bad_code\": \"> from google.adk.tools import ToolContext, FunctionTool\\n> \\n> def update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n>     \\\"\\\"\\\"Updates a user-specific preference.\\\"\\\"\\\"\\n>     user_prefs_key = \\\"user:preferences\\\"\\n>     # Get current preferences or initialize if none exist\\n>     preferences = tool_context.state.get(user_prefs_key, {})\\n>     preferences[preference] = value\\n>     # Write the updated dictionary back to the state\\n>     tool_context.state[user_prefs_key] = preferences\\n>     print(f\\\"Tool: Updated user preference '{preference}' to '{value}'\\\")\\n>     return {\\\"status\\\": \\\"success\\\", \\\"updated_preference\\\": preference}\\n> \\n> pref_tool = FunctionTool(func=update_user_preference)\\n>\\n\\nfrom google.adk.tools import ToolContext, FunctionTool\\n\\ndef update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n    user_prefs_key = \\\"user:preferences\\\"\\n    preferences = tool_context.state.get(user_prefs_key, {})\\n    preferences[preference] = value\\n    tool_context.state[user_prefs_key] = preferences\\n    print(f\\\"Tool: Updated user preference '{preference}' to '{value}'\\\")\\n    return {\\\"status\\\": \\\"success\\\", \\\"updated_preference\\\": preference}\\n\\npref_tool = FunctionTool(func=update_user_preference)\"}, {\"sample\": 7, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 58)\", \"bad_code\": \"# Python\\n\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\nfrom google.adk.runner import InMemoryRunner\\nfrom google.adk.sessions import Session\\nfrom google.genai.types import Content, Part\\nfrom google.adk.events import Event\\n\\n# --- 1. Define the three LlmAgents with output_key to pass data via shared state ---\\n\\n# First agent: validates input and outputs 'validation_status'\\nvalidator = LlmAgent(\\n    name=\\\"ValidateInput\\\",\\n    instruction=\\\"Validate the input.\\\",\\n    output_key=\\\"validation_status\\\"\\n)\\n\\n# Second agent: processes data if validation_status is 'valid', outputs 'result'\\nprocessor = LlmAgent(\\n    name=\\\"ProcessData\\\",\\n    instruction=\\\"Process data if {validation_status} is 'valid'.\\\",\\n    output_key=\\\"result\\\"\\n)\\n\\n# Third agent: reports the result, reads 'result' from state\\nreporter = LlmAgent(\\n    name=\\\"ReportResult\\\",\\n    instruction=\\\"Report the result from {result}.\\\"\\n)\\n\\n# --- 2. Create the SequentialAgent pipeline with the three sub_agents ---\\ndata_pipeline = SequentialAgent(\\n    name=\\\"DataPipeline\\\",\\n    sub_agents=[validator, processor, reporter]\\n)\\n\\n# --- 3. Create an InMemoryRunner to run the pipeline ---\\nrunner = InMemoryRunner(data_pipeline, \\\"DataPipelineApp\\\")\\n\\n# --- 4. Create a session for a user ---\\nuser_id = \\\"user_123\\\"\\nsession = runner.session_service().create_session(\\\"DataPipelineApp\\\", user_id).blocking_get()\\n\\n# --- 5. Prepare user input as Content ---\\nuser_input_text = \\\"Please validate and process this input data.\\\"\\nuser_message = Content.from_parts(Part.from_text(user_input_text))\\n\\n# --- 6. Run the pipeline asynchronously and print final response ---\\nevent_stream = runner.run_async(user_id, session.id(), user_message)\\n\\ndef print_final_response(event: Event):\\n    if event.final_response():\\n        print(\\\"Final response from pipeline:\\\")\\n        print(event.stringify_content())\\n\\nevent_stream.blocking_for_each(print_final_response)\\n\\n> validator = LlmAgent(name=\\\"ValidateInput\\\", instruction=\\\"Validate the input.\\\", output_key=\\\"validation_status\\\")\\n> processor = LlmAgent(name=\\\"ProcessData\\\", instruction=\\\"Process data if {validation_status} is 'valid'.\\\", output_key=\\\"result\\\")\\n> reporter = LlmAgent(name=\\\"ReportResult\\\", instruction=\\\"Report the result from {result}.\\\")\\n> data_pipeline = SequentialAgent(name=\\\"DataPipeline\\\", sub_agents=[validator, processor, reporter])\\n>\"}, {\"sample\": 8, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 1)\", \"bad_code\": \"> # Conceptual Example: Loop with Condition\\n> from google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\n> from google.adk.events import Event, EventActions\\n> from google.adk.agents.invocation_context import InvocationContext\\n> from typing import AsyncGenerator\\n> \\n> class CheckCondition(BaseAgent): # Custom agent to check state\\n>     async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n>         status = ctx.session.state.get(\\\"status\\\", \\\"pending\\\")\\n>         is_done = (status == \\\"completed\\\")\\n>         yield Event(author=self.name, actions=EventActions(escalate=is_done)) # Escalate if done\\n> \\n> process_step = LlmAgent(name=\\\"ProcessingStep\\\") # Agent that might update state['status']\\n> \\n> poller = LoopAgent(\\n>     name=\\\"StatusPoller\\\",\\n>     max_iterations=10,\\n>     sub_agents=[process_step, CheckCondition(name=\\\"Checker\\\")]\\n> )\\n> # When poller runs, it executes process_step then Checker repeatedly\\n> # until Checker escalates (state['status'] == 'completed') or 10 iterations pass.\\n>\\n\\nfrom google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\nclass CheckCondition(BaseAgent):\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        status = ctx.session.state.get(\\\"status\\\", \\\"pending\\\")\\n        should_exit = (status == \\\"completed\\\")\\n        yield Event(author=self.name, actions=EventActions(escalate=should_exit))\\n\\n# Agent that performs some processing and updates state['status']\\nprocess_agent = LlmAgent(name=\\\"ProcessAgent\\\", instruction=\\\"Do some work and update state['status'].\\\")\\n\\nretry_loop = LoopAgent(\\n    name=\\\"RetryLoop\\\",\\n    max_iterations=5,\\n    sub_agents=[process_agent, CheckCondition(name=\\\"ExitChecker\\\")]\\n)\"}, {\"sample\": 9, \"type\": \"code_error\", \"message\": \"[\\\"Instantiation Error on line 92: name 'GEMINI_2_FLASH' is not defined\\\", \\\"Instantiation Error on line 100: name 'GEMINI_2_FLASH' is not defined\\\", \\\"Instantiation Error on line 113: name 'StoryToneAgent' is not defined\\\"]\", \"bad_code\": \"import logging\\nfrom typing import AsyncGenerator\\nfrom typing_extensions import override\\n\\nfrom google.adk.agents import LlmAgent, BaseAgent\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom google.adk.events import Event\\n\\nlogger = logging.getLogger(__name__)\\nlogging.basicConfig(level=logging.INFO)\\n\\nclass StoryToneAgent(BaseAgent):\\n    \\\"\\\"\\\"\\n    Custom agent for story generation with tone checking and conditional regeneration.\\n    \\\"\\\"\\\"\\n\\n    # Declare the agents as class attributes with type hints\\n    story_generator: LlmAgent\\n    tone_check: LlmAgent\\n\\n    model_config = {\\\"arbitrary_types_allowed\\\": True}\\n\\n    def __init__(\\n        self,\\n        name: str,\\n        story_generator: LlmAgent,\\n        tone_check: LlmAgent,\\n    ):\\n        \\\"\\\"\\\"\\n        Initializes the StoryToneAgent.\\n\\n        Args:\\n            name: The name of the agent.\\n            story_generator: An LlmAgent to generate the initial story.\\n            tone_check: An LlmAgent to analyze the tone.\\n        \\\"\\\"\\\"\\n        sub_agents_list = [story_generator, tone_check]\\n\\n        super().__init__(\\n            name=name,\\n            story_generator=story_generator,\\n            tone_check=tone_check,\\n            sub_agents=sub_agents_list,\\n        )\\n\\n    @override\\n    async def _run_async_impl(\\n        self, ctx: InvocationContext\\n    ) -> AsyncGenerator[Event, None]:\\n        \\\"\\\"\\\"\\n        Orchestrates the story generation, tone checking, and conditional regeneration.\\n        \\\"\\\"\\\"\\n        logger.info(f\\\"[{self.name}] Starting story generation workflow.\\\")\\n\\n        # 1. Generate the initial story\\n        logger.info(f\\\"[{self.name}] Running StoryGenerator...\\\")\\n        async for event in self.story_generator.run_async(ctx):\\n            logger.info(f\\\"[{self.name}] Event from StoryGenerator: {event.model_dump_json(indent=2, exclude_none=True)}\\\")\\n            yield event\\n\\n        # Check if story was generated\\n        if \\\"current_story\\\" not in ctx.session.state or not ctx.session.state[\\\"current_story\\\"]:\\n            logger.error(f\\\"[{self.name}] Failed to generate initial story. Aborting workflow.\\\")\\n            return  # Stop processing if no story\\n\\n        logger.info(f\\\"[{self.name}] Story state after generation: {ctx.session.state.get('current_story')}\\\")\\n\\n        # 2. Run tone check\\n        logger.info(f\\\"[{self.name}] Running ToneCheck...\\\")\\n        async for event in self.tone_check.run_async(ctx):\\n            logger.info(f\\\"[{self.name}] Event from ToneCheck: {event.model_dump_json(indent=2, exclude_none=True)}\\\")\\n            yield event\\n\\n        tone_check_result = ctx.session.state.get(\\\"tone_check_result\\\")\\n        logger.info(f\\\"[{self.name}] Tone check result: {tone_check_result}\\\")\\n\\n        # 3. Conditional regeneration if tone is negative\\n        if tone_check_result == \\\"negative\\\":\\n            logger.info(f\\\"[{self.name}] Tone is negative. Regenerating story...\\\")\\n            async for event in self.story_generator.run_async(ctx):\\n                logger.info(f\\\"[{self.name}] Event from StoryGenerator (Regen): {event.model_dump_json(indent=2, exclude_none=True)}\\\")\\n                yield event\\n        else:\\n            logger.info(f\\\"[{self.name}] Tone is not negative. Keeping current story.\\\")\\n\\n        logger.info(f\\\"[{self.name}] Workflow finished.\\\")\\n\\n\\n# Example instantiation of the LlmAgents (replace model and instructions as needed)\\nGEMINI_2_FLASH = \\\"gemini-2.0-flash\\\"\\n\\nstory_generator = LlmAgent(\\n    name=\\\"StoryGenerator\\\",\\n    model=GEMINI_2_FLASH,\\n    instruction=\\\"You are a story writer. Write a short story (around 100 words) on the topic: {topic}\\\",\\n    input_schema=None,\\n    output_key=\\\"current_story\\\",\\n)\\n\\ntone_check = LlmAgent(\\n    name=\\\"ToneCheck\\\",\\n    model=GEMINI_2_FLASH,\\n    instruction=(\\n        \\\"You are a tone analyzer. Analyze the tone of the story provided: {current_story}. \\\"\\n        \\\"Output only one word: 'positive' if the tone is generally positive, \\\"\\n        \\\"'negative' if the tone is generally negative, or 'neutral' otherwise.\\\"\\n    ),\\n    input_schema=None,\\n    output_key=\\\"tone_check_result\\\",\\n)\\n\\n# Create the custom agent instance\\nstory_tone_agent = StoryToneAgent(\\n    name=\\\"StoryToneAgent\\\",\\n    story_generator=story_generator,\\n    tone_check=tone_check,\\n)\"}]}", "artifact_dir": null, "embedding": null}