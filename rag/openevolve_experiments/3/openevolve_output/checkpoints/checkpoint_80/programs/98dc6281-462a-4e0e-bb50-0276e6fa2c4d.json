{"id": "98dc6281-462a-4e0e-bb50-0276e6fa2c4d", "code": "import os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\", \"js\": \"JavaScript\", \"javascript\": \"JavaScript\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Append a standardized language tag for chunk tagging synergy\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"[GENERAL]\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    lang_map = {\n                        \"py\": \"Python\",\n                        \"python\": \"Python\",\n                        \"go\": \"Go\",\n                        \"golang\": \"Go\",\n                        \"java\": \"Java\",\n                        \"js\": \"JavaScript\",\n                        \"javascript\": \"JavaScript\"\n                    }\n                    lang_tag = f\"[LANG={lang_map.get(lang, 'Unknown')}]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                c.metadata[\"language\"] = lang_tag[6:-1] if lang_tag.startswith(\"[LANG=\") else \"Unknown\"\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java|javascript)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(\n                r'\\b(class|func|def|interface|type|package|func\\s+main|enum|struct|const|var|method|import|export)\\s+\\w*',\n                snippet, re.I) else \"[PARTIAL]\"\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            \"- Prefer chunks tagged with [A2A] or [WORKFLOW] if query context suggests\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (improved heuristic + synonyms)\n        requested_lang = None\n        lang_candidates = {\n            \"python\": \"Python\",\n            \"py\": \"Python\",\n            \"go\": \"Go\",\n            \"golang\": \"Go\",\n            \"java\": \"Java\",\n            \"javascript\": \"JavaScript\",\n            \"js\": \"JavaScript\"\n        }\n        lower_query = query_str.lower()\n        for key, val in lang_candidates.items():\n            if re.search(r'\\b' + re.escape(key) + r'\\b', lower_query):\n                requested_lang = val\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: prioritize chunks in requested language and [GENERAL] tags\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            project_area_search = re.search(r'\\[(A2A|WORKFLOW|GENERAL)\\]', c.page_content)\n            project_area_tag = project_area_search.group(0) if project_area_search else \"[GENERAL]\"\n            # Prioritize chunks matching requested language or general docs\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n            elif project_area_tag == \"[GENERAL]\":\n                # Include general docs to boost recall\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(4, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        # Defensive fallback if no contexts found in answer, add explicit note\n        if not final_contexts and \"I don't know\" not in res.content:\n            fallback_note = \"\\n\\nNote: No relevant context was found in the documentation.\"\n            return {\"answer\": res.content + fallback_note, \"contexts\": final_contexts}\n        \n        # Extra cleanup to reduce memory footprint after query\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "5573291a-ca4e-45ec-961c-b08b31c64bba", "generation": 6, "timestamp": 1769738632.1792002, "iteration_found": 80, "metrics": {"combined_score": 0.5434970238095239, "num_samples": 10}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 31 lines with 44 lines\nChange 2: Replace 48 lines with 51 lines\nChange 3: Replace 63 lines with 69 lines\nChange 4: Replace 26 lines with 29 lines", "parent_metrics": {"combined_score": 0.5891845238095239, "num_samples": 10}, "island": 4}, "prompts": {"diff_user": {"system": "You are an expert RAG system optimizer for Google ADK documentation. Improve the code to maximize RAG-pipeline performance.\n\nThe current architecture uses a **Parent-Child Retrieval** system with **Hybrid Search** and **LLM Reranking**.\n\n### High-Leverage Evolution Points:\n\n1. **Hyperparameter Synergy**: Optimize the balance between `chunk_size` (for retrieval precision), `top_k_vector/bm25` (for recall), and `weight_vector/bm25`. The documentation has many exact API names which may favor BM25, while conceptual \"how-to\" queries favor Vector.\n2. **Reranker Prompt Engineering**: The `_llm_rerank` prompt is critical. Evolve it to better identify relevance, handle multi-language snippets, or detect if a chunk contains a complete API definition vs. a partial snippet.\n3. **Metadata-Rich Ingestion**: Enhance `_chunk_document` to include more context in `breadcrumb` and `tagging`. Consider adding language-specific tags or project-area identifiers (e.g. `[A2A]`, `[WORKFLOW]`) to help the reranker and generator.\n4. **Context Assembly & Prompting**: Optimize the `context_block` formatting in `query`. Experiment with summarizing parent context or using XML-style tags to separate sources. Refine the final generation prompt for better \"Quote then Explain\" adherence.\n5. **Cross-Language Filtering**: Implement logic to detect the user's requested language and prioritize or filter retrieved chunks to reduce cross-language noise.\n\n**STRICT RULES**:\n- **Fork-Safety**: Avoid any top-level imports of `langchain`, `agno`, or `openai`. Use lazy imports inside methods.\n- **Compatibility**: Ensure `evaluate_rag(docs_path, query)` maintains its signature.\n- **Cleanup**: Always call `gc.collect()` after LLM calls or retrieval to maintain memory stability.\n\n**Example Improvement (Reranker Logic)**:\n```python\n# You can evolve the reranker prompt to be more specific:\nprompt = (\n    \"Role: ADK Expert Reranker. \"\n    \"Goal: Select chunks that provide EXECUTABLE code for the query. \"\n    f\"Query: {query}\\n\"\n    \"Chunks: ... \"\n)\n```\n- Maintain `evaluate_rag(docs_path, query)` signature and return keys: `answer`, `contexts`.\n- Keep `RAGSystem` class structure and lazy-loading in `__init__`.\n", "user": "# Current Program Information\n- Fitness: 0.5892\n- Feature coordinates: combined_score=0.59\n- Focus areas: - Fitness unchanged at 0.5892\n- Exploring combined_score=0.59 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n## Last Execution Output\n\n### raw_scores\n```\n[0.5, 0.515, 0.5, 0.65, 0.5125000000000001, 0.76, 0.685, 0.4130952380952381, 0.53125, 0.825]\n```\n\n### error_details\n```\n[{'sample': 3, 'type': 'code_error', 'message': 'Syntax Error: invalid syntax (<unknown>, line 12)', 'bad_code': '# Core abstractions: wrap a function as a tool.\\nfrom google.adk.tools import FunctionTool, ToolContext\\n\\ndef get_time(tool_context: ToolContext) -> dict:\\n    return {\"time\": \"12:00\"}\\n\\ntime_tool = FunctionTool(func=get_time, name=\"get_time\")\\n\\nfrom google.adk.tools import FunctionTool, ToolContext\\nfrom typing import Dict\\n\\ndef my_authenticated_tool_function(param1: str, ..., tool_context: ToolContext) -> dict:\\n    # ... your logic ...\\n    pass\\n\\nmy_tool = FunctionTool(func=my_authenticated_tool_function)\\n\\ndef get_weather(city: str) -> dict:\\n    \"\"\"Retrieves weather for a city.\"\"\"\\n    return {\"status\": \"success\", \"temperature\": \"25\u00b0C\"}\\n\\nfrom google.adk.tools import FunctionTool, ToolContext\\n\\ndef get_time(tool_context: ToolContext) -> dict:\\n    \"\"\"Returns the current time.\"\"\"\\n    return {\"status\": \"success\", \"time\": \"12:00\"}\\n\\ntime_tool = FunctionTool(func=get_time, name=\"get_time\")'}, {'sample': 4, 'type': 'code_error', 'message': '[\"Instantiation Error on line 31: name \\'MODEL_NAME\\' is not defined\", \"Instantiation Error on line 46: name \\'agent\\' is not defined\"]', 'bad_code': 'import asyncio\\nimport json\\n\\nfrom google.adk.agents import LlmAgent\\nfrom google.adk.runners import Runner\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.genai import types\\nfrom pydantic import BaseModel, Field\\n\\n# --- 1. Define Constants ---\\nAPP_NAME = \"agent_comparison_app\"\\nUSER_ID = \"test_user_456\"\\nSESSION_ID = \"session_tool_agent_xyz\"\\nMODEL_NAME = \"gemini-2.0-flash\"\\n\\n# --- 2. Define a simple tool function ---\\ndef get_capital_city(country: str) -> str:\\n    \"\"\"Retrieves the capital city of a given country.\"\"\"\\n    print(f\"\\\\n-- Tool Call: get_capital_city(country=\\'{country}\\') --\")\\n    country_capitals = {\\n        \"united states\": \"Washington, D.C.\",\\n        \"canada\": \"Ottawa\",\\n        \"france\": \"Paris\",\\n        \"japan\": \"Tokyo\",\\n    }\\n    result = country_capitals.get(country.lower(), f\"Sorry, I couldn\\'t find the capital for {country}.\")\\n    print(f\"-- Tool Result: \\'{result}\\' --\")\\n    return result\\n\\n# --- 3. Create the LlmAgent with the tool ---\\nagent = LlmAgent(\\n    model=MODEL_NAME,\\n    name=\"capital_agent_tool\",\\n    instruction=\"\"\"You are a helpful agent that provides the capital city of a country using a tool.\\nThe user will provide the country name in a JSON format like {\"country\": \"country_name\"}.\\n1. Extract the country name.\\n2. Use the `get_capital_city` tool to find the capital.\\n3. Respond clearly to the user, stating the capital city found by the tool.\\n\"\"\",\\n    tools=[get_capital_city]\\n)\\n\\n# --- 4. Setup session service and runner ---\\nsession_service = InMemorySessionService()\\nsession = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\\nrunner = Runner(agent=agent, app_name=APP_NAME, session_service=session_service)\\n\\n# --- 5. Define a function to call the agent ---\\ndef call_agent(query: str):\\n    content = types.Content(role=\\'user\\', parts=[types.Part(text=query)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        print(f\"\\\\nDEBUG EVENT: {event}\\\\n\")\\n        if event.is_final_response() and event.content:\\n            final_answer = event.content.parts[0].text.strip()\\n            print(\"\\\\n\ud83d\udfe2 FINAL ANSWER\\\\n\", final_answer, \"\\\\n\")\\n\\n# --- 6. Run the agent with a sample query ---\\nif __name__ == \"__main__\":\\n    call_agent(\\'{\"country\": \"France\"}\\')'}, {'sample': 6, 'type': 'code_error', 'message': '[\"Instantiation Error on line 14: name \\'update_user_preference\\' is not defined\", \"Instantiation Error on line 57: name \\'update_user_preference\\' is not defined\"]', 'bad_code': 'from google.adk.tools import ToolContext, FunctionTool\\n\\ndef update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n    \"\"\"Updates a user-specific preference.\"\"\"\\n    user_prefs_key = \"user:preferences\"\\n    # Get current preferences or initialize if none exist\\n    preferences = tool_context.state.get(user_prefs_key, {})\\n    preferences[preference] = value\\n    # Write the updated dictionary back to the state\\n    tool_context.state[user_prefs_key] = preferences\\n    print(f\"Tool: Updated user preference \\'{preference}\\' to \\'{value}\\'\")\\n    return {\"status\": \"success\", \"updated_preference\": preference}\\n\\npref_tool = FunctionTool(func=update_user_preference)\\n\\n# In an Agent:\\n# my_agent = Agent(..., tools=[pref_tool])\\n\\n# When the LLM calls update_user_preference(preference=\\'theme\\', value=\\'dark\\', ...):\\n# The tool_context.state will be updated, and the change will be part of the\\n# resulting tool response event\\'s actions.state_delta.\\n\\n# Pseudocode: Tool 1 - Fetches user ID\\nfrom google.adk.tools import ToolContext\\nimport uuid\\n\\ndef get_user_profile(tool_context: ToolContext) -> dict:\\n    user_id = str(uuid.uuid4())  # Simulate fetching ID\\n    # Save the ID to state for the next tool\\n    tool_context.state[\"temp:current_user_id\"] = user_id\\n    return {\"profile_status\": \"ID generated\"}\\n\\n# Pseudocode: Tool 2 - Uses user ID from state\\ndef get_user_orders(tool_context: ToolContext) -> dict:\\n    user_id = tool_context.state.get(\"temp:current_user_id\")\\n    if not user_id:\\n        return {\"error\": \"User ID not found in state\"}\\n\\n    print(f\"Fetching orders for user ID: {user_id}\")\\n    # ... logic to fetch orders using user_id ...\\n    return {\"orders\": [\"order123\", \"order456\"]}\\n\\nfrom google.adk.tools import ToolContext, FunctionTool\\n\\ndef update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n    \"\"\"Updates a user-specific preference in session state.\"\"\"\\n    user_prefs_key = \"user:preferences\"\\n    # Read existing preferences or initialize empty dict\\n    preferences = tool_context.state.get(user_prefs_key, {})\\n    preferences[preference] = value\\n    # Write updated preferences back to state\\n    tool_context.state[user_prefs_key] = preferences\\n    print(f\"Tool: Updated user preference \\'{preference}\\' to \\'{value}\\'\")\\n    return {\"status\": \"success\", \"updated_preference\": preference}\\n\\n# Wrap the function as a FunctionTool to use in an agent\\npref_tool = FunctionTool(func=update_user_preference)\\n\\ndef get_user_profile(tool_context: ToolContext) -> dict:\\n    user_id = \"some_generated_id\"\\n    tool_context.state[\"temp:current_user_id\"] = user_id\\n    return {\"profile_status\": \"ID generated\"}\\n\\ndef get_user_orders(tool_context: ToolContext) -> dict:\\n    user_id = tool_context.state.get(\"temp:current_user_id\")\\n    if not user_id:\\n        return {\"error\": \"User ID not found in state\"}\\n    # Fetch orders using user_id...\\n    return {\"orders\": [\"order123\", \"order456\"]}'}, {'sample': 7, 'type': 'code_error', 'message': '[\"Import Error on line 3: No module named \\'google.adk.runner\\'\", \"Import Error on line 7: No module named \\'rx\\'\", \"Instantiation Error on line 35: name \\'InMemoryRunner\\' is not defined\"]', 'bad_code': '# Conceptual Code: Sequential Data Pipeline\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\nfrom google.adk.runner import InMemoryRunner\\nfrom google.adk.sessions import Session\\nfrom google.adk.events import Event\\nfrom google.genai.types import Content, Part\\nfrom rx import operators as ops\\n\\n# Define three LlmAgents with output_key to pass data via shared state\\n\\nvalidator = LlmAgent(\\n    name=\"ValidateInput\",\\n    instruction=\"Validate the input.\",\\n    output_key=\"validation_status\"\\n)\\n\\nprocessor = LlmAgent(\\n    name=\"ProcessData\",\\n    instruction=\"Process data if {validation_status} is \\'valid\\'.\",\\n    output_key=\"result\"\\n)\\n\\nreporter = LlmAgent(\\n    name=\"ReportResult\",\\n    instruction=\"Report the result from {result}.\"\\n)\\n\\n# Create a SequentialAgent pipeline with the three sub_agents\\ndata_pipeline = SequentialAgent(\\n    name=\"DataPipeline\",\\n    sub_agents=[validator, processor, reporter]\\n)\\n\\n# Create an InMemoryRunner to run the pipeline\\nrunner = InMemoryRunner(data_pipeline, app_name=\"DataPipelineApp\")\\n\\n# Create a session for a user\\nuser_id = \"user_123\"\\nsession = runner.session_service().create_session(\"DataPipelineApp\", user_id).blocking_first()\\n\\n# Prepare user input as Content\\nuser_input = Content.from_parts(Part.from_text(\"Sample input data to validate and process.\"))\\n\\n# Run the pipeline asynchronously and print final responses\\nevent_stream = runner.run_async(user_id, session.id, user_input)\\n\\n# Subscribe to events and print final response content\\nevent_stream.pipe(\\n    ops.filter(lambda e: e.final_response())\\n).subscribe(lambda e: print(e.stringify_content()))'}, {'sample': 8, 'type': 'code_error', 'message': '[\"Instantiation Error on line 15: name \\'CheckCondition\\' is not defined\", \"Instantiation Error on line 50: name \\'CheckStatusAndEscalate\\' is not defined\", \"Instantiation Error on line 78: name \\'CheckCondition\\' is not defined\", \"Instantiation Error on line 110: name \\'CheckStatusAndEscalate\\' is not defined\"]', 'bad_code': '# Conceptual Example: Loop with Condition\\nfrom google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\nclass CheckCondition(BaseAgent): # Custom agent to check state\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        status = ctx.session.state.get(\"status\", \"pending\")\\n        is_done = (status == \"completed\")\\n        yield Event(author=self.name, actions=EventActions(escalate=is_done)) # Escalate if done\\n\\nprocess_step = LlmAgent(name=\"ProcessingStep\") # Agent that might update state[\\'status\\']\\n\\npoller = LoopAgent(\\n    name=\"StatusPoller\",\\n    max_iterations=10,\\n    sub_agents=[process_step, CheckCondition(name=\"Checker\")]\\n)\\n# When poller runs, it executes process_step then Checker repeatedly\\n# until Checker escalates (state[\\'status\\'] == \\'completed\\') or 10 iterations pass.\\n\\n# Conceptual Code: Iterative Code Refinement\\nfrom google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\n# Agent to generate/refine code based on state[\\'current_code\\'] and state[\\'requirements\\']\\ncode_refiner = LlmAgent(\\n    name=\"CodeRefiner\",\\n    instruction=\"Read state[\\'current_code\\'] (if exists) and state[\\'requirements\\']. Generate/refine Python code to meet requirements. Save to state[\\'current_code\\'].\",\\n    output_key=\"current_code\" # Overwrites previous code in state\\n)\\n\\n# Agent to check if the code meets quality standards\\nquality_checker = LlmAgent(\\n    name=\"QualityChecker\",\\n    instruction=\"Evaluate the code in state[\\'current_code\\'] against state[\\'requirements\\']. Output \\'pass\\' or \\'fail\\'.\",\\n    output_key=\"quality_status\"\\n)\\n\\n# Custom agent to check the status and escalate if \\'pass\\'\\nclass CheckStatusAndEscalate(BaseAgent):\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        status = ctx.session.state.get(\"quality_status\", \"fail\")\\n        should_stop = (status == \"pass\")\\n        yield Event(author=self.name, actions=EventActions(escalate=should_stop))\\n\\nrefinement_loop = LoopAgent(\\n    name=\"CodeRefinementLoop\",\\n    max_iterations=5,\\n    sub_agents=[code_refiner, quality_checker, CheckStatusAndEscalate(name=\"StopChecker\")]\\n)\\n# Loop runs: Refiner -> Checker -> StopChecker\\n# State[\\'current_code\\'] is updated each iteration.\\n# Loop stops if QualityChecker outputs \\'pass\\' (leading to StopChecker escalating) or after 5 iterations.\\n\\nfrom google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\n# Custom agent that checks a condition in the shared state and escalates to exit the loop\\nclass CheckCondition(BaseAgent):\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        # Read a status flag from the session state\\n        status = ctx.session.state.get(\"status\", \"pending\")\\n        # Determine if the condition to exit the loop is met\\n        is_done = (status == \"completed\")\\n        # Emit an event with escalate=True to signal the LoopAgent to stop\\n        yield Event(author=self.name, actions=EventActions(escalate=is_done))\\n\\n# An example agent that performs some processing and may update the state\\nprocess_step = LlmAgent(name=\"ProcessingStep\")\\n\\n# Create the LoopAgent with a max iteration limit and the sub-agents\\npoller = LoopAgent(\\n    name=\"StatusPoller\",\\n    max_iterations=10,\\n    sub_agents=[process_step, CheckCondition(name=\"Checker\")]\\n)\\n\\n# When poller runs, it executes process_step then Checker repeatedly\\n# until Checker escalates (state[\\'status\\'] == \\'completed\\') or 10 iterations pass.\\n\\nfrom google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\ncode_refiner = LlmAgent(\\n    name=\"CodeRefiner\",\\n    instruction=\"Read state[\\'current_code\\'] and state[\\'requirements\\']. Generate/refine code.\",\\n    output_key=\"current_code\"\\n)\\n\\nquality_checker = LlmAgent(\\n    name=\"QualityChecker\",\\n    instruction=\"Evaluate code quality and output \\'pass\\' or \\'fail\\'.\",\\n    output_key=\"quality_status\"\\n)\\n\\nclass CheckStatusAndEscalate(BaseAgent):\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        status = ctx.session.state.get(\"quality_status\", \"fail\")\\n        should_stop = (status == \"pass\")\\n        yield Event(author=self.name, actions=EventActions(escalate=should_stop))\\n\\nrefinement_loop = LoopAgent(\\n    name=\"CodeRefinementLoop\",\\n    max_iterations=5,\\n    sub_agents=[code_refiner, quality_checker, CheckStatusAndEscalate(name=\"StopChecker\")]\\n)'}]\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Change 1: Replace 9 lines with 9 lines\nChange 2: Replace 15 lines with 15 lines\nChange 3: Replace 62 lines with 63 lines\nChange 4: Replace 8 lines with 8 lines\nChange 5: Replace 28 lines with 28 lines\nChange 6: Replace 21 lines with 26 lines\n- Metrics: combined_score: 0.5892, num_samples: 10.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 9 lines with 11 lines\nChange 2: Replace 10 lines with 11 lines\nChange 3: Replace 21 lines with 24 lines\nChange 4: Replace 47 lines with 47 lines\nChange 5: Replace 28 lines with 31 lines\nChange 6: Replace 45 lines with 46 lines\n- Metrics: combined_score: 0.6422, num_samples: 10.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.6422)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Append a standardized language tag for chunk tagging synergy\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6422), Performs well on num_samples (10.0000)\n\n### Program 2 (Score: 0.5892)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\", \"js\": \"JavaScript\", \"javascript\": \"JavaScript\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Append a standardized language tag for chunk tagging synergy\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"[GENERAL]\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\", \"javascript\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                c.metadata[\"language\"] = lang_tag[6:-1] if lang_tag.startswith(\"[LANG=\") else \"Unknown\"\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main|enum|struct|const|var|method)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\", \"javascript\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        # Defensive fallback if no contexts found in answer, add explicit note\n        if not final_contexts and \"I don't know\" not in res.content:\n            fallback_note = \"\\n\\nNote: No relevant context was found in the documentation.\"\n            return {\"answer\": res.content + fallback_note, \"contexts\": final_contexts}\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.5892), Performs well on num_samples (10.0000)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.6422, Type: Migrant)\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Append a standardized language tag for chunk tagging synergy\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], Object-oriented approach, Comprehensive implementation\n\n# Current Program\n```python\nimport os\nimport re\nimport gc\nfrom typing import Dict, Any, List, Optional\nfrom dotenv import load_dotenv\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        # Simple caching to avoid re-ingesting for every query if docs path hasn't changed\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n            \n        return _rag_system_cache.query(query)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\n# Imports removed from top-level to prevent parent process initialization\n# They are now lazy-loaded in RAGSystem.__init__ to ensure fork-safety\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.retriever = None\n        self.llm = None\n        self.rerank_llm = None\n        self.parent_docs = {} # storage for parent nodes\n        \n        # Hyperparameters for evolution\n        self.chunk_size = 500 # Smaller chunks for better retrieval precision\n        self.chunk_overlap = 100\n        self.top_k_vector = 20 \n        self.top_k_bm25 = 20\n        self.weight_vector = 0.5\n        self.weight_bm25 = 0.5\n        self.rerank_top_n = 8 \n        self.temperature = 0.0\n        self.expand_query = False \n        \n        # Load env from multiple possible locations\n        env_paths = [\n            os.path.join(os.path.dirname(__file__), '.env'),\n            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')\n        ]\n        for p in env_paths:\n            if os.path.exists(p):\n                load_dotenv(p)\n                \n        self._initialize_system()\n\n    def _initialize_system(self):\n        from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n        from langchain_core.documents import Document\n        from langchain_community.vectorstores import FAISS\n        from langchain_community.retrievers import BM25Retriever\n        from langchain_classic.retrievers import EnsembleRetriever\n        \n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=self.temperature)\n        self.rerank_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n        \n        # Ingestion\n        all_chunks = []\n        if os.path.exists(self.docs_dir):\n            for root, dirs, files in os.walk(self.docs_dir):\n                dirs[:] = [d for d in dirs if not d.startswith('.')]\n                for file in files:\n                    if not file.endswith('.md') or file.startswith('.'): continue\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text_content = f.read()\n                        \n                        # Apply evolved chunking strategy\n                        chunks = self._chunk_document(text_content, file_path)\n                        all_chunks.extend(chunks)\n                    except Exception as e:\n                        print(f\"Skipping {file_path}: {e}\")\n\n        if not all_chunks:\n            self.retriever = None\n            return\n\n        # Vector Retriever\n        vector_db = FAISS.from_documents(all_chunks, embeddings)\n        vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": self.top_k_vector})\n        \n        # BM25 Retriever\n        bm25_retriever = BM25Retriever.from_documents(all_chunks)\n        bm25_retriever.k = self.top_k_bm25\n        \n        # Hybrid Ensemble (Base retrieval)\n        ensemble_retriever = EnsembleRetriever(\n            retrievers=[vector_retriever, bm25_retriever],\n            weights=[self.weight_vector, self.weight_bm25]\n        )\n        self.retriever = ensemble_retriever\n        \n        gc.collect()\n\n    def _flatten_markdown_tabs(self, text: str) -> str:\n        \"\"\"Unrolls Google-style tab widgets into sequential headers with normalized language tags.\"\"\"\n        # Enhanced regex for {% tab label=\"Python\" %} blocks with language normalization\n        pattern = r'{% tab label=\"(.*?)\" %}(.*?){% endtab %}'\n        def replacement(match):\n            label = match.group(1).strip()\n            content = match.group(2)\n            lang_map = {\"py\": \"Python\", \"python\": \"Python\", \"go\": \"Go\", \"golang\": \"Go\", \"java\": \"Java\", \"js\": \"JavaScript\", \"javascript\": \"JavaScript\"}\n            lang_tag = lang_map.get(label.lower(), label)\n            # Append a standardized language tag for chunk tagging synergy\n            return f\"\\n\\n#### [{lang_tag}] {label} Implementation\\n{content}\\n\"\n        \n        text = re.sub(r'{% tabs %}', '', text)\n        text = re.sub(r'{% endtabs %}', '', text)\n        return re.sub(pattern, replacement, text, flags=re.DOTALL)\n\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"[GENERAL]\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\", \"javascript\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                c.metadata[\"language\"] = lang_tag[6:-1] if lang_tag.startswith(\"[LANG=\") else \"Unknown\"\n                child_chunks.append(c)\n                \n        return child_chunks\n\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main|enum|struct|const|var|method)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        if not self.retriever:\n            return {\"answer\": \"No documents ingested.\", \"contexts\": []}\n\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\", \"javascript\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n        \n        # 3. Reranking\n        top_chunks = self._llm_rerank(query_str, candidates)\n        \n        # 4. Parent Retrieval & Formatting\n        final_contexts = []\n        seen_parents = set()\n        context_block = \"\"\n        \n        for c in top_chunks:\n            p_id = c.metadata.get(\"parent_id\")\n            if p_id in self.parent_docs and p_id not in seen_parents:\n                parent = self.parent_docs[p_id]\n                # Filter parent context by language tag similarly\n                lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', parent.page_content)\n                parent_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n                # Accept unknown language parents for Python queries to preserve recall\n                if requested_lang == \"Python\" and parent_lang not in (\"Python\", \"Unknown\"):\n                    continue\n                elif requested_lang != \"Python\" and parent_lang != requested_lang:\n                    continue\n                \n                seen_parents.add(p_id)\n                final_contexts.append(parent.page_content)\n                src = os.path.basename(parent.metadata.get(\"source\", \"unknown\"))\n                breadcrumb = parent.metadata.get(\"breadcrumb\", \"\")\n                # Wrap each source in XML-style tags for clearer separation\n                context_block += f\"<source file=\\\"{src}\\\" section=\\\"{breadcrumb}\\\">\\n{parent.page_content}\\n</source>\\n\\n\"\n\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        # Defensive fallback if no contexts found in answer, add explicit note\n        if not final_contexts and \"I don't know\" not in res.content:\n            fallback_note = \"\\n\\nNote: No relevant context was found in the documentation.\"\n            return {\"answer\": res.content + fallback_note, \"contexts\": final_contexts}\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, combined_score\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (simple heuristic)\n        requested_lang = None\n        lang_candidates = [\"python\", \"go\", \"java\", \"javascript\"]\n        lower_query = query_str.lower()\n        for lang in lang_candidates:\n            if lang in lower_query:\n                requested_lang = lang.capitalize()\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: filter candidates to requested language if specified\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            # Include chunks with unknown language for Python requests to avoid losing recall\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(3, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n=======\n        # 1. Retrieval (Hybrid)\n        candidates = self.retriever.invoke(query_str)\n        \n        # Detect requested language from query (improved heuristic + synonyms)\n        requested_lang = None\n        lang_candidates = {\n            \"python\": \"Python\",\n            \"py\": \"Python\",\n            \"go\": \"Go\",\n            \"golang\": \"Go\",\n            \"java\": \"Java\",\n            \"javascript\": \"JavaScript\",\n            \"js\": \"JavaScript\"\n        }\n        lower_query = query_str.lower()\n        for key, val in lang_candidates.items():\n            if re.search(r'\\b' + re.escape(key) + r'\\b', lower_query):\n                requested_lang = val\n                break\n        if not requested_lang:\n            requested_lang = \"Python\"  # default\n        \n        # 2. Cross-language filtering: prioritize chunks in requested language and [GENERAL] tags\n        filtered_candidates = []\n        for c in candidates:\n            # Use chunk language tag in content or fallback to unknown\n            lang_tag_search = re.search(r'\\[LANG=(\\w+)\\]', c.page_content)\n            chunk_lang = lang_tag_search.group(1) if lang_tag_search else \"Unknown\"\n            project_area_search = re.search(r'\\[(A2A|WORKFLOW|GENERAL)\\]', c.page_content)\n            project_area_tag = project_area_search.group(0) if project_area_search else \"[GENERAL]\"\n            # Prioritize chunks matching requested language or general docs\n            if requested_lang == \"Python\" and chunk_lang in (\"Python\", \"Unknown\"):\n                filtered_candidates.append(c)\n            elif chunk_lang == requested_lang:\n                filtered_candidates.append(c)\n            elif project_area_tag == \"[GENERAL]\":\n                # Include general docs to boost recall\n                filtered_candidates.append(c)\n        # Use filtered candidates only if enough remain, else fallback to full candidate set\n        if filtered_candidates and len(filtered_candidates) >= max(4, self.rerank_top_n):\n            candidates = filtered_candidates\n        else:\n            # If too few, fallback to original for recall preservation\n            pass\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(r'\\b(class|func|def|interface|type|package|func\\s+main|enum|struct|const|var|method)\\s+\\w*', snippet, re.I) else \"[PARTIAL]\"\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n=======\n    def _llm_rerank(self, query: str, docs: List[Any]) -> List[Any]:\n        \"\"\"Rerank candidates using a small LLM with enhanced prompt for multi-language & completeness detection.\"\"\"\n        if not docs: return []\n        \n        packed = []\n        requested_lang = None\n        # Detect requested language from query for prompt emphasis\n        lang_match = re.search(r'\\b(python|go|java|javascript)\\b', query, re.I)\n        if lang_match:\n            requested_lang = lang_match.group(1).lower()\n\n        for i, d in enumerate(docs):\n            snippet = d.page_content[:1400].replace('\\n', ' ').strip()\n            completeness_tag = \"[COMPLETE_API]\" if re.search(\n                r'\\b(class|func|def|interface|type|package|func\\s+main|enum|struct|const|var|method|import|export)\\s+\\w*',\n                snippet, re.I) else \"[PARTIAL]\"\n            lang = d.metadata.get(\"language\", \"unknown\").lower()\n            requested_lang_tag = \"[REQUESTED_LANG]\" if requested_lang and lang == requested_lang else \"\"\n            tags = d.metadata.get(\"tags\", \"\")\n            breadcrumb = d.metadata.get(\"breadcrumb\", \"\")\n            packed.append(f\"[{i}] {completeness_tag} {requested_lang_tag} Tags:{tags} Breadcrumb:{breadcrumb}\\n{snippet}\")\n\n        prompt = (\n            \"Role: ADK Expert Reranker.\\n\"\n            \"Goal: Score each chunk from 0 to 100 for relevance to the query.\\n\"\n            \"Prioritize chunks with:\\n\"\n            \"- Exact API names matching the query\\n\"\n            \"- Complete API definitions over partial snippets\\n\"\n            \"- Code examples in the user's requested programming language\\n\"\n            \"- Proper handling of multi-language snippets\\n\"\n            \"- Prefer chunks tagged with [A2A] or [WORKFLOW] if query context suggests\\n\"\n            f\"User Query: {query}\\n\\n\"\n            \"Chunks:\\n\" + \"\\n\\n\".join(packed) + \"\\n\\n\"\n            \"Return JSON object with 'scores' mapping chunk IDs to numeric scores.\"\n        )\n\n        try:\n            import json\n            res = self.rerank_llm.invoke(prompt)\n            match = re.search(r'\\{.*\\}', res.content, re.DOTALL)\n            if not match:\n                return docs[:self.rerank_top_n]\n\n            scores = json.loads(match.group(0)).get(\"scores\", {})\n            ranked_indices = sorted(range(len(docs)),\n                                    key=lambda i: float(scores.get(str(i), scores.get(i, 0))),\n                                    reverse=True)\n            return [docs[i] for i in ranked_indices[:self.rerank_top_n]]\n        except Exception as e:\n            print(f\"Reranking failed: {e}\")\n            return docs[:self.rerank_top_n]\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"[GENERAL]\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    if lang in (\"python\", \"go\", \"java\", \"javascript\"):\n                        lang_tag = f\"[LANG={lang.capitalize()}]\"\n                    else:\n                        lang_tag = \"[LANG=Unknown]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                c.metadata[\"language\"] = lang_tag[6:-1] if lang_tag.startswith(\"[LANG=\") else \"Unknown\"\n                child_chunks.append(c)\n                \n        return child_chunks\n=======\n    def _chunk_document(self, text: str, source: str) -> List[Any]:\n        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n        from langchain_core.documents import Document\n        import uuid\n        \n        text = self._flatten_markdown_tabs(text)\n        \n        # 1. Split by headers to create Parent documents\n        header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n            (\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\"), (\"####\", \"h4\")\n        ])\n        parent_docs = header_splitter.split_text(text)\n        \n        child_chunks = []\n        child_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=[\"\\n```\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        for p in parent_docs:\n            parent_id = str(uuid.uuid4())\n            breadcrumb = \" > \".join([p.metadata.get(f\"h{i}\", \"\") for i in range(1, 5)]).strip(\" > \")\n            p.metadata[\"source\"] = source\n            p.metadata[\"breadcrumb\"] = breadcrumb\n            \n            # Add project-area tags if identifiable from breadcrumb (example)\n            project_tags = []\n            if re.search(r'\\[A2A\\]', breadcrumb): project_tags.append(\"[A2A]\")\n            if re.search(r'\\[WORKFLOW\\]', breadcrumb): project_tags.append(\"[WORKFLOW]\")\n            if project_tags:\n                p.metadata[\"tags\"] = \" \".join(project_tags)\n            else:\n                p.metadata[\"tags\"] = \"[GENERAL]\"\n            \n            self.parent_docs[parent_id] = p\n            \n            # 2. Split parents into children\n            for c in child_splitter.split_documents([p]):\n                # Add tagging for better semantic separation\n                prefix = \"[DOC_CODE]\" if \"```\" in c.page_content else \"[DOC_TEXT]\"\n                # Detect language from code fences for tagging\n                lang_tag = \"\"\n                lang_match = re.search(r'```(\\w+)', c.page_content)\n                if lang_match:\n                    lang = lang_match.group(1).lower()\n                    lang_map = {\n                        \"py\": \"Python\",\n                        \"python\": \"Python\",\n                        \"go\": \"Go\",\n                        \"golang\": \"Go\",\n                        \"java\": \"Java\",\n                        \"js\": \"JavaScript\",\n                        \"javascript\": \"JavaScript\"\n                    }\n                    lang_tag = f\"[LANG={lang_map.get(lang, 'Unknown')}]\"\n                else:\n                    lang_tag = \"[LANG=Unknown]\"\n                # Add tags and breadcrumb into chunk content for better reranking and generation\n                # Also add project area tag into chunk content for reranker synergy\n                project_area = p.metadata.get(\"tags\", \"[GENERAL]\")\n                c.page_content = f\"{prefix} {lang_tag} {project_area} {breadcrumb}\\n{c.page_content}\"\n                c.metadata[\"parent_id\"] = parent_id\n                c.metadata[\"source\"] = source\n                c.metadata[\"project_area\"] = project_area\n                c.metadata[\"language\"] = lang_tag[6:-1] if lang_tag.startswith(\"[LANG=\") else \"Unknown\"\n                child_chunks.append(c)\n                \n        return child_chunks\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        # Defensive fallback if no contexts found in answer, add explicit note\n        if not final_contexts and \"I don't know\" not in res.content:\n            fallback_note = \"\\n\\nNote: No relevant context was found in the documentation.\"\n            return {\"answer\": res.content + fallback_note, \"contexts\": final_contexts}\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n=======\n        # 5. Generation (Strict Grounding)\n        prompt = (\n            \"You are a technical expert on Google ADK. Answer the user's question STRICTLY using the provided context. \"\n            \"Follow these rules:\\n\"\n            \"1. If the answer is not in the context, explicitly state 'I don't know from the provided documentation'.\\n\"\n            \"2. When mentioning API classes or methods, use the EXACT names found in the context.\\n\"\n            \"3. Use the 'Quote then Explain' pattern: first quote the relevant doc/code, then provide your explanation.\\n\"\n            f\"4. Only provide code or examples in the requested language: {requested_lang}. \"\n            \"If none requested, default to Python but mention other languages exist.\\n\\n\"\n            \"5. Clearly indicate the programming language of any code snippet you quote.\\n\\n\"\n            \"6. Use XML tags to indicate sources.\\n\"\n            f\"Context:\\n{context_block}\\n\\n\"\n            f\"Question: {query_str}\\n\\n\"\n            \"Answer:\"\n        )\n        \n        # Use caching for llm.invoke to reduce latency on repeated queries (optional)\n        res = self.llm.invoke(prompt)\n        gc.collect()\n        \n        # Defensive fallback if no contexts found in answer, add explicit note\n        if not final_contexts and \"I don't know\" not in res.content:\n            fallback_note = \"\\n\\nNote: No relevant context was found in the documentation.\"\n            return {\"answer\": res.content + fallback_note, \"contexts\": final_contexts}\n        \n        # Extra cleanup to reduce memory footprint after query\n        gc.collect()\n        \n        return {\"answer\": res.content, \"contexts\": final_contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": "{\"raw_scores\": [0.016666666666666666, 0.52, 0.66, 0.6781250000000001, 0.32000000000000006, 0.7750000000000001, 0.6937500000000001, 0.6547619047619048, 0.275, 0.8416666666666668], \"error_details\": [{\"sample\": 3, \"type\": \"code_error\", \"message\": \"Syntax Error: invalid syntax (<unknown>, line 43)\", \"bad_code\": \"# Core abstractions: wrap a function as a tool.\\nfrom google.adk.tools import FunctionTool, ToolContext\\n\\ndef get_time(tool_context: ToolContext) -> dict:\\n    return {\\\"time\\\": \\\"12:00\\\"}\\n\\ntime_tool = FunctionTool(func=get_time, name=\\\"get_time\\\")\\n\\ndef lookup_order_status(order_id: str) -> dict:\\n    \\\"\\\"\\\"Fetches the current status of a customer's order using its ID.\\n\\n    Use this tool ONLY when a user explicitly asks for the status of\\n    a specific order and provides the order ID. Do not use it for\\n    general inquiries.\\n\\n    Args:\\n        order_id: The unique identifier of the order to look up.\\n\\n    Returns:\\n        A dictionary indicating the outcome.\\n        On success, status is 'success' and includes an 'order' dictionary.\\n        On failure, status is 'error' and includes an 'error_message'.\\n        Example success: {'status': 'success', 'order': {'state': 'shipped', 'tracking_number': '1Z9...'}}\\n        Example error: {'status': 'error', 'error_message': 'Order ID not found.'}\\n    \\\"\\\"\\\"\\n    # ... function implementation to fetch status ...\\n    if status_details := fetch_status_from_backend(order_id):\\n        return {\\n            \\\"status\\\": \\\"success\\\",\\n            \\\"order\\\": {\\n                \\\"state\\\": status_details.state,\\n                \\\"tracking_number\\\": status_details.tracking,\\n            },\\n        }\\n    else:\\n        return {\\\"status\\\": \\\"error\\\", \\\"error_message\\\": f\\\"Order ID {order_id} not found.\\\"}\\n\\nmy_tool = FunctionTool(func=lookup_order_status)\\n\\nfrom google.adk.tools import FunctionTool, ToolContext\\nfrom typing import Dict\\n\\ndef my_authenticated_tool_function(param1: str, ..., tool_context: ToolContext) -> dict:\\n    # ... your logic ...\\n    pass\\n\\nmy_tool = FunctionTool(func=my_authenticated_tool_function)\\n\\n> from google.adk.tools import FunctionTool, ToolContext\\n> \\n> def get_time(tool_context: ToolContext) -> dict:\\n>     return {\\\"time\\\": \\\"12:00\\\"}\\n> \\n> time_tool = FunctionTool(func=get_time, name=\\\"get_time\\\")\\n>\\n\\nfrom google.adk.tools import FunctionTool, ToolContext\\n\\ndef lookup_order_status(order_id: str) -> dict:\\n    \\\"\\\"\\\"Fetches the current status of a customer's order using its ID.\\n\\n    Args:\\n        order_id: The unique identifier of the order to look up.\\n\\n    Returns:\\n        A dictionary indicating success or error with details.\\n    \\\"\\\"\\\"\\n    # Your implementation here\\n    if status_details := fetch_status_from_backend(order_id):\\n        return {\\n            \\\"status\\\": \\\"success\\\",\\n            \\\"order\\\": {\\n                \\\"state\\\": status_details.state,\\n                \\\"tracking_number\\\": status_details.tracking,\\n            },\\n        }\\n    else:\\n        return {\\\"status\\\": \\\"error\\\", \\\"error_message\\\": f\\\"Order ID {order_id} not found.\\\"}\\n\\nmy_tool = FunctionTool(func=lookup_order_status)\\n\\nfrom google.adk.tools import FunctionTool, ToolContext\\n\\ndef my_authenticated_tool_function(param1: str, tool_context: ToolContext) -> dict:\\n    # Your authenticated logic here\\n    pass\\n\\nmy_tool = FunctionTool(func=my_authenticated_tool_function)\"}, {\"sample\": 4, \"type\": \"code_error\", \"message\": \"[\\\"Instantiation Error on line 89: name 'get_weather' is not defined\\\", \\\"Instantiation Error on line 100: name 'agent' is not defined\\\"]\", \"bad_code\": \"from dotenv import load_dotenv\\n\\nimport asyncio\\nimport os\\n\\nfrom google.genai import types\\nfrom google.adk.agents.llm_agent import LlmAgent\\nfrom google.adk.runners import Runner\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.adk.artifacts.in_memory_artifact_service import InMemoryArtifactService # Optional\\nfrom google.adk.planners import BasePlanner, BuiltInPlanner, PlanReActPlanner\\nfrom google.adk.models import LlmRequest\\n\\nfrom google.genai.types import ThinkingConfig\\nfrom google.genai.types import GenerateContentConfig\\n\\nimport datetime\\nfrom zoneinfo import ZoneInfo\\n\\nAPP_NAME = \\\"weather_app\\\"\\nUSER_ID = \\\"1234\\\"\\nSESSION_ID = \\\"session1234\\\"\\n\\ndef get_weather(city: str) -> dict:\\n    \\\"\\\"\\\"Retrieves the current weather report for a specified city.\\n\\n    Args:\\n        city (str): The name of the city for which to retrieve the weather report.\\n\\n    Returns:\\n        dict: status and result or error msg.\\n    \\\"\\\"\\\"\\n    if city.lower() == \\\"new york\\\":\\n        return {\\n            \\\"status\\\": \\\"success\\\",\\n            \\\"report\\\": (\\n                \\\"The weather in New York is sunny with a temperature of 25 degrees\\\"\\n                \\\" Celsius (77 degrees Fahrenheit).\\\"\\n            ),\\n        }\\n    else:\\n        return {\\n            \\\"status\\\": \\\"error\\\",\\n            \\\"error_message\\\": f\\\"Weather information for '{city}' is not available.\\\",\\n        }\\n\\n\\ndef get_current_time(city: str) -> dict:\\n    \\\"\\\"\\\"Returns the current time in a specified city.\\n\\n    Args:\\n        city (str): The name of the city for which to retrieve the current time.\\n\\n    Returns:\\n        dict: status and result or error msg.\\n    \\\"\\\"\\\"\\n\\n    if city.lower() == \\\"new york\\\":\\n        tz_identifier = \\\"America/New_York\\\"\\n    else:\\n        return {\\n            \\\"status\\\": \\\"error\\\",\\n            \\\"error_message\\\": (\\n                f\\\"Sorry, I don't have timezone information for {city}.\\\"\\n            ),\\n        }\\n\\n    tz = ZoneInfo(tz_identifier)\\n    now = datetime.datetime.now(tz)\\n    report = (\\n        f'The current time in {city} is {now.strftime(\\\"%Y-%m-%d %H:%M:%S %Z%z\\\")}'\\n    )\\n    return {\\\"status\\\": \\\"success\\\", \\\"report\\\": report}\\n\\n# Step 1: Create a ThinkingConfig\\nthinking_config = ThinkingConfig(\\n    include_thoughts=True,   # Ask the model to include its thoughts in the response\\n    thinking_budget=256      # Limit the 'thinking' to 256 tokens (adjust as needed)\\n)\\nprint(\\\"ThinkingConfig:\\\", thinking_config)\\n\\n# Step 2: Instantiate BuiltInPlanner\\nplanner = BuiltInPlanner(\\n    thinking_config=thinking_config\\n)\\nprint(\\\"BuiltInPlanner created.\\\")\\n\\n# Step 3: Wrap the planner in an LlmAgent\\nagent = LlmAgent(\\n    model=\\\"gemini-2.5-pro-preview-03-25\\\",  # Set your model name\\n    name=\\\"weather_and_time_agent\\\",\\n    instruction=\\\"You are an agent that returns time and weather\\\",\\n    planner=planner,\\n    tools=[get_weather, get_current_time]\\n)\\n\\n# Session and Runner\\nsession_service = InMemorySessionService()\\nsession = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\\nrunner = Runner(agent=agent, app_name=APP_NAME, session_service=session_service)\\n\\n# Agent Interaction\\ndef call_agent(query):\\n    content = types.Content(role='user', parts=[types.Part(text=query)])\\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\\n\\n    for event in events:\\n        print(f\\\"\\\\nDEBUG EVENT: {event}\\\\n\\\")\\n        if event.is_final_response() and event.content:\\n            final_answer = event.content.parts[0].text.strip()\\n            print(\\\"\\\\n\\ud83d\\udfe2 FINAL ANSWER\\\\n\\\", final_answer, \\\"\\\\n\\\")\\n\\ncall_agent(\\\"If it's raining in New York right now, what is the current temperature?\\\")\"}, {\"sample\": 6, \"type\": \"code_error\", \"message\": \"[\\\"Import Error on line 24: cannot import name 'CallbackContext' from 'google.adk.agents' (/home/erick/repo/google_adk_chatbot/venv/lib/python3.13/site-packages/google/adk/agents/__init__.py)\\\", \\\"Import Error on line 57: cannot import name 'CallbackContext' from 'google.adk.agents' (/home/erick/repo/google_adk_chatbot/venv/lib/python3.13/site-packages/google/adk/agents/__init__.py)\\\", \\\"Instantiation Error on line 14: name 'update_user_preference' is not defined\\\", \\\"Instantiation Error on line 52: name 'update_user_preference' is not defined\\\"]\", \"bad_code\": \"from google.adk.tools import ToolContext, FunctionTool\\n\\ndef update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n    \\\"\\\"\\\"Updates a user-specific preference.\\\"\\\"\\\"\\n    user_prefs_key = \\\"user:preferences\\\"\\n    # Get current preferences or initialize if none exist\\n    preferences = tool_context.state.get(user_prefs_key, {})\\n    preferences[preference] = value\\n    # Write the updated dictionary back to the state\\n    tool_context.state[user_prefs_key] = preferences\\n    print(f\\\"Tool: Updated user preference '{preference}' to '{value}'\\\")\\n    return {\\\"status\\\": \\\"success\\\", \\\"updated_preference\\\": preference}\\n\\npref_tool = FunctionTool(func=update_user_preference)\\n\\n# In an Agent:\\n# my_agent = Agent(..., tools=[pref_tool])\\n\\n# When the LLM calls update_user_preference(preference='theme', value='dark', ...):\\n# The tool_context.state will be updated, and the change will be part of the\\n# resulting tool response event's actions.state_delta.\\n\\n# In an agent callback or tool function\\nfrom google.adk.agents import CallbackContext # or ToolContext\\n\\ndef my_callback_or_tool_function(context: CallbackContext, # Or ToolContext\\n                                 # ... other parameters ...\\n                                ):\\n    # Update existing state\\n    count = context.state.get(\\\"user_action_count\\\", 0)\\n    context.state[\\\"user_action_count\\\"] = count + 1\\n\\n    # Add new state\\n    context.state[\\\"temp:last_operation_status\\\"] = \\\"success\\\"\\n\\n    # State changes are automatically part of the event's state_delta\\n    # ... rest of callback/tool logic ...\\n\\nfrom google.adk.tools import ToolContext, FunctionTool\\n\\ndef update_user_preference(preference: str, value: str, tool_context: ToolContext):\\n    \\\"\\\"\\\"Updates a user-specific preference.\\\"\\\"\\\"\\n    user_prefs_key = \\\"user:preferences\\\"\\n    # Get current preferences or initialize if none exist\\n    preferences = tool_context.state.get(user_prefs_key, {})\\n    preferences[preference] = value\\n    # Write the updated dictionary back to the state\\n    tool_context.state[user_prefs_key] = preferences\\n    print(f\\\"Tool: Updated user preference '{preference}' to '{value}'\\\")\\n    return {\\\"status\\\": \\\"success\\\", \\\"updated_preference\\\": preference}\\n\\npref_tool = FunctionTool(func=update_user_preference)\\n\\n# You can then add this tool to your agent's tools list.\\n# When the tool is called, the state changes will be recorded and persisted automatically.\\n\\nfrom google.adk.agents import CallbackContext  # or ToolContext\\n\\ndef my_callback_or_tool_function(context: CallbackContext):\\n    count = context.state.get(\\\"user_action_count\\\", 0)\\n    context.state[\\\"user_action_count\\\"] = count + 1\\n    context.state[\\\"temp:last_operation_status\\\"] = \\\"success\\\"\\n    # These changes are automatically tracked and persisted.\"}, {\"sample\": 7, \"type\": \"code_error\", \"message\": \"[\\\"Import Error on line 115: No module named 'google.adk.runner'\\\", \\\"Import Error on line 141: No module named 'google.adk.runner'\\\", \\\"Instantiation Error on line 31: name 'GEMINI_MODEL' is not defined\\\", \\\"Instantiation Error on line 47: name 'GEMINI_MODEL' is not defined\\\", \\\"Instantiation Error on line 74: name 'GEMINI_MODEL' is not defined\\\", \\\"Instantiation Error on line 103: name 'code_writer_agent' is not defined\\\", \\\"Instantiation Error on line 124: name 'InMemoryRunner' is not defined\\\", \\\"Instantiation Error on line 176: name 'InMemoryRunner' is not defined\\\"]\", \"bad_code\": \"# Conceptual Code: Sequential Data Pipeline\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\n\\nvalidator = LlmAgent(name=\\\"ValidateInput\\\", instruction=\\\"Validate the input.\\\", output_key=\\\"validation_status\\\")\\nprocessor = LlmAgent(name=\\\"ProcessData\\\", instruction=\\\"Process data if {validation_status} is 'valid'.\\\", output_key=\\\"result\\\")\\nreporter = LlmAgent(name=\\\"ReportResult\\\", instruction=\\\"Report the result from {result}.\\\")\\n\\ndata_pipeline = SequentialAgent(\\n    name=\\\"DataPipeline\\\",\\n    sub_agents=[validator, processor, reporter]\\n)\\n# validator runs -> saves to state['validation_status']\\n# processor runs -> reads state['validation_status'], saves to state['result']\\n# reporter runs -> reads state['result']\\n\\n# Conceptual Example: Sequential Pipeline\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\n\\nstep1 = LlmAgent(name=\\\"Step1_Fetch\\\", output_key=\\\"data\\\") # Saves output to state['data']\\nstep2 = LlmAgent(name=\\\"Step2_Process\\\", instruction=\\\"Process data from {data}.\\\")\\n\\npipeline = SequentialAgent(name=\\\"MyPipeline\\\", sub_agents=[step1, step2])\\n# When pipeline runs, Step2 can access the state['data'] set by Step1.\\n\\n# Part of agent.py --> Follow https://google.github.io/adk-docs/get-started/quickstart/ to learn the setup\\n\\n# --- 1. Define Sub-Agents for Each Pipeline Stage ---\\n\\n# Code Writer Agent\\n# Takes the initial specification (from user query) and writes code.\\ncode_writer_agent = LlmAgent(\\n    name=\\\"CodeWriterAgent\\\",\\n    model=GEMINI_MODEL,\\n    instruction=\\\"\\\"\\\"You are a Python Code Generator.\\nBased *only* on the user's request, write Python code that fulfills the requirement.\\nOutput *only* the complete Python code block, enclosed in triple backticks (\\n\\n).\\nDo not add any other text before or after the code block.\\n\\\"\\\"\\\",\\n    description=\\\"Writes initial Python code based on a specification.\\\",\\n    output_key=\\\"generated_code\\\" # Stores output in state['generated_code']\\n)\\n\\n# Code Reviewer Agent\\n# Takes the code generated by the previous agent (read from state) and provides feedback.\\ncode_reviewer_agent = LlmAgent(\\n    name=\\\"CodeReviewerAgent\\\",\\n    model=GEMINI_MODEL,\\n    instruction=\\\"\\\"\\\"You are an expert Python Code Reviewer.\\nYour task is to provide constructive feedback on the provided code.\\n\\n**Code to Review:**\\n\\n**Review Criteria:**\\n1.  **Correctness:** Does the code work as intended? Are there logic errors?\\n2.  **Readability:** Is the code clear and easy to understand? Follows PEP 8 style guidelines?\\n3.  **Efficiency:** Is the code reasonably efficient? Any obvious performance bottlenecks?\\n4.  **Edge Cases:** Does the code handle potential edge cases or invalid inputs gracefully?\\n5.  **Best Practices:** Does the code follow common Python best practices?\\n\\n**Output:**\\nProvide your feedback as a concise, bulleted list. Focus on the most important points for improvement.\\nIf the code is excellent and requires no changes, simply state: \\\"No major issues found.\\\"\\nOutput *only* the review comments or the \\\"No major issues\\\" statement.\\n\\\"\\\"\\\",\\n    description=\\\"Reviews code and provides feedback.\\\",\\n    output_key=\\\"review_comments\\\", # Stores output in state['review_comments']\\n)\\n\\n\\n# Code Refactorer Agent\\n# Takes the original code and the review comments (read from state) and refactors the code.\\ncode_refactorer_agent = LlmAgent(\\n    name=\\\"CodeRefactorerAgent\\\",\\n    model=GEMINI_MODEL,\\n    instruction=\\\"\\\"\\\"You are a Python Code Refactoring AI.\\nYour goal is to improve the given Python code based on the provided review comments.\\n\\n**Original Code:**\\n\\n**Review Comments:**\\n{review_comments}\\n\\n**Task:**\\nCarefully apply the suggestions from the review comments to refactor the original code.\\nIf the review comments state \\\"No major issues found,\\\" return the original code unchanged.\\nEnsure the final code is complete, functional, and includes necessary imports and docstrings.\\n\\n**Output:**\\nOutput *only* the final, refactored Python code block, enclosed in triple backticks (\\n\\n).\\nDo not add any other text before or after the code block.\\n\\\"\\\"\\\",\\n    description=\\\"Refactors code based on review comments.\\\",\\n    output_key=\\\"refactored_code\\\", # Stores output in state['refactored_code']\\n)\\n\\n\\n# --- 2. Create the SequentialAgent ---\\n# This agent orchestrates the pipeline by running the sub_agents in order.\\ncode_pipeline_agent = SequentialAgent(\\n    name=\\\"CodePipelineAgent\\\",\\n    sub_agents=[code_writer_agent, code_reviewer_agent, code_refactorer_agent],\\n    description=\\\"Executes a sequence of code writing, reviewing, and refactoring.\\\",\\n    # The agents will run in the order provided: Writer -> Reviewer -> Refactorer\\n)\\n\\n# For ADK tools compatibility, the root agent must be named `root_agent`\\nroot_agent = code_pipeline_agent\\n\\n# Running the SequentialAgent with InMemoryRunner\\n\\nfrom google.adk.runner import InMemoryRunner\\nfrom google.adk.sessions import Session\\nfrom google.genai.types import Content, Part\\nfrom google.adk.events import Event\\n\\nUSER_ID = \\\"user_123\\\"\\nAPP_NAME = \\\"CodePipelineAgent\\\"\\n\\n# Create the runner with the root agent\\nrunner = InMemoryRunner(root_agent, APP_NAME)\\n\\n# Create a session for the user\\nsession = runner.session_service().create_session(APP_NAME, USER_ID).blocking_get()\\n\\n# Prepare user input as Content\\nuser_input = Content.from_parts(Part.from_text(\\\"Write a Python function to compute Fibonacci numbers.\\\"))\\n\\n# Run the agent asynchronously\\nevent_stream = runner.run_async(USER_ID, session.id(), user_input)\\n\\n# Collect and print final response events\\nfor event in event_stream.blocking_iter():\\n    if event.final_response():\\n        print(event.stringify_content())\\n\\nfrom google.adk.agents import SequentialAgent, LlmAgent\\nfrom google.adk.runner import InMemoryRunner\\nfrom google.adk.sessions import Session\\nfrom google.genai.types import Content, Part\\n\\n# --- Define Sub-Agents ---\\n\\nvalidator = LlmAgent(\\n    name=\\\"ValidateInput\\\",\\n    instruction=\\\"Validate the input.\\\",\\n    output_key=\\\"validation_status\\\"\\n)\\n\\nprocessor = LlmAgent(\\n    name=\\\"ProcessData\\\",\\n    instruction=\\\"Process data if {validation_status} is 'valid'.\\\",\\n    output_key=\\\"result\\\"\\n)\\n\\nreporter = LlmAgent(\\n    name=\\\"ReportResult\\\",\\n    instruction=\\\"Report the result from {result}.\\\"\\n)\\n\\n# --- Create SequentialAgent Pipeline ---\\n\\ndata_pipeline = SequentialAgent(\\n    name=\\\"DataPipeline\\\",\\n    sub_agents=[validator, processor, reporter]\\n)\\n\\n# --- Setup Runner and Session ---\\n\\nUSER_ID = \\\"user_123\\\"\\nAPP_NAME = \\\"DataPipeline\\\"\\n\\nrunner = InMemoryRunner(data_pipeline, APP_NAME)\\n\\nsession = runner.session_service().create_session(APP_NAME, USER_ID).blocking_get()\\n\\n# --- Prepare User Input ---\\n\\nuser_input = Content.from_parts(Part.from_text(\\\"Input data to validate and process.\\\"))\\n\\n# --- Run the Pipeline ---\\n\\nevent_stream = runner.run_async(USER_ID, session.id(), user_input)\\n\\n# --- Print Final Output ---\\n\\nfor event in event_stream.blocking_iter():\\n    if event.final_response():\\n        print(event.stringify_content())\"}, {\"sample\": 8, \"type\": \"code_error\", \"message\": \"[\\\"Instantiation Error on line 28: name 'CheckStatusAndEscalate' is not defined\\\", \\\"Instantiation Error on line 51: name 'CheckCondition' is not defined\\\", \\\"Instantiation Error on line 75: name 'CheckCondition' is not defined\\\"]\", \"bad_code\": \"# Conceptual Code: Iterative Code Refinement\\nfrom google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\n# Agent to generate/refine code based on state['current_code'] and state['requirements']\\ncode_refiner = LlmAgent(\\n    name=\\\"CodeRefiner\\\",\\n    instruction=\\\"Read state['current_code'] (if exists) and state['requirements']. Generate/refine Python code to meet requirements. Save to state['current_code'].\\\",\\n    output_key=\\\"current_code\\\"  # Overwrites previous code in state\\n)\\n\\n# Agent to check if the code meets quality standards\\nquality_checker = LlmAgent(\\n    name=\\\"QualityChecker\\\",\\n    instruction=\\\"Evaluate the code in state['current_code'] against state['requirements']. Output 'pass' or 'fail'.\\\",\\n    output_key=\\\"quality_status\\\"\\n)\\n\\n# Custom agent to check the status and escalate if 'pass'\\nclass CheckStatusAndEscalate(BaseAgent):\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        status = ctx.session.state.get(\\\"quality_status\\\", \\\"fail\\\")\\n        should_stop = (status == \\\"pass\\\")\\n        yield Event(author=self.name, actions=EventActions(escalate=should_stop))\\n\\nrefinement_loop = LoopAgent(\\n    name=\\\"CodeRefinementLoop\\\",\\n    max_iterations=5,\\n    sub_agents=[code_refiner, quality_checker, CheckStatusAndEscalate(name=\\\"StopChecker\\\")]\\n)\\n# Loop runs: Refiner -> Checker -> StopChecker\\n# State['current_code'] is updated each iteration.\\n# Loop stops if QualityChecker outputs 'pass' (leading to StopChecker escalating) or after 5 iterations.\\n\\n# Conceptual Example: Loop with Condition\\nfrom google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\nclass CheckCondition(BaseAgent):  # Custom agent to check state\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        status = ctx.session.state.get(\\\"status\\\", \\\"pending\\\")\\n        is_done = (status == \\\"completed\\\")\\n        yield Event(author=self.name, actions=EventActions(escalate=is_done))  # Escalate if done\\n\\nprocess_step = LlmAgent(name=\\\"ProcessingStep\\\")  # Agent that might update state['status']\\n\\npoller = LoopAgent(\\n    name=\\\"StatusPoller\\\",\\n    max_iterations=10,\\n    sub_agents=[process_step, CheckCondition(name=\\\"Checker\\\")]\\n)\\n# When poller runs, it executes process_step then Checker repeatedly\\n# until Checker escalates (state['status'] == 'completed') or 10 iterations pass.\\n\\nfrom google.adk.agents import LoopAgent, LlmAgent, BaseAgent\\nfrom google.adk.events import Event, EventActions\\nfrom google.adk.agents.invocation_context import InvocationContext\\nfrom typing import AsyncGenerator\\n\\n# Agent that performs some processing and updates state\\nprocess_step = LlmAgent(name=\\\"ProcessingStep\\\", output_key=\\\"status\\\")\\n\\n# Custom agent that checks the status and escalates to exit loop early\\nclass CheckCondition(BaseAgent):\\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\\n        status = ctx.session.state.get(\\\"status\\\", \\\"pending\\\")\\n        is_done = (status == \\\"completed\\\")\\n        yield Event(author=self.name, actions=EventActions(escalate=is_done))\\n\\n# LoopAgent with max iterations and early exit on escalate\\npoller = LoopAgent(\\n    name=\\\"StatusPoller\\\",\\n    max_iterations=10,\\n    sub_agents=[process_step, CheckCondition(name=\\\"Checker\\\")]\\n)\"}]}", "artifact_dir": null, "embedding": null}