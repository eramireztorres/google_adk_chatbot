{"id": "7bf66f70-814a-45d7-a939-34bbfc5bee06", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "d22c41ef-5e5e-4a1d-8683-6115c81449d7", "generation": 1, "timestamp": 1767447277.2089355, "iteration_found": 1, "metrics": {"combined_score": 0.711703666899449, "metrics": {"avg_score": 0.6156891025641027, "time_penalty": 0.05398543566465378, "avg_latency": 10.398543566465378, "framework_id": 2.0, "complexity": 36.6, "diversity": 2.0, "performance": 0.711703666899449}, "framework_id": 2.0, "complexity": 36.6, "artifacts": {"critique": "Framework: LangChain\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by the retrieval context, which directly quotes the import statement and the addition of a 'get_current_time' tool, while sentences 2 and 4 are unsupported as they lack direct references to the context.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.62): The score is 0.62 because while the response includes a Python example of a Function Tool with 'city' and 'unit' parameters, it also contains irrelevant details about a 'get_weather' function and its return report, which are not requested. These extraneous statements reduce the overall relevance, preventing a higher score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output's sentences do not align with any nodes in the retrieval context, which only discusses the function's purpose, arguments, logic, and return value, none of which are supported by the original expected output.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output closely matches the context's code snippets, especially in the first two sentences that directly reference 'my_before_model_logic' and its registration as a callback, aligning with the supportive reasons. However, the presence of unrelated agent names and callback functions in the latter sentences introduces ambiguity, which is why the score isn't higher.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low faithfulness (0.00): The score is 0.00 because the actual output references a 'FileSystemAgent' class and its methods, which are not mentioned in the retrieval context that only discusses the 'MCPToolset' class and its usage."}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 52 lines with 52 lines", "parent_metrics": {"combined_score": 0.8835826990762456, "metrics": {"avg_score": 0.774390172735761, "time_penalty": 0.040807473659515384, "avg_latency": 9.080747365951538, "framework_id": 2.0, "complexity": 36.6, "diversity": 2.0, "performance": 0.8835826990762456}, "framework_id": 2.0, "complexity": 36.6, "artifacts": {"critique": "Framework: LangChain\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output closely matches the code snippet in the retrieval context, especially the import statement and the agent's configuration, but some parts of the explanation are more general and not directly supported by specific lines in the context, leading to a moderate confidence level.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output's sentences 1 and 2 are not supported by the retrieval context, which lacks details about function parameters and specific function calls, as indicated by the unsupportive reasons. This disconnect results in no alignment between the expected output and the context.\nMetric faithfulness failed: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=32768, prompt_tokens=876, total_tokens=33644, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the sentence in expected output references 'MCPToolset' and 'Example 1: File System MCP Server', which are directly supported by nodes in retrieval context that mention these components, indicating partial relevance.\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because none of the sentences in the expected output are supported by the retrieval context, as all are either code definitions or setup steps unrelated to the context."}}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert Python engineer specialized in LangChain/LangGraph RAG.\nYour job is to EVOLVE a WORKING, RUNNABLE `RAGSystem` for querying a Markdown docs directory.\n\n========================\nCRITICAL CONSTRAINTS\n========================\n1) Output ONLY valid Python code (no markdown fences).\n2) The code must be self-contained within the EVOLVE-BLOCK (imports + class RAGSystem).\n3) Use gpt-4.1-mini inside the RAG code.\n4) LangChain/LangGraph only. Do NOT use LlamaIndex or Agno.\n5) Any symbol you reference MUST be imported in the EVOLVE block using allowed imports.\n\n========================\nMUTATION STRATEGY\n========================\nAlternate between these two architectures across candidates:\n  A) Manual retrieval + single LLM call\n  B) LangGraph retrieve -> generate workflow\nIf you stay with the same architecture, you must change chunking method.\n\n========================\nALLOWED IMPORTS\n========================\n  from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n  from langchain_community.document_loaders import DirectoryLoader, TextLoader\n  from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n  from langchain_experimental.text_splitter import SemanticChunker\n  from langchain_core.vectorstores import InMemoryVectorStore\n  from langchain_core.messages import HumanMessage\n  from langgraph.graph import StateGraph, START, END, MessagesState\n  from langchain.tools import tool\n  from langchain.agents import create_agent\n\nREQUIRED LOADER:\n  loader = DirectoryLoader(docs_dir, glob=\"**/*.md\", loader_cls=TextLoader, recursive=True, silent_errors=True)\n  docs = loader.load()\n\nREQUIRED EMBEDDINGS:\n  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n  probe_vec = embedding_model.embed_query(\"probe\")\n\nCHUNKING METHODS (pick one per candidate):\n  - RecursiveCharacterTextSplitter\n  - CharacterTextSplitter\n  - SemanticChunker (use try/except fallback if unavailable)\n  - Document-based (no splitting)\n\nSEMANTIC CHUNKING (SAFE PATTERN):\n  try:\n      splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n      splits = splitter.split_documents(docs)\n  except Exception:\n      splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n      splits = splitter.split_documents(docs)\n", "user": "# Current Program Information\n- Fitness: 0.8836\n- Feature coordinates: complexity=36.60, framework_id=2.00\n- Focus areas: - Fitness unchanged at 0.8836\n- Exploring complexity=36.60, framework_id=2.00 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.8836, metrics: {'avg_score': 0.774390172735761, 'time_penalty': 0.040807473659515384, 'avg_latency': 9.080747365951538, 'framework_id': 2.0, 'complexity': 36.6, 'diversity': 2.0, 'performance': 0.8835826990762456}, framework_id: 2.0000, complexity: 36.6000, artifacts: {'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output closely matches the code snippet in the retrieval context, especially the import statement and the agent's configuration, but some parts of the explanation are more general and not directly supported by specific lines in the context, leading to a moderate confidence level.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output's sentences 1 and 2 are not supported by the retrieval context, which lacks details about function parameters and specific function calls, as indicated by the unsupportive reasons. This disconnect results in no alignment between the expected output and the context.\\nMetric faithfulness failed: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=32768, prompt_tokens=876, total_tokens=33644, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the sentence in expected output references 'MCPToolset' and 'Example 1: File System MCP Server', which are directly supported by nodes in retrieval context that mention these components, indicating partial relevance.\\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because none of the sentences in the expected output are supported by the retrieval context, as all are either code definitions or setup steps unrelated to the context.\"}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.8836)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.8836), Performs well on metrics ({'avg_score': 0.774390172735761, 'time_penalty': 0.040807473659515384, 'avg_latency': 9.080747365951538, 'framework_id': 2.0, 'complexity': 36.6, 'diversity': 2.0, 'performance': 0.8835826990762456}), Performs well on framework_id (2.0000), Performs well on complexity (36.6000), Performs well on artifacts ({'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output closely matches the code snippet in the retrieval context, especially the import statement and the agent's configuration, but some parts of the explanation are more general and not directly supported by specific lines in the context, leading to a moderate confidence level.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output's sentences 1 and 2 are not supported by the retrieval context, which lacks details about function parameters and specific function calls, as indicated by the unsupportive reasons. This disconnect results in no alignment between the expected output and the context.\\nMetric faithfulness failed: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=32768, prompt_tokens=876, total_tokens=33644, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the sentence in expected output references 'MCPToolset' and 'Example 1: File System MCP Server', which are directly supported by nodes in retrieval context that mention these components, indicating partial relevance.\\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because none of the sentences in the expected output are supported by the retrieval context, as all are either code definitions or setup steps unrelated to the context.\"})\n\n\n\n# Current Program\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity, framework_id\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> dict:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n=======\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> dict:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}