{"id": "4a41f870-2c29-42f4-9970-3c5ebf4546be", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 400\n        Settings.chunk_overlap = 80\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n\n        splitter = SentenceSplitter(chunk_size=400, chunk_overlap=80)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=6)\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "fad9e496-9a00-4b67-9d27-6e06549413d0", "generation": 2, "timestamp": 1767453732.4627419, "iteration_found": 7, "metrics": {"combined_score": 0.4972325653471909, "metrics": {"avg_score": 0.5742457542457543, "time_penalty": 0.07701318889856339, "avg_latency": 12.701318889856339, "framework_id": 0.0, "complexity": 37.4, "diversity": 0.0, "performance": 0.4972325653471909}, "framework_id": 0.0, "complexity": 37.4, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low answer_relevancy (0.33): The score is 0.33 because the output includes several irrelevant statements about importing modules, agent instantiation, configuration details, and descriptive attributes, which are not directly related to the specific Python code defining the get_current_time tool and the root_agent. These extraneous details reduce the overall relevance, but some core code snippets are present, preventing the score from being lower.\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.20): The score is 0.20 because the expected output's first sentence aligns with node 1 in the retrieval context, which discusses defining tools and including 'get_current_time'. The other sentences are unrelated to the retrieval context, leading to a low score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.67): The score is 0.67 because while the response includes a Python example of a Function Tool with the specified parameters, it also contains irrelevant statements about weather reports and function return structures that do not directly address the user's request, slightly reducing the overall relevance.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output's content about weather reporting does not align with the context, which only discusses function parameters and signatures, leading to a complete lack of relevant support from the retrieval node.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.00): The score is 0.00 because the actual output introduces a function named 'my_before_model_logic' with parameters 'callback_context' and 'llm_request', which are not mentioned in the retrieval context. Additionally, it states that this function returns 'None', whereas the context indicates callback functions return 'nil' or 'Optional', and does not specify such a function or its return value. It also references an 'LlmAgent' instance named 'SimpleAgent' with specific parameters, which are not present in the context. These discrepancies show the output is not faithful to the provided retrieval information."}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 58 lines with 53 lines", "parent_metrics": {"combined_score": 0.763305855431254, "metrics": {"avg_score": 0.8175099206349207, "time_penalty": 0.05420406520366669, "avg_latency": 10.420406520366669, "framework_id": 0.0, "complexity": 40.0, "diversity": 0.0, "performance": 0.763305855431254}, "framework_id": 0.0, "complexity": 40.0, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low answer_relevancy (0.62): The score is 0.62 because some parts of the output, such as explanations about expected function outputs and unrelated import statements, are irrelevant to the actual code definitions. However, the core code for 'get_current_time' and 'root_agent' is present, which justifies the partial relevance and prevents a lower score.\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.12): The score is 0.12 because the only sentence in the expected output that directly relates to the retrieval context is the first, which mentions 'Install ADK by running the following command', aligning with node 1 in the context. The remaining sentences are not supported by the context, leading to a low recall score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.62): The score is 0.62 because the response included irrelevant details about weather reports instead of providing the Python example of the Function Tool with 'city' and 'unit' parameters as requested. However, it partially addressed the request, which is why the score isn't lower.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.38): The score is 0.38 because the output contains several irrelevant statements about importing modules and setting properties that do not directly demonstrate registering a before_model callback, which limits its relevance despite some related context.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the context mentions 'os.path.abspath(TARGET_FOLDER_PATH)' used in args, indicating the path is relevant, which supports sentence 13 in the expected output. However, the context lacks specific sentences that directly correspond to other parts of the output, leading to a moderate confidence in attribution."}}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert Python engineer specialized in LlamaIndex RAG.\nYour job is to EVOLVE a WORKING, RUNNABLE `RAGSystem` for querying a Markdown docs directory.\n\n========================\nCRITICAL CONSTRAINTS\n========================\n1) Output ONLY valid Python code (no markdown fences).\n2) The code must be self-contained within the EVOLVE-BLOCK (imports + class RAGSystem).\n3) Use gpt-4.1-mini inside the RAG code.\n4) LlamaIndex only. Do NOT use LangChain/LangGraph or Agno.\n5) Any symbol you reference MUST be imported in the EVOLVE block using allowed imports.\n\n========================\nMUTATION STRATEGY\n========================\nAlternate between these retrieval architectures across candidates:\n  A) RetrieverQueryEngine + LLMRerank\n  B) SubQuestionQueryEngine + QueryEngineTool\n  C) RouterQueryEngine + QueryEngineTool\nIf you stay with the same architecture, you must change chunking method.\nAvoid external vector stores unless dependencies are installed; fall back to VectorStoreIndex.\n\n========================\nALLOWED IMPORTS\n========================\n  from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n  from llama_index.llms.openai import OpenAI\n  from llama_index.embeddings.openai import OpenAIEmbedding\n  from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n  from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step, Context\n  from llama_index.core.schema import NodeWithScore\n  from llama_index.core.query_engine import SubQuestionQueryEngine, RouterQueryEngine, RetrieverQueryEngine\n  from llama_index.core.postprocessor import LLMRerank\n  from llama_index.core.tools import QueryEngineTool\n\nREQUIRED INGESTION:\n  reader = SimpleDirectoryReader(input_dir=docs_dir, recursive=True, required_exts=[\".md\", \".txt\"])\n  documents = reader.load_data()\n\nCHUNKING METHODS (pick one per candidate):\n  - SentenceSplitter\n  - SemanticSplitterNodeParser (with try/except fallback)\n  - Document-based (VectorStoreIndex.from_documents)\n\nTOOL ROUTING (SAFE PATTERNS):\n  tool1 = QueryEngineTool.from_defaults(\n      query_engine=index.as_query_engine(similarity_top_k=3),\n      description=\"General documentation queries.\",\n  )\n  tool2 = QueryEngineTool.from_defaults(\n      query_engine=index.as_query_engine(similarity_top_k=6),\n      description=\"Broader context queries.\",\n  )\n\n  # SubQuestionQueryEngine\n  query_engine = SubQuestionQueryEngine.from_defaults(\n      query_engine_tools=[tool1, tool2]\n  )\n\n  # RouterQueryEngine\n  query_engine = RouterQueryEngine.from_defaults(\n      query_engine_tools=[tool1, tool2]\n  )\n\nWORKFLOW RAG (SAFE SKELETON):\n  class RetrieverEvent(Event):\n      nodes: list[NodeWithScore]\n\n  class RerankEvent(Event):\n      nodes: list[NodeWithScore]\n\n  class RAGWorkflow(Workflow):\n      @step\n      async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n          dirname = ev.get(\"dirname\")\n          if not dirname:\n              return None\n          docs = SimpleDirectoryReader(dirname).load_data()\n          index = VectorStoreIndex.from_documents(docs)\n          return StopEvent(result=index)\n\n      @step\n      async def retrieve(self, ctx: Context, ev: StartEvent) -> RetrieverEvent | None:\n          query = ev.get(\"query\")\n          index = ev.get(\"index\")\n          if not query or index is None:\n              return None\n          retriever = index.as_retriever(similarity_top_k=2)\n          nodes = await retriever.aretrieve(query)\n          return RetrieverEvent(nodes=nodes)\n\n      @step\n      async def rerank(self, ctx: Context, ev: RetrieverEvent) -> RerankEvent:\n          ranker = LLMRerank(choice_batch_size=5, top_n=3)\n          new_nodes = ranker.postprocess_nodes(ev.nodes, query_str=ev.get(\"query\"))\n          return RerankEvent(nodes=new_nodes)\n\n      @step\n      async def synthesize(self, ctx: Context, ev: RerankEvent) -> StopEvent:\n          response = \" \".join([n.node.get_content() for n in ev.nodes])\n          return StopEvent(result=response)\n", "user": "# Current Program Information\n- Fitness: 0.7633\n- Feature coordinates: complexity=40.00, framework_id=0.00\n- Focus areas: - Fitness unchanged at 0.7633\n- Exploring complexity=40.00, framework_id=0.00 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Change 1: Replace 49 lines with 55 lines\n- Metrics: combined_score: 0.7633, metrics: {'avg_score': 0.8175099206349207, 'time_penalty': 0.05420406520366669, 'avg_latency': 10.420406520366669, 'framework_id': 0.0, 'complexity': 40.0, 'diversity': 0.0, 'performance': 0.763305855431254}, framework_id: 0.0000, complexity: 40.0000, artifacts: {'critique': \"Framework: Llama\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low answer_relevancy (0.62): The score is 0.62 because some parts of the output, such as explanations about expected function outputs and unrelated import statements, are irrelevant to the actual code definitions. However, the core code for 'get_current_time' and 'root_agent' is present, which justifies the partial relevance and prevents a lower score.\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.12): The score is 0.12 because the only sentence in the expected output that directly relates to the retrieval context is the first, which mentions 'Install ADK by running the following command', aligning with node 1 in the context. The remaining sentences are not supported by the context, leading to a low recall score.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.62): The score is 0.62 because the response included irrelevant details about weather reports instead of providing the Python example of the Function Tool with 'city' and 'unit' parameters as requested. However, it partially addressed the request, which is why the score isn't lower.\\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.38): The score is 0.38 because the output contains several irrelevant statements about importing modules and setting properties that do not directly demonstrate registering a before_model callback, which limits its relevance despite some related context.\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the context mentions 'os.path.abspath(TARGET_FOLDER_PATH)' used in args, indicating the path is relevant, which supports sentence 13 in the expected output. However, the context lacks specific sentences that directly correspond to other parts of the output, leading to a moderate confidence in attribution.\"}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.7633)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 128\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        try:\n            splitter = SemanticSplitterNodeParser(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n        except Exception:\n            # fallback to sentence splitter\n            from llama_index.core.node_parser import SentenceSplitter\n            splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=3)\n        reranker = LLMRerank(top_n=2)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.7633), Performs well on metrics ({'avg_score': 0.8175099206349207, 'time_penalty': 0.05420406520366669, 'avg_latency': 10.420406520366669, 'framework_id': 0.0, 'complexity': 40.0, 'diversity': 0.0, 'performance': 0.763305855431254}), Performs well on framework_id (0.0000), Performs well on complexity (40.0000), Performs well on artifacts ({'critique': \"Framework: Llama\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low answer_relevancy (0.62): The score is 0.62 because some parts of the output, such as explanations about expected function outputs and unrelated import statements, are irrelevant to the actual code definitions. However, the core code for 'get_current_time' and 'root_agent' is present, which justifies the partial relevance and prevents a lower score.\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.12): The score is 0.12 because the only sentence in the expected output that directly relates to the retrieval context is the first, which mentions 'Install ADK by running the following command', aligning with node 1 in the context. The remaining sentences are not supported by the context, leading to a low recall score.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.62): The score is 0.62 because the response included irrelevant details about weather reports instead of providing the Python example of the Function Tool with 'city' and 'unit' parameters as requested. However, it partially addressed the request, which is why the score isn't lower.\\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.38): The score is 0.38 because the output contains several irrelevant statements about importing modules and setting properties that do not directly demonstrate registering a before_model callback, which limits its relevance despite some related context.\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the context mentions 'os.path.abspath(TARGET_FOLDER_PATH)' used in args, indicating the path is relevant, which supports sentence 13 in the expected output. However, the context lacks specific sentences that directly correspond to other parts of the output, leading to a moderate confidence in attribution.\"})\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.6206)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 128\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        try:\n            splitter = SemanticSplitterNodeParser(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n        except Exception:\n            # fallback to sentence splitter\n            from llama_index.core.node_parser import SentenceSplitter\n            splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=3)\n        reranker = LLMRerank(top_n=5)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D2 (Score: 0.7314)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 128\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        try:\n            splitter = SemanticSplitterNodeParser(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n        except Exception:\n            # fallback to sentence splitter\n            from llama_index.core.node_parser import SentenceSplitter\n            splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=3)\n        reranker = LLMRerank(top_n=2)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D3 (Score: 0.5480)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 400\n        Settings.chunk_overlap = 80\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=400, chunk_overlap=80)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=6)\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D4 (Score: 0.0000)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import SubQuestionQueryEngine, QueryEngineTool\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index1 = None\n        self.index2 = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 350\n        Settings.chunk_overlap = 70\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n\n        splitter = SentenceSplitter(chunk_size=350, chunk_overlap=70)\n        nodes = splitter.get_nodes_from_documents(documents)\n        self.index1 = VectorStoreIndex(nodes)\n\n        # Document-based chunking for broader context\n        self.index2 = VectorStoreIndex.from_documents(documents)\n\n        tool1 = QueryEngineTool.from_defaults(\n            query_engine=self.index1.as_query_engine(similarity_top_k=4),\n            description=\"Focused documentation queries.\",\n        )\n        tool2 = QueryEngineTool.from_defaults(\n            query_engine=self.index2.as_query_engine(similarity_top_k=7),\n            description=\"Broader context documentation queries.\",\n        )\n\n        self.query_engine = SubQuestionQueryEngine.from_defaults(\n            query_engine_tools=[tool1, tool2]\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        # response might have source_nodes attribute or not depending on engine; collect context safely\n        contexts = []\n        if hasattr(response, \"source_nodes\") and response.source_nodes is not None:\n            contexts = [node.node.get_content() for node in response.source_nodes]\n        elif hasattr(response, \"nodes\") and response.nodes is not None:\n            contexts = [node.node.get_content() for node in response.nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import SubQuestionQueryEngine, QueryEngineTool\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index1 = None\n        self.index2 = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 350\n        Settings.chunk_overlap = 70\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n\n        splitter = SentenceSplitter(chunk_size=350, chunk_overlap=70)\n        nodes = splitter.get_nodes_from_documents(documents)\n        self.index1 = VectorStoreIndex(nodes)\n\n        # Document-based chunking for broader context\n        self.index2 = VectorStoreIndex.from_documents(documents)\n\n        tool1 = QueryEngineTool.from_defaults(\n            query_engine=self.index1.as_query_engine(similarity_top_k=4),\n            description=\"Focused documentation queries.\",\n        )\n        tool2 = QueryEngineTool.from_defaults(\n            query_engine=self.index2.as_query_engine(similarity_top_k=7),\n            description=\"Broader context documentation queries.\",\n        )\n\n        self.query_engine = SubQuestionQueryEngine.from_defaults(\n            query_engine_tools=[tool1, tool2]\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        # response might have source_nodes attribute or not depending on engine; collect context safely\n        contexts = []\n        if hasattr(response, \"source_nodes\") and response.source_nodes is not None:\n            contexts = [node.node.get_content() for node in response.source_nodes]\n        elif hasattr(response, \"nodes\") and response.nodes is not None:\n            contexts = [node.node.get_content() for node in response.nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Modification: Change 1: Replace 55 lines with 65 lines\n\n# Current Program\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 128\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        try:\n            splitter = SemanticSplitterNodeParser(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n        except Exception:\n            # fallback to sentence splitter\n            from llama_index.core.node_parser import SentenceSplitter\n            splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=3)\n        reranker = LLMRerank(top_n=2)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity, framework_id\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 128\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        try:\n            splitter = SemanticSplitterNodeParser(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n        except Exception:\n            # fallback to sentence splitter\n            from llama_index.core.node_parser import SentenceSplitter\n            splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=3)\n        reranker = LLMRerank(top_n=2)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n=======\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 400\n        Settings.chunk_overlap = 80\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n\n        splitter = SentenceSplitter(chunk_size=400, chunk_overlap=80)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=6)\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n>>>>>>> REPLACE\n\nExplanation:\n- Switched chunking to SentenceSplitter for a different chunking approach to promote diversity.\n- Reduced chunk size from 512 to 400 and chunk overlap from 128 to 80 to reduce complexity and possibly improve latency.\n- Increased similarity_top_k to 6 and rerank top_n to 3 to potentially improve answer quality with more candidate contexts.\n- Removed SemanticSplitterNodeParser usage to simplify code and reduce complexity.\n- This corresponds to retrieval architecture A (RetrieverQueryEngine + LLMRerank) with alternative chunking method (SentenceSplitter)."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}