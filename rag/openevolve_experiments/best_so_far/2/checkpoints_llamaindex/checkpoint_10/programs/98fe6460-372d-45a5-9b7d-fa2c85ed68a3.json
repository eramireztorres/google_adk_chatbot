{"id": "98fe6460-372d-45a5-9b7d-fa2c85ed68a3", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        # retrieve top 5 for better context coverage\n        retrieved = self.vector_store.similarity_search(query_str, k=5)\n        context = \"\\n\\n---\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context below to answer the question.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\\n\"\n            \"Answer in detail, referencing the context.\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "36814b78-2950-4fee-b673-2726207a4f03", "generation": 2, "timestamp": 1767448750.2343428, "iteration_found": 0, "metrics": {"combined_score": 0.9437343948737287, "metrics": {"avg_score": 0.9082864832535886, "time_penalty": 0.11455208837985993, "avg_latency": 16.455208837985992, "framework_id": 2.0, "complexity": 36.9, "diversity": 2.0, "performance": 0.9437343948737287}, "framework_id": 2.0, "complexity": 36.9, "artifacts": {"critique": "Framework: LangChain\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output consists entirely of code snippets and import statements that are unrelated to the retrieval context, which is focused on a function returning the current time in a city. The retrieval context does not support any of the sentences in the expected output, leading to a low score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.67): The score is 0.67 because the first two sentences in the expected output are supported by nodes 2 and 3, which contain the function definition and its description, but the remaining sentences are unrelated to the nodes, leading to a moderate confidence in the attribution."}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: 'from langchain_text_splitters import RecursiveCharacterTextSplitter' to 'from langchain_text_splitters import CharacterTextSplitter'\nChange 2: Replace 2 lines with 2 lines\nChange 3: Replace 9 lines with 11 lines", "parent_metrics": {"combined_score": 0.711703666899449, "metrics": {"avg_score": 0.6156891025641027, "time_penalty": 0.05398543566465378, "avg_latency": 10.398543566465378, "framework_id": 2.0, "complexity": 36.6, "diversity": 2.0, "performance": 0.711703666899449}, "framework_id": 2.0, "complexity": 36.6, "artifacts": {"critique": "Framework: LangChain\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by the retrieval context, which directly quotes the import statement and the addition of a 'get_current_time' tool, while sentences 2 and 4 are unsupported as they lack direct references to the context.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.62): The score is 0.62 because while the response includes a Python example of a Function Tool with 'city' and 'unit' parameters, it also contains irrelevant details about a 'get_weather' function and its return report, which are not requested. These extraneous statements reduce the overall relevance, preventing a higher score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output's sentences do not align with any nodes in the retrieval context, which only discusses the function's purpose, arguments, logic, and return value, none of which are supported by the original expected output.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output closely matches the context's code snippets, especially in the first two sentences that directly reference 'my_before_model_logic' and its registration as a callback, aligning with the supportive reasons. However, the presence of unrelated agent names and callback functions in the latter sentences introduces ambiguity, which is why the score isn't higher.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low faithfulness (0.00): The score is 0.00 because the actual output references a 'FileSystemAgent' class and its methods, which are not mentioned in the retrieval context that only discusses the 'MCPToolset' class and its usage."}}, "island": 1, "migrant": true}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}