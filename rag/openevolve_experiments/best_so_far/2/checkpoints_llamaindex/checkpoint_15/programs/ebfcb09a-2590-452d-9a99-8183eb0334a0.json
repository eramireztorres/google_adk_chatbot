{"id": "ebfcb09a-2590-452d-9a99-8183eb0334a0", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        try:\n            splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n            splits = splitter.split_documents(docs)\n        except Exception:\n            from langchain_text_splitters import RecursiveCharacterTextSplitter\n            splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n            splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n---\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are an expert assistant. Use the following extracted context to answer the question.\\n\\n\"\n            f\"EXTRACTED CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n            \"If the answer cannot be found in the context, reply that you do not know.\\n\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "7bf66f70-814a-45d7-a939-34bbfc5bee06", "generation": 2, "timestamp": 1767448750.224079, "iteration_found": 8, "metrics": {"combined_score": 0.9118235268616919, "metrics": {"avg_score": 0.7648358585858587, "time_penalty": 0.00301233172416687, "avg_latency": 5.301233172416687, "framework_id": 2.0, "complexity": 39.6, "diversity": 2.0, "performance": 0.9118235268616919}, "framework_id": 2.0, "complexity": 39.6, "artifacts": {"critique": "Framework: LangChain\nExecution Error on query 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.': Function timed out after 120 seconds\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.50): The score is 0.50 because the code snippet's function and docstring are somewhat supported by the node in retrieval context, but the unrelated documentation reduces confidence in full attribution.\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.50): The score is 0.50 because the supportive reasons (sentences 1 and 3) directly relate to the retrieval context nodes discussing 'output_key' and 'append_event', while the unsupportive reasons (sentences 2 and 4) do not directly connect to the nodes, leading to a moderate confidence in the attribution.\nQuery: 'In ADK, what is session.state and how does it differ from session.events?' -> Low contextual_recall (0.29): The score is 0.29 because the first two sentences in the expected output are directly supported by the retrieval context, referencing 'session.state' and its role, which aligns with nodes 1 and 2. The remaining sentences are unsupported as they mention concepts like 'session.events', 'session.actions', and other mechanisms not present in the retrieval context, leading to a low confidence in the attribution."}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: 'from langchain_text_splitters import RecursiveCharacterTextSplitter' to 'from langchain_experimental.text_splitter import SemanticChunker'\nChange 2: Replace 2 lines with 7 lines\nChange 3: Replace 9 lines with 10 lines", "parent_metrics": {"combined_score": 0.711703666899449, "metrics": {"avg_score": 0.6156891025641027, "time_penalty": 0.05398543566465378, "avg_latency": 10.398543566465378, "framework_id": 2.0, "complexity": 36.6, "diversity": 2.0, "performance": 0.711703666899449}, "framework_id": 2.0, "complexity": 36.6, "artifacts": {"critique": "Framework: LangChain\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by the retrieval context, which directly quotes the import statement and the addition of a 'get_current_time' tool, while sentences 2 and 4 are unsupported as they lack direct references to the context.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.62): The score is 0.62 because while the response includes a Python example of a Function Tool with 'city' and 'unit' parameters, it also contains irrelevant details about a 'get_weather' function and its return report, which are not requested. These extraneous statements reduce the overall relevance, preventing a higher score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output's sentences do not align with any nodes in the retrieval context, which only discusses the function's purpose, arguments, logic, and return value, none of which are supported by the original expected output.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output closely matches the context's code snippets, especially in the first two sentences that directly reference 'my_before_model_logic' and its registration as a callback, aligning with the supportive reasons. However, the presence of unrelated agent names and callback functions in the latter sentences introduces ambiguity, which is why the score isn't higher.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low faithfulness (0.00): The score is 0.00 because the actual output references a 'FileSystemAgent' class and its methods, which are not mentioned in the retrieval context that only discusses the 'MCPToolset' class and its usage."}}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert Python engineer specialized in LangChain/LangGraph RAG.\nYour job is to EVOLVE a WORKING, RUNNABLE `RAGSystem` for querying a Markdown docs directory.\n\n========================\nCRITICAL CONSTRAINTS\n========================\n1) Output ONLY valid Python code (no markdown fences).\n2) The code must be self-contained within the EVOLVE-BLOCK (imports + class RAGSystem).\n3) Use gpt-4.1-mini inside the RAG code.\n4) LangChain/LangGraph only. Do NOT use LlamaIndex or Agno.\n5) Any symbol you reference MUST be imported in the EVOLVE block using allowed imports.\n\n========================\nMUTATION STRATEGY\n========================\nAlternate between these two architectures across candidates:\n  A) Manual retrieval + single LLM call\n  B) LangGraph retrieve -> generate workflow\nIf you stay with the same architecture, you must change chunking method.\n\n========================\nALLOWED IMPORTS\n========================\n  from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n  from langchain_community.document_loaders import DirectoryLoader, TextLoader\n  from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n  from langchain_experimental.text_splitter import SemanticChunker\n  from langchain_core.vectorstores import InMemoryVectorStore\n  from langchain_core.messages import HumanMessage\n  from langgraph.graph import StateGraph, START, END, MessagesState\n  from langchain.tools import tool\n  from langchain.agents import create_agent\n\nREQUIRED LOADER:\n  loader = DirectoryLoader(docs_dir, glob=\"**/*.md\", loader_cls=TextLoader, recursive=True, silent_errors=True)\n  docs = loader.load()\n\nREQUIRED EMBEDDINGS:\n  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n  probe_vec = embedding_model.embed_query(\"probe\")\n\nCHUNKING METHODS (pick one per candidate):\n  - RecursiveCharacterTextSplitter\n  - CharacterTextSplitter\n  - SemanticChunker (use try/except fallback if unavailable)\n  - Document-based (no splitting)\n\nSEMANTIC CHUNKING (SAFE PATTERN):\n  try:\n      splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n      splits = splitter.split_documents(docs)\n  except Exception:\n      splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n      splits = splitter.split_documents(docs)\n", "user": "# Current Program Information\n- Fitness: 0.7117\n- Feature coordinates: complexity=36.60, framework_id=2.00\n- Focus areas: - Fitness declined: 0.9134 \u2192 0.7117. Consider revising recent changes.\n- Exploring complexity=36.60, framework_id=2.00 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Change 1: 'from langchain_text_splitters import RecursiveCharacterTextSplitter' to 'from langchain_text_splitters import CharacterTextSplitter'\nChange 2: Replace 2 lines with 2 lines\nChange 3: Replace 9 lines with 10 lines\n- Metrics: combined_score: 0.9134, metrics: {'avg_score': 0.7896103896103897, 'time_penalty': 0.026199783980846405, 'avg_latency': 7.6199783980846405, 'framework_id': 2.0, 'complexity': 36.9, 'diversity': 2.0, 'performance': 0.9134106056295432}, framework_id: 2.0000, complexity: 36.9000, artifacts: {'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's focus on telling the current time aligns with the context's description, but the presence of unrelated code and instructions reduces the overall confidence in a direct match.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low faithfulness (0.50): The score is 0.50 because the contradiction states that the context indicates it does not contain a Python example of a Function Tool with parameters 'city' and 'unit', which aligns with the claim, suggesting some inconsistency in the actual output.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.00): The score is 0.00 because the actual output did not include the requested Python example of a Function Tool with parameters city and unit, making all statements irrelevant.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the context discusses tools and functions in general, but does not include any information about the specific weather function or its output, which are necessary to support the expected output.\\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.00): The score is 0.00 because the actual output contradicts the retrieval context by claiming the `name` parameter is `'MyCallbackAgent'` instead of `'guardrail_agent'`, the `model` parameter is `'gemini-2.0-flash'` instead of `'GEMINI_2_FLASH'`, and the `instruction` parameter is `'Be helpful.'`' instead of `'You are a helpful assistant.'.\"}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9134)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=3)\n        context = \"\\n\\n---\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are an expert assistant. Use the following extracted context to answer the question.\\n\\n\"\n            f\"EXTRACTED CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n            \"If the answer cannot be found in the context, reply that you do not know.\\n\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.9134), Performs well on metrics ({'avg_score': 0.7896103896103897, 'time_penalty': 0.026199783980846405, 'avg_latency': 7.6199783980846405, 'framework_id': 2.0, 'complexity': 36.9, 'diversity': 2.0, 'performance': 0.9134106056295432}), Performs well on framework_id (2.0000), Performs well on complexity (36.9000), Performs well on artifacts ({'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's focus on telling the current time aligns with the context's description, but the presence of unrelated code and instructions reduces the overall confidence in a direct match.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low faithfulness (0.50): The score is 0.50 because the contradiction states that the context indicates it does not contain a Python example of a Function Tool with parameters 'city' and 'unit', which aligns with the claim, suggesting some inconsistency in the actual output.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.00): The score is 0.00 because the actual output did not include the requested Python example of a Function Tool with parameters city and unit, making all statements irrelevant.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the context discusses tools and functions in general, but does not include any information about the specific weather function or its output, which are necessary to support the expected output.\\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.00): The score is 0.00 because the actual output contradicts the retrieval context by claiming the `name` parameter is `'MyCallbackAgent'` instead of `'guardrail_agent'`, the `model` parameter is `'gemini-2.0-flash'` instead of `'GEMINI_2_FLASH'`, and the `instruction` parameter is `'Be helpful.'`' instead of `'You are a helpful assistant.'.\"})\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.8836)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D2 (Score: 0.8032)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D3 (Score: 0.7117)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D4 (Score: 0.8384)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.9134, Type: High-Performer)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=3)\n        context = \"\\n\\n---\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are an expert assistant. Use the following extracted context to answer the question.\\n\\n\"\n            f\"EXTRACTED CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n            \"If the answer cannot be found in the context, reply that you do not know.\\n\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Excellent combined_score (0.913)\n\n# Current Program\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity, framework_id\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n=======\nfrom langchain_experimental.text_splitter import SemanticChunker\n\n>>>>>>> REPLACE\n<<<<<<< SEARCH\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n=======\n        try:\n            splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n            splits = splitter.split_documents(docs)\n        except Exception:\n            from langchain_text_splitters import RecursiveCharacterTextSplitter\n            splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n            splits = splitter.split_documents(docs)\n>>>>>>> REPLACE\n<<<<<<< SEARCH\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n=======\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n---\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are an expert assistant. Use the following extracted context to answer the question.\\n\\n\"\n            f\"EXTRACTED CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n            \"If the answer cannot be found in the context, reply that you do not know.\\n\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}