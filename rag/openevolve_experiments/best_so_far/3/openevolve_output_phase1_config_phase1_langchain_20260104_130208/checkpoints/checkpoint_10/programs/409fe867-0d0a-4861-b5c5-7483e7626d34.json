{"id": "409fe867-0d0a-4861-b5c5-7483e7626d34", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import SemanticChunker, RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import StateGraph, START, END, MessagesState\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self.vector_store = None\n        self.graph = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n\n        try:\n            splitter = SemanticChunker(embeddings)\n            splits = splitter.split_documents(docs)\n        except Exception:\n            # Fallback to recursive splitter on failure\n            splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n            splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        def retrieve(state: MessagesState):\n            user_text = state[\"messages\"][-1].content\n            retrieved = self.vector_store.similarity_search(user_text, k=4)\n            context = \"\\n\\n\".join(d.page_content for d in retrieved)\n            augmented = f\"{user_text}\\n\\nCONTEXT:\\n{context}\"\n            return {\"messages\": [HumanMessage(content=augmented)]}\n\n        def generate(state: MessagesState):\n            response = self.llm.invoke(state[\"messages\"])\n            return {\"messages\": [response]}\n\n        self.graph = StateGraph(MessagesState)\n        self.graph.add_node(\"retrieve\", retrieve)\n        self.graph.add_node(\"generate\", generate)\n        self.graph.add_edge(START, \"retrieve\")\n        self.graph.add_edge(\"retrieve\", \"generate\")\n        self.graph.add_edge(\"generate\", END)\n\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        # Run LangGraph state graph workflow retrieve->generate\n        state = MessagesState(messages=[HumanMessage(content=query_str)])\n        final_state = self.graph.run(state)\n        answer = final_state[\"messages\"][-1].content.strip()\n        # Also return contexts from retrieval step for debugging\n        # We replicate retrieval to get contexts\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "c7d95831-f072-4d8c-be31-1f1e405bf6e3", "generation": 2, "timestamp": 1767529227.8179476, "iteration_found": 6, "metrics": {"combined_score": 0.0, "metrics": {"avg_score": 0.0, "time_penalty": 0.0, "avg_latency": 0.0, "framework_id": 2.0, "complexity": 56.1, "diversity": 2.0, "performance": 0.0}, "artifacts": {"critique": "Import failed: cannot import name 'SemanticChunker' from 'langchain_text_splitters' (/home/erick/repo/openevolve/trials/optimum_rag/venv/lib/python3.13/site-packages/langchain_text_splitters/__init__.py)"}, "framework_id": 2.0, "complexity": 56.1}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 58 lines with 78 lines", "parent_metrics": {"combined_score": 0.8321746342664674, "metrics": {"avg_score": 0.7100595238095239, "time_penalty": 0.027884889543056488, "avg_latency": 7.788488954305649, "framework_id": 2.0, "complexity": 36.9, "diversity": 2.0, "performance": 0.8321746342664674}, "framework_id": 2.0, "complexity": 36.9, "artifacts": {"critique": "Framework: LangChain\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the first sentence in the expected output directly references the node about updating the agent code, aligning with the supportive reason, while the second sentence pertains to deployment instructions, which are not supported by the retrieval context, leading to a moderate confidence in the attribution.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low faithfulness (0.25): The score is 0.25 because the context includes a partial example with `city` and `unit` parameters, contradicting the claim that it lacks a complete Python example of a Function Tool with these parameters.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.25): The score is 0.25 because the output contains only a partial Python example related to 'city' and 'unit' parameters, and does not provide a complete, fully detailed example as requested, making it only somewhat relevant.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because none of the sentences in the expected output are supported by the nodes in retrieval context, which do not mention the specific weather function or related details.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.33): The score is 0.33 because the contradictions indicate that all claims about the code snippets, including language, class instantiation, parameters, and callback functions, align with the retrieval context, which should result in a higher faithfulness score."}}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert Python engineer specialized in LangChain/LangGraph RAG.\nYour job is to EVOLVE a WORKING, RUNNABLE `RAGSystem` for querying a Markdown docs directory.\n\n========================\nCRITICAL CONSTRAINTS\n========================\n1) Output ONLY valid Python code (no markdown fences).\n2) The code must be self-contained within the EVOLVE-BLOCK (imports + class RAGSystem).\n3) Use gpt-4.1-mini inside the RAG code.\n4) LangChain/LangGraph only. Do NOT use LlamaIndex or Agno.\n5) Any symbol you reference MUST be imported in the EVOLVE block using allowed imports.\n\n========================\nMUTATION STRATEGY\n========================\nAlternate between these two architectures across candidates:\n  A) Manual retrieval + single LLM call\n  B) LangGraph retrieve -> generate workflow\nIf you stay with the same architecture, you must change chunking method.\nChunking must rotate across these five styles over the run:\n  1) Fixed-size CharacterTextSplitter\n  2) RecursiveCharacterTextSplitter\n  3) SemanticChunker (with fallback)\n  4) Document-based (no splitting)\n  5) Fixed-size CharacterTextSplitter with different separator config\nEnforce LangGraph usage: every second candidate MUST use StateGraph.\n\n========================\nALLOWED IMPORTS\n========================\n  from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n  from langchain_community.document_loaders import DirectoryLoader, TextLoader\n  from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n  from langchain_experimental.text_splitter import SemanticChunker\n  from langchain_community.retrievers import BM25Retriever\n  from langchain_core.vectorstores import InMemoryVectorStore\n  from langchain_core.messages import HumanMessage\n  from langgraph.graph import StateGraph, START, END, MessagesState\n  from langgraph.prebuilt import ToolNode, tools_condition\n  from langchain.tools import tool\n  from langchain.agents import create_agent\n\nREQUIRED LOADER:\n  loader = DirectoryLoader(docs_dir, glob=\"**/*.md\", loader_cls=TextLoader, recursive=True, silent_errors=True)\n  docs = loader.load()\n\nREQUIRED EMBEDDINGS:\n  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n  probe_vec = embedding_model.embed_query(\"probe\")\n\nCHUNKING METHODS (pick one per candidate):\n  - RecursiveCharacterTextSplitter\n  - CharacterTextSplitter\n  - SemanticChunker (use try/except fallback if unavailable)\n  - Document-based (no splitting)\n  - CharacterTextSplitter with custom separators\n\nSEMANTIC CHUNKING (SAFE PATTERN):\n  try:\n      splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n      splits = splitter.split_documents(docs)\n  except Exception:\n      splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n      splits = splitter.split_documents(docs)\n\nFIXED-CHARACTER CHUNKING (CUSTOM SEPARATORS):\n  splitter = CharacterTextSplitter(\n      chunk_size=600,\n      chunk_overlap=100,\n      separator=\"\\\\n\\\\n\",\n  )\n  splits = splitter.split_documents(docs)\n\nHYBRID RETRIEVAL (BM25 + VECTOR, SAFE PATTERN):\n  bm25 = BM25Retriever.from_documents(docs)\n  bm25.k = 4\n  vector_store = InMemoryVectorStore(embedding_model)\n  vector_store.add_documents(splits)\n  vector_docs = vector_store.similarity_search(query, k=4)\n  bm25_docs = bm25.invoke(query)\n  # simple merge by content to dedupe\n  merged = {}\n  for d in vector_docs + bm25_docs:\n      merged[d.page_content] = d\n  retrieved = list(merged.values())[:6]\n\nLANGGRAPH TEMPLATE (RETRIEVE -> GENERATE):\n  def retrieve(state: MessagesState):\n      user_text = state[\"messages\"][-1].content\n      retrieved = vector_store.similarity_search(user_text, k=4)\n      context = \"\\n\\n\".join(d.page_content for d in retrieved)\n      augmented = f\"{user_text}\\n\\nCONTEXT:\\n{context}\"\n      return {\"messages\": [HumanMessage(content=augmented)]}\n\n  def generate(state: MessagesState):\n      response = llm.invoke(state[\"messages\"])\n      return {\"messages\": [response]}\n\n  graph = StateGraph(MessagesState)\n  graph.add_node(\"retrieve\", retrieve)\n  graph.add_node(\"generate\", generate)\n  graph.add_edge(START, \"retrieve\")\n  graph.add_edge(\"retrieve\", \"generate\")\n  graph.add_edge(\"generate\", END)\n\nLANGGRAPH AGENTIC TEMPLATE (TOOL ROUTING):\n  @tool\n  def retrieve_context(query: str) -> str:\n      retrieved = vector_store.similarity_search(query, k=4)\n      return \"\\n\\n\".join(d.page_content for d in retrieved)\n\n  def generate_query_or_respond(state: MessagesState):\n      response = llm.bind_tools([retrieve_context]).invoke(state[\"messages\"])\n      return {\"messages\": [response]}\n\n  workflow = StateGraph(MessagesState)\n  workflow.add_node(generate_query_or_respond)\n  workflow.add_node(\"retrieve\", ToolNode([retrieve_context]))\n  workflow.add_node(\"generate\", generate)\n  workflow.add_edge(START, \"generate_query_or_respond\")\n  workflow.add_conditional_edges(\n      \"generate_query_or_respond\",\n      tools_condition,\n      {\"tools\": \"retrieve\", END: END},\n  )\n  workflow.add_edge(\"retrieve\", \"generate\")\n  workflow.add_edge(\"generate\", END)\n", "user": "# Current Program Information\n- Fitness: 0.8322\n- Feature coordinates: complexity=36.90, framework_id=2.00\n- Focus areas: - Fitness declined: 0.9226 \u2192 0.8322. Consider revising recent changes.\n- Exploring complexity=36.90, framework_id=2.00 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.9226, metrics: {'avg_score': 0.8249891774891774, 'time_penalty': 0.052411045730113986, 'avg_latency': 10.241104573011398, 'framework_id': 2.0, 'complexity': 36.6, 'diversity': 2.0, 'performance': 0.9225781317590634}, framework_id: 2.0000, complexity: 36.6000, artifacts: {'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by nodes 1 and 1 respectively, indicating some relevance, while sentences 2 and 4 are unsupported as they are not directly referenced in the retrieval context, leading to a moderate overall alignment.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output is a code snippet unrelated to the retrieval context nodes, which are not referenced in the output, indicating no alignment.\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the original expected output contains code and setup details that are supported by the context mentioning 'MCPToolset' and its usage, indicating relevance to the retrieval nodes, while some parts are just comments and setup instructions not directly supported by the nodes.\\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because all sentences in the expected output are about code setup and execution, which are not supported by any nodes in the retrieval context, indicating no relevant information was recalled.\"}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9226)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.9226), Performs well on metrics ({'avg_score': 0.8249891774891774, 'time_penalty': 0.052411045730113986, 'avg_latency': 10.241104573011398, 'framework_id': 2.0, 'complexity': 36.6, 'diversity': 2.0, 'performance': 0.9225781317590634}), Performs well on framework_id (2.0000), Performs well on complexity (36.6000), Performs well on artifacts ({'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by nodes 1 and 1 respectively, indicating some relevance, while sentences 2 and 4 are unsupported as they are not directly referenced in the retrieval context, leading to a moderate overall alignment.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output is a code snippet unrelated to the retrieval context nodes, which are not referenced in the output, indicating no alignment.\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the original expected output contains code and setup details that are supported by the context mentioning 'MCPToolset' and its usage, indicating relevance to the retrieval nodes, while some parts are just comments and setup instructions not directly supported by the nodes.\\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because all sentences in the expected output are about code setup and execution, which are not supported by any nodes in the retrieval context, indicating no relevant information was recalled.\"})\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.5000)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100, separator=\"\\n\\n\")\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = [\n            HumanMessage(content=(\n                \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n                f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n            ))\n        ]\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D2 (Score: 0.8322)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        # Switch to fixed-size character chunking to vary chunking method\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        # Retrieve top 4 docs from vector store\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        # Concatenate context from retrieved docs\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        # Compose prompt with clear instructions\n        prompt = (\n            \"You are a helpful assistant answering questions based on the provided context. \"\n            \"If the answer is not contained in the context, say you don't know.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        # Invoke LLM with prompt\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D3 (Score: 0.8601)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        # Switch chunking to fixed-size CharacterTextSplitter with smaller chunk and overlap\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer the question concisely.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        # Use HumanMessage to send prompt as message\n        from langchain_core.messages import HumanMessage\n        messages = [HumanMessage(content=prompt)]\n        response = self.llm.invoke(messages)\n        answer = response.content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.9226, Type: High-Performer)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Excellent combined_score (0.923)\n\n# Current Program\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        # Switch to fixed-size character chunking to vary chunking method\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        # Retrieve top 4 docs from vector store\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        # Concatenate context from retrieved docs\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        # Compose prompt with clear instructions\n        prompt = (\n            \"You are a helpful assistant answering questions based on the provided context. \"\n            \"If the answer is not contained in the context, say you don't know.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        # Invoke LLM with prompt\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity, framework_id\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        # Switch to fixed-size character chunking to vary chunking method\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        # Retrieve top 4 docs from vector store\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        # Concatenate context from retrieved docs\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        # Compose prompt with clear instructions\n        prompt = (\n            \"You are a helpful assistant answering questions based on the provided context. \"\n            \"If the answer is not contained in the context, say you don't know.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        # Invoke LLM with prompt\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n=======\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import SemanticChunker, RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import StateGraph, START, END, MessagesState\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self.vector_store = None\n        self.graph = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n\n        try:\n            splitter = SemanticChunker(embeddings)\n            splits = splitter.split_documents(docs)\n        except Exception:\n            # Fallback to recursive splitter on failure\n            splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n            splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        def retrieve(state: MessagesState):\n            user_text = state[\"messages\"][-1].content\n            retrieved = self.vector_store.similarity_search(user_text, k=4)\n            context = \"\\n\\n\".join(d.page_content for d in retrieved)\n            augmented = f\"{user_text}\\n\\nCONTEXT:\\n{context}\"\n            return {\"messages\": [HumanMessage(content=augmented)]}\n\n        def generate(state: MessagesState):\n            response = self.llm.invoke(state[\"messages\"])\n            return {\"messages\": [response]}\n\n        self.graph = StateGraph(MessagesState)\n        self.graph.add_node(\"retrieve\", retrieve)\n        self.graph.add_node(\"generate\", generate)\n        self.graph.add_edge(START, \"retrieve\")\n        self.graph.add_edge(\"retrieve\", \"generate\")\n        self.graph.add_edge(\"generate\", END)\n\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        # Run LangGraph state graph workflow retrieve->generate\n        state = MessagesState(messages=[HumanMessage(content=query_str)])\n        final_state = self.graph.run(state)\n        answer = final_state[\"messages\"][-1].content.strip()\n        # Also return contexts from retrieval step for debugging\n        # We replicate retrieval to get contexts\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}