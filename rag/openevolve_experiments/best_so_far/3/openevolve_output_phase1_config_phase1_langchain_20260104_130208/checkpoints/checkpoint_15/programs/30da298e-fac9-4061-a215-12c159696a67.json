{"id": "30da298e-fac9-4061-a215-12c159696a67", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\nfrom llama_index.core.tools import QueryEngineTool\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index1 = None\n        self.index2 = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 256\n        Settings.chunk_overlap = 64\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n\n        # Use sentence splitter for chunking\n        splitter = SentenceSplitter(chunk_size=256, chunk_overlap=64)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        # Create two indexes with different chunk sizes for tool routing\n        self.index1 = VectorStoreIndex(nodes)\n\n        # For the second index, chunk documents differently\n        splitter2 = SentenceSplitter(chunk_size=128, chunk_overlap=32)\n        nodes2 = splitter2.get_nodes_from_documents(documents)\n        self.index2 = VectorStoreIndex(nodes2)\n\n        tool1 = QueryEngineTool.from_defaults(\n            query_engine=self.index1.as_query_engine(similarity_top_k=3),\n            description=\"General documentation queries.\",\n        )\n        tool2 = QueryEngineTool.from_defaults(\n            query_engine=self.index2.as_query_engine(similarity_top_k=6),\n            description=\"Broader context queries.\",\n        )\n\n        self.query_engine = SubQuestionQueryEngine.from_defaults(\n            query_engine_tools=[tool1, tool2]\n        )\n\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "82c5d2f0-d5f9-4685-a226-b4bdd6738357", "generation": 2, "timestamp": 1767455697.9351149, "iteration_found": 12, "metrics": {"error": 0.0, "timeout": true}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 55 lines with 67 lines", "parent_metrics": {"combined_score": 0.763305855431254, "metrics": {"avg_score": 0.8175099206349207, "time_penalty": 0.05420406520366669, "avg_latency": 10.420406520366669, "framework_id": 0.0, "complexity": 40.0, "diversity": 0.0, "performance": 0.763305855431254}, "framework_id": 0.0, "complexity": 40.0, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low answer_relevancy (0.62): The score is 0.62 because some parts of the output, such as explanations about expected function outputs and unrelated import statements, are irrelevant to the actual code definitions. However, the core code for 'get_current_time' and 'root_agent' is present, which justifies the partial relevance and prevents a lower score.\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.12): The score is 0.12 because the only sentence in the expected output that directly relates to the retrieval context is the first, which mentions 'Install ADK by running the following command', aligning with node 1 in the context. The remaining sentences are not supported by the context, leading to a low recall score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.62): The score is 0.62 because the response included irrelevant details about weather reports instead of providing the Python example of the Function Tool with 'city' and 'unit' parameters as requested. However, it partially addressed the request, which is why the score isn't lower.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.38): The score is 0.38 because the output contains several irrelevant statements about importing modules and setting properties that do not directly demonstrate registering a before_model callback, which limits its relevance despite some related context.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the context mentions 'os.path.abspath(TARGET_FOLDER_PATH)' used in args, indicating the path is relevant, which supports sentence 13 in the expected output. However, the context lacks specific sentences that directly correspond to other parts of the output, leading to a moderate confidence in attribution."}}, "island": 2}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}