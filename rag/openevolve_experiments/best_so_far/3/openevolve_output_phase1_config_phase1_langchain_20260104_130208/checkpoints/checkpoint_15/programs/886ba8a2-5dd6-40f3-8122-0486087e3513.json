{"id": "886ba8a2-5dd6-40f3-8122-0486087e3513", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        try:\n            splitter = SemanticChunker(embeddings)\n            splits = splitter.split_documents(docs)\n        except Exception:\n            splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n            splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context below to answer the question.\\n\"\n            \"If the answer is not found in the context, reply that you don't know.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "fa83d0b8-73f8-431d-9e17-c253bdc919ea", "generation": 1, "timestamp": 1767529213.3191602, "iteration_found": 5, "metrics": {"combined_score": 0.7059383425562701, "metrics": {"avg_score": 0.5615372512190661, "time_penalty": 0.005598908662796021, "avg_latency": 5.559890866279602, "framework_id": 2.0, "complexity": 39.3, "diversity": 2.0, "performance": 0.7059383425562701}, "framework_id": 2.0, "complexity": 39.3, "artifacts": {"critique": "Framework: LangChain\nExecution Error on query 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.': Function timed out after 120 seconds\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.50): The score is 0.50 because the code snippet's function and docstring are somewhat aligned with the retrieval context, but the extensive unrelated documentation reduces confidence in direct attribution.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.07): The score is 0.07 because the actual output introduces a 'before_model' callback in an 'LlmAgent', which the documentation does not specify; it also imports modules and defines constants not mentioned, and describes a callback function with parameters and behaviors (like inspecting messages, printing, modifying instructions, and blocking messages) that are not detailed in the documentation, indicating significant deviations from the provided context.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.27): The score is 0.27 because the output includes detailed callback implementation steps rather than focusing solely on the registration of the 'before_model' callback, making it somewhat relevant but not directly aligned with the specific request.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low contextual_recall (0.25): The score is 0.25 because the first sentence in the expected output directly references the 'expected output' and discusses attributing sentences to retrieval context nodes, aligning with the retrieval context. The other sentences are either instructions or the retrieval context itself, which do not support attribution, leading to a low score."}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace from langchain_text_splitters import RecursiveCharacterTextSplitter with 2 lines\nChange 2: Replace 5 lines with 9 lines\nChange 3: Replace 7 lines with 8 lines", "parent_metrics": {"combined_score": 0.9225781317590634, "metrics": {"avg_score": 0.8249891774891774, "time_penalty": 0.052411045730113986, "avg_latency": 10.241104573011398, "framework_id": 2.0, "complexity": 36.6, "diversity": 2.0, "performance": 0.9225781317590634}, "framework_id": 2.0, "complexity": 36.6, "artifacts": {"critique": "Framework: LangChain\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by nodes 1 and 1 respectively, indicating some relevance, while sentences 2 and 4 are unsupported as they are not directly referenced in the retrieval context, leading to a moderate overall alignment.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output is a code snippet unrelated to the retrieval context nodes, which are not referenced in the output, indicating no alignment.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the original expected output contains code and setup details that are supported by the context mentioning 'MCPToolset' and its usage, indicating relevance to the retrieval nodes, while some parts are just comments and setup instructions not directly supported by the nodes.\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because all sentences in the expected output are about code setup and execution, which are not supported by any nodes in the retrieval context, indicating no relevant information was recalled."}}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert Python engineer specialized in LangChain/LangGraph RAG.\nYour job is to EVOLVE a WORKING, RUNNABLE `RAGSystem` for querying a Markdown docs directory.\n\n========================\nCRITICAL CONSTRAINTS\n========================\n1) Output ONLY valid Python code (no markdown fences).\n2) The code must be self-contained within the EVOLVE-BLOCK (imports + class RAGSystem).\n3) Use gpt-4.1-mini inside the RAG code.\n4) LangChain/LangGraph only. Do NOT use LlamaIndex or Agno.\n5) Any symbol you reference MUST be imported in the EVOLVE block using allowed imports.\n\n========================\nMUTATION STRATEGY\n========================\nAlternate between these two architectures across candidates:\n  A) Manual retrieval + single LLM call\n  B) LangGraph retrieve -> generate workflow\nIf you stay with the same architecture, you must change chunking method.\nChunking must rotate across these five styles over the run:\n  1) Fixed-size CharacterTextSplitter\n  2) RecursiveCharacterTextSplitter\n  3) SemanticChunker (with fallback)\n  4) Document-based (no splitting)\n  5) Fixed-size CharacterTextSplitter with different separator config\nEnforce LangGraph usage: every second candidate MUST use StateGraph.\n\n========================\nALLOWED IMPORTS\n========================\n  from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n  from langchain_community.document_loaders import DirectoryLoader, TextLoader\n  from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n  from langchain_experimental.text_splitter import SemanticChunker\n  from langchain_community.retrievers import BM25Retriever\n  from langchain_core.vectorstores import InMemoryVectorStore\n  from langchain_core.messages import HumanMessage\n  from langgraph.graph import StateGraph, START, END, MessagesState\n  from langgraph.prebuilt import ToolNode, tools_condition\n  from langchain.tools import tool\n  from langchain.agents import create_agent\n\nREQUIRED LOADER:\n  loader = DirectoryLoader(docs_dir, glob=\"**/*.md\", loader_cls=TextLoader, recursive=True, silent_errors=True)\n  docs = loader.load()\n\nREQUIRED EMBEDDINGS:\n  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n  probe_vec = embedding_model.embed_query(\"probe\")\n\nCHUNKING METHODS (pick one per candidate):\n  - RecursiveCharacterTextSplitter\n  - CharacterTextSplitter\n  - SemanticChunker (use try/except fallback if unavailable)\n  - Document-based (no splitting)\n  - CharacterTextSplitter with custom separators\n\nSEMANTIC CHUNKING (SAFE PATTERN):\n  try:\n      splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n      splits = splitter.split_documents(docs)\n  except Exception:\n      splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n      splits = splitter.split_documents(docs)\n\nFIXED-CHARACTER CHUNKING (CUSTOM SEPARATORS):\n  splitter = CharacterTextSplitter(\n      chunk_size=600,\n      chunk_overlap=100,\n      separator=\"\\\\n\\\\n\",\n  )\n  splits = splitter.split_documents(docs)\n\nHYBRID RETRIEVAL (BM25 + VECTOR, SAFE PATTERN):\n  bm25 = BM25Retriever.from_documents(docs)\n  bm25.k = 4\n  vector_store = InMemoryVectorStore(embedding_model)\n  vector_store.add_documents(splits)\n  vector_docs = vector_store.similarity_search(query, k=4)\n  bm25_docs = bm25.invoke(query)\n  # simple merge by content to dedupe\n  merged = {}\n  for d in vector_docs + bm25_docs:\n      merged[d.page_content] = d\n  retrieved = list(merged.values())[:6]\n\nLANGGRAPH TEMPLATE (RETRIEVE -> GENERATE):\n  def retrieve(state: MessagesState):\n      user_text = state[\"messages\"][-1].content\n      retrieved = vector_store.similarity_search(user_text, k=4)\n      context = \"\\n\\n\".join(d.page_content for d in retrieved)\n      augmented = f\"{user_text}\\n\\nCONTEXT:\\n{context}\"\n      return {\"messages\": [HumanMessage(content=augmented)]}\n\n  def generate(state: MessagesState):\n      response = llm.invoke(state[\"messages\"])\n      return {\"messages\": [response]}\n\n  graph = StateGraph(MessagesState)\n  graph.add_node(\"retrieve\", retrieve)\n  graph.add_node(\"generate\", generate)\n  graph.add_edge(START, \"retrieve\")\n  graph.add_edge(\"retrieve\", \"generate\")\n  graph.add_edge(\"generate\", END)\n\nLANGGRAPH AGENTIC TEMPLATE (TOOL ROUTING):\n  @tool\n  def retrieve_context(query: str) -> str:\n      retrieved = vector_store.similarity_search(query, k=4)\n      return \"\\n\\n\".join(d.page_content for d in retrieved)\n\n  def generate_query_or_respond(state: MessagesState):\n      response = llm.bind_tools([retrieve_context]).invoke(state[\"messages\"])\n      return {\"messages\": [response]}\n\n  workflow = StateGraph(MessagesState)\n  workflow.add_node(generate_query_or_respond)\n  workflow.add_node(\"retrieve\", ToolNode([retrieve_context]))\n  workflow.add_node(\"generate\", generate)\n  workflow.add_edge(START, \"generate_query_or_respond\")\n  workflow.add_conditional_edges(\n      \"generate_query_or_respond\",\n      tools_condition,\n      {\"tools\": \"retrieve\", END: END},\n  )\n  workflow.add_edge(\"retrieve\", \"generate\")\n  workflow.add_edge(\"generate\", END)\n", "user": "# Current Program Information\n- Fitness: 0.9226\n- Feature coordinates: complexity=36.60, framework_id=2.00\n- Focus areas: - Fitness unchanged at 0.9226\n- Exploring complexity=36.60, framework_id=2.00 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.9226, metrics: {'avg_score': 0.8249891774891774, 'time_penalty': 0.052411045730113986, 'avg_latency': 10.241104573011398, 'framework_id': 2.0, 'complexity': 36.6, 'diversity': 2.0, 'performance': 0.9225781317590634}, framework_id: 2.0000, complexity: 36.6000, artifacts: {'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by nodes 1 and 1 respectively, indicating some relevance, while sentences 2 and 4 are unsupported as they are not directly referenced in the retrieval context, leading to a moderate overall alignment.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output is a code snippet unrelated to the retrieval context nodes, which are not referenced in the output, indicating no alignment.\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the original expected output contains code and setup details that are supported by the context mentioning 'MCPToolset' and its usage, indicating relevance to the retrieval nodes, while some parts are just comments and setup instructions not directly supported by the nodes.\\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because all sentences in the expected output are about code setup and execution, which are not supported by any nodes in the retrieval context, indicating no relevant information was recalled.\"}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9226)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.9226), Performs well on metrics ({'avg_score': 0.8249891774891774, 'time_penalty': 0.052411045730113986, 'avg_latency': 10.241104573011398, 'framework_id': 2.0, 'complexity': 36.6, 'diversity': 2.0, 'performance': 0.9225781317590634}), Performs well on framework_id (2.0000), Performs well on complexity (36.6000), Performs well on artifacts ({'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by nodes 1 and 1 respectively, indicating some relevance, while sentences 2 and 4 are unsupported as they are not directly referenced in the retrieval context, leading to a moderate overall alignment.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output is a code snippet unrelated to the retrieval context nodes, which are not referenced in the output, indicating no alignment.\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the original expected output contains code and setup details that are supported by the context mentioning 'MCPToolset' and its usage, indicating relevance to the retrieval nodes, while some parts are just comments and setup instructions not directly supported by the nodes.\\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because all sentences in the expected output are about code setup and execution, which are not supported by any nodes in the retrieval context, indicating no relevant information was recalled.\"})\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.8601)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        # Switch chunking to fixed-size CharacterTextSplitter with smaller chunk and overlap\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer the question concisely.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        # Use HumanMessage to send prompt as message\n        from langchain_core.messages import HumanMessage\n        messages = [HumanMessage(content=prompt)]\n        response = self.llm.invoke(messages)\n        answer = response.content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D2 (Score: 0.8322)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        # Switch to fixed-size character chunking to vary chunking method\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        # Retrieve top 4 docs from vector store\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        # Concatenate context from retrieved docs\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        # Compose prompt with clear instructions\n        prompt = (\n            \"You are a helpful assistant answering questions based on the provided context. \"\n            \"If the answer is not contained in the context, say you don't know.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        # Invoke LLM with prompt\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.8322, Type: High-Performer)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        # Switch to fixed-size character chunking to vary chunking method\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        # Retrieve top 4 docs from vector store\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        # Concatenate context from retrieved docs\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        # Compose prompt with clear instructions\n        prompt = (\n            \"You are a helpful assistant answering questions based on the provided context. \"\n            \"If the answer is not contained in the context, say you don't know.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        # Invoke LLM with prompt\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Excellent framework_id (2.000)\n\n# Current Program\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity, framework_id\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n=======\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n=======\n        try:\n            splitter = SemanticChunker(embeddings)\n            splits = splitter.split_documents(docs)\n        except Exception:\n            splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n            splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n=======\n        prompt = (\n            \"You are a helpful assistant. Use the context below to answer the question.\\n\"\n            \"If the answer is not found in the context, reply that you don't know.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}