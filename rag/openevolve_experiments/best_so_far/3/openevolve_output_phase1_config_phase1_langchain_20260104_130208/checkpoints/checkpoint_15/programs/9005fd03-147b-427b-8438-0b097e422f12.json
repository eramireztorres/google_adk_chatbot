{"id": "9005fd03-147b-427b-8438-0b097e422f12", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        try:\n            splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n            splits = splitter.split_documents(docs)\n        except Exception:\n            splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n            splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Answer the question concisely using the provided context. \"\n            \"If the answer is not contained in the context, respond with: 'I don't know.'\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\\nANSWER:\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "ae181325-2ac9-4880-bebf-121a66a821f1", "generation": 2, "timestamp": 1767530786.6810305, "iteration_found": 14, "metrics": {"combined_score": 0.6868993506493507, "metrics": {"avg_score": 0.5368993506493507, "time_penalty": 0.0, "avg_latency": 4.687031507492065, "framework_id": 2.0, "complexity": 39.6, "diversity": 2.0, "performance": 0.6868993506493507}, "framework_id": 2.0, "complexity": 39.6, "artifacts": {"critique": "Framework: LangChain\nExecution Error on query 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.': Function timed out after 120 seconds\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.57): The score is 0.57 because the output includes irrelevant details about function implementation and placeholders, which are not part of the required parameters. However, it contains some relevant information about the function's parameters, preventing a lower score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output's sentences are general code descriptions and do not match any specific nodes in the retrieval context, which only contains unsupportive reasons indicating no direct relation.\nMetric faithfulness failed: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=32768, prompt_tokens=1157, total_tokens=33925, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low contextual_recall (0.40): The score is 0.40 because the supportive reasons (sentences 1 and 2) relate to the task of attributing sentences to retrieval context nodes, aligning with the retrieval context content, while the unsupportive reasons (sentences 3 to 5) involve code snippets and descriptions not directly linked to the retrieval context, leading to a moderate confidence in the attribution."}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace from langchain_text_splitters import CharacterTextSplitter with 2 lines\nChange 2: Replace 6 lines with 9 lines\nChange 3: Replace 9 lines with 8 lines", "parent_metrics": {"combined_score": 0.8197069703870351, "metrics": {"avg_score": 0.697013888888889, "time_penalty": 0.027306918501853943, "avg_latency": 7.730691850185394, "framework_id": 2.0, "complexity": 36.9, "diversity": 2.0, "performance": 0.8197069703870351}, "framework_id": 2.0, "complexity": 36.9, "artifacts": {"critique": "Framework: LangChain\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low answer_relevancy (0.62): The score is 0.62 because some irrelevant statements about unrelated code components and configurations were included, which do not directly address the request for the specific Python code definitions. However, the response contains some relevant information, preventing the score from being lower.\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.25): The score is 0.25 because only the first sentence directly relates to the node 'root_agent' in retrieval context, while the remaining sentences are general instructions not supported by specific nodes, indicating limited overlap.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.25): The score is 0.25 because only the first sentence in the expected output directly relates to node 1 in the retrieval context, while the other sentences mention unrelated concepts and are not supported by the context.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low faithfulness (0.00): The score is 0.00 because the claims describe a Python code snippet involving MCPToolset and specific import statements, whereas the retrieval context discusses Java examples and general usage without these details, indicating a complete mismatch.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low answer_relevancy (0.67): The score is 0.67 because some parts of the output included comments about code and agent interaction, which are not directly relevant to the requested Python sample code connecting to the MCP server. However, the core code was present, so the score isn't lower."}}, "island": 1}, "prompts": {"diff_user": {"system": "You are an expert Python engineer specialized in LangChain/LangGraph RAG.\nYour job is to EVOLVE a WORKING, RUNNABLE `RAGSystem` for querying a Markdown docs directory.\n\n========================\nCRITICAL CONSTRAINTS\n========================\n1) Output ONLY valid Python code (no markdown fences).\n2) The code must be self-contained within the EVOLVE-BLOCK (imports + class RAGSystem).\n3) Use gpt-4.1-mini inside the RAG code.\n4) LangChain/LangGraph only. Do NOT use LlamaIndex or Agno.\n5) Any symbol you reference MUST be imported in the EVOLVE block using allowed imports.\n\n========================\nMUTATION STRATEGY\n========================\nAlternate between these two architectures across candidates:\n  A) Manual retrieval + single LLM call\n  B) LangGraph retrieve -> generate workflow\nIf you stay with the same architecture, you must change chunking method.\nChunking must rotate across these five styles over the run:\n  1) Fixed-size CharacterTextSplitter\n  2) RecursiveCharacterTextSplitter\n  3) SemanticChunker (with fallback)\n  4) Document-based (no splitting)\n  5) Fixed-size CharacterTextSplitter with different separator config\nEnforce LangGraph usage: every second candidate MUST use StateGraph.\n\n========================\nALLOWED IMPORTS\n========================\n  from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n  from langchain_community.document_loaders import DirectoryLoader, TextLoader\n  from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n  from langchain_experimental.text_splitter import SemanticChunker\n  from langchain_community.retrievers import BM25Retriever\n  from langchain_core.vectorstores import InMemoryVectorStore\n  from langchain_core.messages import HumanMessage\n  from langgraph.graph import StateGraph, START, END, MessagesState\n  from langgraph.prebuilt import ToolNode, tools_condition\n  from langchain.tools import tool\n  from langchain.agents import create_agent\n\nREQUIRED LOADER:\n  loader = DirectoryLoader(docs_dir, glob=\"**/*.md\", loader_cls=TextLoader, recursive=True, silent_errors=True)\n  docs = loader.load()\n\nREQUIRED EMBEDDINGS:\n  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n  probe_vec = embedding_model.embed_query(\"probe\")\n\nCHUNKING METHODS (pick one per candidate):\n  - RecursiveCharacterTextSplitter\n  - CharacterTextSplitter\n  - SemanticChunker (use try/except fallback if unavailable)\n  - Document-based (no splitting)\n  - CharacterTextSplitter with custom separators\n\nSEMANTIC CHUNKING (SAFE PATTERN):\n  try:\n      splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n      splits = splitter.split_documents(docs)\n  except Exception:\n      splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n      splits = splitter.split_documents(docs)\n\nFIXED-CHARACTER CHUNKING (CUSTOM SEPARATORS):\n  splitter = CharacterTextSplitter(\n      chunk_size=600,\n      chunk_overlap=100,\n      separator=\"\\\\n\\\\n\",\n  )\n  splits = splitter.split_documents(docs)\n\nHYBRID RETRIEVAL (BM25 + VECTOR, SAFE PATTERN):\n  bm25 = BM25Retriever.from_documents(docs)\n  bm25.k = 4\n  vector_store = InMemoryVectorStore(embedding_model)\n  vector_store.add_documents(splits)\n  vector_docs = vector_store.similarity_search(query, k=4)\n  bm25_docs = bm25.invoke(query)\n  # simple merge by content to dedupe\n  merged = {}\n  for d in vector_docs + bm25_docs:\n      merged[d.page_content] = d\n  retrieved = list(merged.values())[:6]\n\nLANGGRAPH TEMPLATE (RETRIEVE -> GENERATE):\n  def retrieve(state: MessagesState):\n      user_text = state[\"messages\"][-1].content\n      retrieved = vector_store.similarity_search(user_text, k=4)\n      context = \"\\n\\n\".join(d.page_content for d in retrieved)\n      augmented = f\"{user_text}\\n\\nCONTEXT:\\n{context}\"\n      return {\"messages\": [HumanMessage(content=augmented)]}\n\n  def generate(state: MessagesState):\n      response = llm.invoke(state[\"messages\"])\n      return {\"messages\": [response]}\n\n  graph = StateGraph(MessagesState)\n  graph.add_node(\"retrieve\", retrieve)\n  graph.add_node(\"generate\", generate)\n  graph.add_edge(START, \"retrieve\")\n  graph.add_edge(\"retrieve\", \"generate\")\n  graph.add_edge(\"generate\", END)\n\nLANGGRAPH AGENTIC TEMPLATE (TOOL ROUTING):\n  @tool\n  def retrieve_context(query: str) -> str:\n      retrieved = vector_store.similarity_search(query, k=4)\n      return \"\\n\\n\".join(d.page_content for d in retrieved)\n\n  def generate_query_or_respond(state: MessagesState):\n      response = llm.bind_tools([retrieve_context]).invoke(state[\"messages\"])\n      return {\"messages\": [response]}\n\n  workflow = StateGraph(MessagesState)\n  workflow.add_node(generate_query_or_respond)\n  workflow.add_node(\"retrieve\", ToolNode([retrieve_context]))\n  workflow.add_node(\"generate\", generate)\n  workflow.add_edge(START, \"generate_query_or_respond\")\n  workflow.add_conditional_edges(\n      \"generate_query_or_respond\",\n      tools_condition,\n      {\"tools\": \"retrieve\", END: END},\n  )\n  workflow.add_edge(\"retrieve\", \"generate\")\n  workflow.add_edge(\"generate\", END)\n", "user": "# Current Program Information\n- Fitness: 0.8197\n- Feature coordinates: complexity=36.90, framework_id=2.00\n- Focus areas: - Fitness declined: 0.9226 \u2192 0.8197. Consider revising recent changes.\n- Exploring complexity=36.90, framework_id=2.00 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.9226, metrics: {'avg_score': 0.8249891774891774, 'time_penalty': 0.052411045730113986, 'avg_latency': 10.241104573011398, 'framework_id': 2.0, 'complexity': 36.6, 'diversity': 2.0, 'performance': 0.9225781317590634}, framework_id: 2.0000, complexity: 36.6000, artifacts: {'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by nodes 1 and 1 respectively, indicating some relevance, while sentences 2 and 4 are unsupported as they are not directly referenced in the retrieval context, leading to a moderate overall alignment.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output is a code snippet unrelated to the retrieval context nodes, which are not referenced in the output, indicating no alignment.\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the original expected output contains code and setup details that are supported by the context mentioning 'MCPToolset' and its usage, indicating relevance to the retrieval nodes, while some parts are just comments and setup instructions not directly supported by the nodes.\\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because all sentences in the expected output are about code setup and execution, which are not supported by any nodes in the retrieval context, indicating no relevant information was recalled.\"}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9226)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.9226), Performs well on metrics ({'avg_score': 0.8249891774891774, 'time_penalty': 0.052411045730113986, 'avg_latency': 10.241104573011398, 'framework_id': 2.0, 'complexity': 36.6, 'diversity': 2.0, 'performance': 0.9225781317590634}), Performs well on framework_id (2.0000), Performs well on complexity (36.6000), Performs well on artifacts ({'critique': \"Framework: LangChain\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentences 1 and 3 are supported by nodes 1 and 1 respectively, indicating some relevance, while sentences 2 and 4 are unsupported as they are not directly referenced in the retrieval context, leading to a moderate overall alignment.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output is a code snippet unrelated to the retrieval context nodes, which are not referenced in the output, indicating no alignment.\\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.50): The score is 0.50 because the original expected output contains code and setup details that are supported by the context mentioning 'MCPToolset' and its usage, indicating relevance to the retrieval nodes, while some parts are just comments and setup instructions not directly supported by the nodes.\\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low contextual_recall (0.00): The score is 0.00 because all sentences in the expected output are about code setup and execution, which are not supported by any nodes in the retrieval context, indicating no relevant information was recalled.\"})\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.8197)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        # Switch chunking to fixed-size CharacterTextSplitter with smaller chunk and overlap\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        # Improved prompt with explicit instructions and answer format\n        prompt = (\n            \"You are a helpful assistant. Answer the question using the provided context.\\n\"\n            \"If the answer is not contained in the context, say 'I don't know.'\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\\nANSWER:\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.9226, Type: Migrant)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        prompt = (\n            \"You are a helpful assistant. Use the context to answer.\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\"\n        )\n        answer = self.llm.invoke(prompt).content\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Excellent combined_score (0.923)\n\n# Current Program\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.vector_store = None\n        self.llm = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        try:\n            probe_vec = embeddings.embed_query(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        loader = DirectoryLoader(\n            self.docs_dir,\n            glob=\"**/*.md\",\n            loader_cls=TextLoader,\n            recursive=True,\n            silent_errors=True,\n        )\n        docs = loader.load()\n        # Switch chunking to fixed-size CharacterTextSplitter with smaller chunk and overlap\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n\n        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        retrieved = self.vector_store.similarity_search(query_str, k=4)\n        context = \"\\n\\n\".join(d.page_content for d in retrieved)\n        # Improved prompt with explicit instructions and answer format\n        prompt = (\n            \"You are a helpful assistant. Answer the question using the provided context.\\n\"\n            \"If the answer is not contained in the context, say 'I don't know.'\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\\nANSWER:\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity, framework_id\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom langchain_text_splitters import CharacterTextSplitter\n=======\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Switch chunking to fixed-size CharacterTextSplitter with smaller chunk and overlap\n        splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n        splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n=======\n        try:\n            splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n            splits = splitter.split_documents(docs)\n        except Exception:\n            splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n            splits = splitter.split_documents(docs)\n\n        self.vector_store = InMemoryVectorStore(embeddings)\n        self.vector_store.add_documents(splits)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Improved prompt with explicit instructions and answer format\n        prompt = (\n            \"You are a helpful assistant. Answer the question using the provided context.\\n\"\n            \"If the answer is not contained in the context, say 'I don't know.'\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\\nANSWER:\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n=======\n        prompt = (\n            \"You are a helpful assistant. Answer the question concisely using the provided context. \"\n            \"If the answer is not contained in the context, respond with: 'I don't know.'\\n\\n\"\n            f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{query_str}\\n\\nANSWER:\"\n        )\n        answer = self.llm.invoke(prompt).content.strip()\n        contexts = [d.page_content for d in retrieved]\n        return {\"answer\": answer, \"contexts\": contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}