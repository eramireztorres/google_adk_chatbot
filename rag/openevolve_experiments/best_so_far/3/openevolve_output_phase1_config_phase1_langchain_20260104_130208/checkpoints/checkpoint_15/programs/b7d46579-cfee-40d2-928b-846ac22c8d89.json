{"id": "b7d46579-cfee-40d2-928b-846ac22c8d89", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 400\n        Settings.chunk_overlap = 80\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=400, chunk_overlap=80)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=6)\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": null, "generation": 0, "timestamp": 1767452324.7908015, "iteration_found": 0, "metrics": {"combined_score": 0.548016796331385, "metrics": {"avg_score": 0.6179978354978355, "time_penalty": 0.0699810391664505, "avg_latency": 11.99810391664505, "framework_id": 0.0, "complexity": 37.4, "diversity": 0.0, "performance": 0.548016796331385}, "framework_id": 0.0, "complexity": 37.4, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low answer_relevancy (0.50): The score is 0.50 because some irrelevant statements about importing modules and agent configuration are included, which do not directly address defining the get_current_time tool and root_agent. However, the core code is present, so the score isn't lower.\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.20): The score is 0.20 because the expected output's code and description align with node 1 in the retrieval context, which involves defining tools and including a 'get_current_time' tool, but the rest of the output contains unrelated details, leading to a low relevance score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.40): The score is 0.40 because the output contains multiple irrelevant statements about docstrings, placeholders, and return values that do not directly address providing a Python example of the Function Tool with 'city' and 'unit' parameters, thus limiting its relevance.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.00): The score is 0.00 because the actual output references callback functions named 'my_before_model_logic' and 'myBeforeModelLogic', which are not mentioned in the retrieval context that specifies 'onBeforeModel' and 'myBeforeModelLogic'. Additionally, it describes an 'agent' with specific attributes and initialization details not present in the context, leading to contradictions.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.57): The score is 0.57 because the output contains some irrelevant statements about printing messages and function logic that do not directly address registering a before_model callback, but it also includes relevant code snippets, so the score isn't lower."}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"island": 0}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}