{"id": "d84c33c9-a9c2-4636-a779-e0a155688ac8", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom agno.knowledge.embedder.openai import OpenAIEmbedder\nfrom agno.knowledge.knowledge import Knowledge\nfrom agno.knowledge.reader.markdown_reader import MarkdownReader\nfrom agno.vectordb.lancedb import LanceDb, SearchType\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.agent = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n\n        embedder = OpenAIEmbedder(id=\"text-embedding-3-small\")\n        try:\n            probe_vec = embedder.get_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        knowledge = Knowledge(\n            vector_db=LanceDb(\n                table_name=\"adk_docs\",\n                uri=\"tmp/lancedb\",\n                search_type=SearchType.vector,\n                embedder=embedder,\n            ),\n        )\n        knowledge.add_content(\n            path=self.docs_dir,\n            reader=MarkdownReader(),\n            include=[\"**/*.md\"],\n            upsert=True,\n        )\n\n        self.agent = Agent(\n            model=OpenAIChat(id=\"gpt-4.1-mini\"),\n            knowledge=knowledge,\n            search_knowledge=True,\n            markdown=True,\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.agent.run(query_str)\n        answer = getattr(response, \"content\", str(response))\n        contexts = []\n        resp_contexts = getattr(response, \"contexts\", None)\n        if isinstance(resp_contexts, (list, tuple)):\n            contexts = [str(c) for c in resp_contexts]\n        return {\"answer\": answer, \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": null, "generation": 0, "timestamp": 1767474457.9190123, "iteration_found": 0, "metrics": {"combined_score": 0.610442159175873, "metrics": {"avg_score": 0.5050000000000001, "time_penalty": 0.0445578408241272, "avg_latency": 9.45578408241272, "framework_id": 1.0, "complexity": 34.6, "diversity": 1.0, "performance": 0.610442159175873}, "framework_id": 1.0, "complexity": 34.6, "artifacts": {"critique": "Framework: Agno\nExecution Error on query 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.': Function timed out after 120 seconds\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the retrieval context is empty, making it impossible to attribute any sentences from the expected output to the nodes in the context.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low contextual_recall (0.00): The score is 0.00 because the retrieval context is empty, making it impossible to attribute any sentences from the expected output to the nodes in the context.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low contextual_recall (0.00): The score is 0.00 because the retrieval context is empty, making it impossible to attribute any sentences from the expected output to the context, resulting in no supportive reasons.\nQuery: 'Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.' -> Low answer_relevancy (0.20): The score is 0.20 because the provided output contains multiple irrelevant statements about code setup and execution details that do not specifically address the use of 'output_key' to store 'last_greeting' or the execution with 'InMemorySessionService'. However, some parts of the output may touch on related concepts, preventing the score from being zero. Overall, the response is largely off-topic, but not entirely unrelated."}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"island": 0}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}