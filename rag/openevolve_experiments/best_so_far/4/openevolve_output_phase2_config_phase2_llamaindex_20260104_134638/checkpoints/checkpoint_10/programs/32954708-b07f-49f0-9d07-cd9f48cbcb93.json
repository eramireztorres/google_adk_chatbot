{"id": "32954708-b07f-49f0-9d07-cd9f48cbcb93", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        # Increase chunk size and overlap for semantic splitting\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 128\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        try:\n            splitter = SemanticSplitterNodeParser()\n            nodes = splitter.get_nodes_from_documents(documents)\n        except Exception:\n            # Fallback to SentenceSplitter with larger chunk size and overlap\n            from llama_index.core.node_parser import SentenceSplitter\n            splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=5)\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "1705b075-4b81-4f3b-93cb-889479c853ea", "generation": 3, "timestamp": 1767532887.5011275, "iteration_found": 0, "metrics": {"combined_score": 0.6740306216431835, "metrics": {"avg_score": 0.7158458751393535, "time_penalty": 0.041815253496170046, "avg_latency": 9.181525349617004, "framework_id": 0.0, "complexity": 39.6, "diversity": 0.0, "performance": 0.6740306216431835}, "framework_id": 0.0, "complexity": 39.6, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentence 2 about returning the current time in a city is supported by node 1 in the retrieval context, which includes the 'get_current_time' function and the agent's description related to telling time, but the rest of the output is not directly supported by the context, leading to a moderate confidence level.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.50): The score is 0.50 because the first two sentences in the expected output align with the node in retrieval context, describing the function's purpose and arguments, while the remaining sentences are implementation details not supported by the context.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.00): The score is 0.00 because the actual output introduces imports from 'google.adk.agents', defines a function 'my_before_model_logic' with unspecified parameters and return type, includes print statements with 'callback_context.agent_name', and creates an 'LlmAgent' instance with specific parameters\u2014all of which are not mentioned or supported by the retrieval context.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.50): The score is 0.50 because the output contains some relevant information about setting up an agent, but it includes multiple irrelevant details about imports and agent parameters that do not directly address registering a before_model callback, thus limiting the overall relevance.\nMetric faithfulness failed: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=32768, prompt_tokens=1238, total_tokens=34006, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))"}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 53 lines with 59 lines", "parent_metrics": {"combined_score": 0.6082784086458889, "metrics": {"avg_score": 0.6852156177156178, "time_penalty": 0.07693720906972885, "avg_latency": 12.693720906972885, "framework_id": 0.0, "complexity": 37.4, "diversity": 0.0, "performance": 0.6082784086458889}, "framework_id": 0.0, "complexity": 37.4, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output code about a time-telling agent does not relate to the context, which discusses agent project structure and API setup, indicating no relevant connection.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.45): The score is 0.45 because the output contains multiple irrelevant statements about docstrings and descriptions that do not directly address providing a Python example of the Function Tool with specified parameters, thus limiting its relevance.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.67): The score is 0.67 because the first two sentences in the expected output are well supported by the retrieval context, which discusses a weather retrieval function, while the third sentence, about function logic and return statement, is less directly supported, leading to a moderate confidence in the attribution.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.00): The score is 0.00 because the actual output introduces specific details such as 'llm_agent' being assigned 'LlmAgent('.', comments about 'Other agent parameters...', and the function name 'my_before_model_logic', none of which are supported by the retrieval context. The context only mentions 'before_model_callback' as a parameter assigned to a function and that 'my_before_model_logic' is a function, without additional details or comments, leading to contradictions.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low faithfulness (0.00): The score is 0.00 because the actual output contains contradictions such as claiming module imports from 'google.adk.agents' and 'google.adk.tools.mcp_tool.mcp_toolset', which are not supported by the context, and stating that 'PATH_TO_YOUR_MCP_SERVER_SCRIPT' is set to a specific path, whereas the context indicates it's a placeholder to be replaced. These discrepancies show the output is not faithful to the context."}}, "island": 2, "migrant": true}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}