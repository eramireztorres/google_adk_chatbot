{"id": "e2bae59f-086d-4244-b725-8ccc955bcc8a", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 400\n        Settings.chunk_overlap = 80\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=400, chunk_overlap=80)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=6)\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": null, "generation": 0, "timestamp": 1767531129.5326967, "iteration_found": 0, "metrics": {"combined_score": 0.5759668322502477, "metrics": {"avg_score": 0.6372481684981687, "time_penalty": 0.06128133624792099, "avg_latency": 11.128133624792099, "framework_id": 0.0, "complexity": 37.4, "diversity": 0.0, "performance": 0.5759668322502477}, "framework_id": 0.0, "complexity": 37.4, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output includes code defining an agent and a tool, which directly relates to node 1 in the retrieval context, but the rest of the code and explanations do not align with the context, leading to partial support.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.67): The score is 0.67 because while the response includes a relevant Python example of the Function Tool with the specified parameters, it also contains unrelated statements about weather reports and function return values that do not directly address the input request, slightly reducing overall relevance.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the expected output's content about weather reporting does not relate to the context, which only discusses function parameters and signatures, leading to no supportive connection.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.57): The score is 0.57 because the output included some irrelevant details about printing messages and function behavior that do not directly address registering a before_model callback in an LlmAgent, but it also contained some relevant information, preventing a lower score.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low faithfulness (0.00): The score is 0.00 because the actual output fully aligns with the retrieval context, with no contradictions present."}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"island": 0}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}