{"id": "34cf9722-c46b-4793-a159-e54018b4646f", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\nfrom llama_index.core.tools import QueryEngineTool\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 256\n        Settings.chunk_overlap = 32\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=256, chunk_overlap=32)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        index = VectorStoreIndex(nodes)\n        tool1 = QueryEngineTool.from_defaults(\n            query_engine=index.as_query_engine(similarity_top_k=3),\n            description=\"General documentation queries.\",\n        )\n        tool2 = QueryEngineTool.from_defaults(\n            query_engine=index.as_query_engine(similarity_top_k=6),\n            description=\"Broader context queries.\",\n        )\n\n        self.query_engine = SubQuestionQueryEngine.from_defaults(\n            query_engine_tools=[tool1, tool2]\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "4ab381c4-7550-4e03-9a17-1888555891d3", "generation": 3, "timestamp": 1767535452.9072928, "iteration_found": 13, "metrics": {"combined_score": 0.1833333333333333, "metrics": {"avg_score": 0.3833333333333333, "time_penalty": 0.2, "avg_latency": 56.82045206427574, "framework_id": 0.0, "complexity": 38.6, "diversity": 0.0, "performance": 0.1833333333333333}, "framework_id": 0.0, "complexity": 38.6, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low answer_relevancy (0.00): The score is 0.00 because the output contains entirely irrelevant statements about package installation errors, which do not address the request for Python code defining the get_current_time tool and the root_agent.\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.00): The score is 0.00 because the retrieval context is empty, making it impossible to attribute any sentences from the expected output to the nodes in the context.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.00): The score is 0.00 because the actual output contained irrelevant statements about package installation, which do not address the request for a Python example of a Function Tool with specific parameters.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because there is no retrieval context provided, making it impossible to attribute any part of the expected output to the context.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.00): The score is 0.00 because the output contained irrelevant information about package installation and error handling, which do not address how to register a before_model callback in an LlmAgent. This extraneous content reduces relevance, but the score remains at zero due to complete lack of the requested information.", "ingestion_guard": "query='Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.': rag_system_cache is None\nquery='Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.': rag_system_cache is None\nquery='Show the Python example that registers a before_model callback in an LlmAgent.': rag_system_cache is None\nquery='From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.': rag_system_cache is None\nquery='Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.': rag_system_cache is None"}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 49 lines with 55 lines", "parent_metrics": {"combined_score": 0.6214292364319166, "metrics": {"avg_score": 0.7108333333333334, "time_penalty": 0.08940409690141678, "avg_latency": 13.940409690141678, "framework_id": 0.0, "complexity": 37.4, "diversity": 0.0, "performance": 0.6214292364319166}, "framework_id": 0.0, "complexity": 37.4, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.33): The score is 0.33 because the second and third sentences directly relate to the node in retrieval context defining 'get_current_time' and 'root_agent', supporting their attribution. However, the first, fourth, fifth, and sixth sentences are unrelated to the context, leading to a low overall score.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.67): The score is 0.67 because while the response includes a Python example of a Function Tool with the specified parameters, it also contains irrelevant statements about a weather report and a dictionary structure, which do not directly address the request. These extraneous details reduce the overall relevance, preventing a higher score.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.20): The score is 0.20 because the actual output incorrectly states that 'before_model_callback' is set to the string 'my_before_model_logic', whereas the context indicates it is a function reference, not a string, and it mentions a comment '# Other agent parameters...' which is not present in the context, and also incorrectly describes the code snippet ending with a parenthesis and code block, which the context does not specify.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low faithfulness (0.00): The score is 0.00 because the actual output contains contradictions such as claiming the script is at './adk_agent_samples/mcp_client_agent/agent.py' and specific modules are imported, which are not confirmed in the context. Additionally, it states that 'PATH_TO_YOUR_MCP_SERVER_SCRIPT' is set to a specific path, whereas the context indicates this is a placeholder. These discrepancies show the output is not faithful to the context.\nQuery: 'From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.' -> Low answer_relevancy (0.60): The score is 0.60 because the output included irrelevant statements about placeholders and environment variables, which do not directly address providing the Python 'agent.py' sample for connecting to the MCP server. However, the response was somewhat related to the task, preventing a lower score."}}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert Python engineer specialized in LlamaIndex RAG.\nYour job is to EVOLVE a WORKING, RUNNABLE `RAGSystem` for querying a Markdown docs directory.\n\n========================\nCRITICAL CONSTRAINTS\n========================\n1) Output ONLY valid Python code (no markdown fences).\n2) The code must be self-contained within the EVOLVE-BLOCK (imports + class RAGSystem).\n3) Use gpt-4.1-mini inside the RAG code.\n4) LlamaIndex only. Do NOT use LangChain/LangGraph or Agno.\n5) Any symbol you reference MUST be imported in the EVOLVE block using allowed imports.\n\n========================\nMUTATION STRATEGY\n========================\nAlternate between these retrieval architectures across candidates:\n  A) RetrieverQueryEngine + LLMRerank\n  B) SubQuestionQueryEngine + QueryEngineTool\n  C) RouterQueryEngine + QueryEngineTool\nIf you stay with the same architecture, you must change chunking method.\nChunking must rotate across these four styles over the run:\n  1) SentenceSplitter (fixed-size)\n  2) SemanticSplitterNodeParser (with fallback)\n  3) Document-based (VectorStoreIndex.from_documents)\n  4) SentenceSplitter with smaller chunks + larger overlap\nAvoid external vector stores unless dependencies are installed; fall back to VectorStoreIndex.\n\n========================\nALLOWED IMPORTS\n========================\n  from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n  from llama_index.llms.openai import OpenAI\n  from llama_index.embeddings.openai import OpenAIEmbedding\n  from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n  from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step, Context\n  from llama_index.core.schema import NodeWithScore\n  from llama_index.core.query_engine import SubQuestionQueryEngine, RouterQueryEngine, RetrieverQueryEngine\n  from llama_index.core.postprocessor import LLMRerank\n  from llama_index.core.tools import QueryEngineTool\n  from llama_index.core.retrievers import QueryFusionRetriever, VectorIndexRetriever\n  from llama_index.core.indices.keyword_table import KeywordTableIndex\n\nREQUIRED INGESTION:\n  reader = SimpleDirectoryReader(input_dir=docs_dir, recursive=True, required_exts=[\".md\", \".txt\"])\n  documents = reader.load_data()\n\nCHUNKING METHODS (pick one per candidate):\n  - SentenceSplitter\n  - SemanticSplitterNodeParser (with try/except fallback)\n  - Document-based (VectorStoreIndex.from_documents)\n  - SentenceSplitter with different chunk_size/overlap\n\nSENTENCE SPLITTER (ALT SIZES):\n  splitter = SentenceSplitter(chunk_size=256, chunk_overlap=32)\n  nodes = splitter.get_nodes_from_documents(documents)\n  index = VectorStoreIndex(nodes)\n\nSENTENCE SPLITTER (LARGE OVERLAP):\n  splitter = SentenceSplitter(chunk_size=512, chunk_overlap=192)\n  nodes = splitter.get_nodes_from_documents(documents)\n  index = VectorStoreIndex(nodes)\n\nTOOL ROUTING (SAFE PATTERNS):\n  tool1 = QueryEngineTool.from_defaults(\n      query_engine=index.as_query_engine(similarity_top_k=3),\n      description=\"General documentation queries.\",\n  )\n  tool2 = QueryEngineTool.from_defaults(\n      query_engine=index.as_query_engine(similarity_top_k=6),\n      description=\"Broader context queries.\",\n  )\n\n  # SubQuestionQueryEngine\n  query_engine = SubQuestionQueryEngine.from_defaults(\n      query_engine_tools=[tool1, tool2]\n  )\n\n  # RouterQueryEngine\n  query_engine = RouterQueryEngine.from_defaults(\n      query_engine_tools=[tool1, tool2]\n  )\n\nHYBRID RETRIEVAL (VECTOR + KEYWORD, SAFE PATTERN):\n  keyword_index = KeywordTableIndex.from_documents(documents)\n  keyword_retriever = keyword_index.as_retriever(similarity_top_k=3)\n  vector_retriever = VectorIndexRetriever(index, similarity_top_k=3)\n  hybrid_retriever = QueryFusionRetriever(\n      retrievers=[vector_retriever, keyword_retriever],\n      similarity_top_k=4,\n      num_queries=2,\n      use_async=False,\n  )\n  query_engine = RetrieverQueryEngine(hybrid_retriever)\n\nWORKFLOW RAG (SAFE SKELETON):\n  class RetrieverEvent(Event):\n      nodes: list[NodeWithScore]\n\n  class RerankEvent(Event):\n      nodes: list[NodeWithScore]\n\n  class RAGWorkflow(Workflow):\n      @step\n      async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n          dirname = ev.get(\"dirname\")\n          if not dirname:\n              return None\n          docs = SimpleDirectoryReader(dirname).load_data()\n          index = VectorStoreIndex.from_documents(docs)\n          return StopEvent(result=index)\n\n      @step\n      async def retrieve(self, ctx: Context, ev: StartEvent) -> RetrieverEvent | None:\n          query = ev.get(\"query\")\n          index = ev.get(\"index\")\n          if not query or index is None:\n              return None\n          retriever = index.as_retriever(similarity_top_k=2)\n          nodes = await retriever.aretrieve(query)\n          return RetrieverEvent(nodes=nodes)\n\n      @step\n      async def rerank(self, ctx: Context, ev: RetrieverEvent) -> RerankEvent:\n          ranker = LLMRerank(choice_batch_size=5, top_n=3)\n          new_nodes = ranker.postprocess_nodes(ev.nodes, query_str=ev.get(\"query\"))\n          return RerankEvent(nodes=new_nodes)\n\n      @step\n      async def synthesize(self, ctx: Context, ev: RerankEvent) -> StopEvent:\n          response = \" \".join([n.node.get_content() for n in ev.nodes])\n          return StopEvent(result=response)\n", "user": "# Current Program Information\n- Fitness: 0.6214\n- Feature coordinates: complexity=37.40, framework_id=0.00\n- Focus areas: - Fitness declined: 0.6740 \u2192 0.6214. Consider revising recent changes.\n- Exploring complexity=37.40, framework_id=0.00 region of solution space\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Change 1: Replace 53 lines with 59 lines\n- Metrics: combined_score: 0.6740, metrics: {'avg_score': 0.7158458751393535, 'time_penalty': 0.041815253496170046, 'avg_latency': 9.181525349617004, 'framework_id': 0.0, 'complexity': 39.6, 'diversity': 0.0, 'performance': 0.6740306216431835}, framework_id: 0.0000, complexity: 39.6000, artifacts: {'critique': \"Framework: Llama\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentence 2 about returning the current time in a city is supported by node 1 in the retrieval context, which includes the 'get_current_time' function and the agent's description related to telling time, but the rest of the output is not directly supported by the context, leading to a moderate confidence level.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.50): The score is 0.50 because the first two sentences in the expected output align with the node in retrieval context, describing the function's purpose and arguments, while the remaining sentences are implementation details not supported by the context.\\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.00): The score is 0.00 because the actual output introduces imports from 'google.adk.agents', defines a function 'my_before_model_logic' with unspecified parameters and return type, includes print statements with 'callback_context.agent_name', and creates an 'LlmAgent' instance with specific parameters\u2014all of which are not mentioned or supported by the retrieval context.\\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.50): The score is 0.50 because the output contains some relevant information about setting up an agent, but it includes multiple irrelevant details about imports and agent parameters that do not directly address registering a before_model callback, thus limiting the overall relevance.\\nMetric faithfulness failed: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=32768, prompt_tokens=1238, total_tokens=34006, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\"}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.6740)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        # Increase chunk size and overlap for semantic splitting\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 128\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        try:\n            splitter = SemanticSplitterNodeParser()\n            nodes = splitter.get_nodes_from_documents(documents)\n        except Exception:\n            # Fallback to SentenceSplitter with larger chunk size and overlap\n            from llama_index.core.node_parser import SentenceSplitter\n            splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=5)\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Performs well on combined_score (0.6740), Performs well on metrics ({'avg_score': 0.7158458751393535, 'time_penalty': 0.041815253496170046, 'avg_latency': 9.181525349617004, 'framework_id': 0.0, 'complexity': 39.6, 'diversity': 0.0, 'performance': 0.6740306216431835}), Performs well on framework_id (0.0000), Performs well on complexity (39.6000), Performs well on artifacts ({'critique': \"Framework: Llama\\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.50): The score is 0.50 because the expected output's sentence 2 about returning the current time in a city is supported by node 1 in the retrieval context, which includes the 'get_current_time' function and the agent's description related to telling time, but the rest of the output is not directly supported by the context, leading to a moderate confidence level.\\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.50): The score is 0.50 because the first two sentences in the expected output align with the node in retrieval context, describing the function's purpose and arguments, while the remaining sentences are implementation details not supported by the context.\\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low faithfulness (0.00): The score is 0.00 because the actual output introduces imports from 'google.adk.agents', defines a function 'my_before_model_logic' with unspecified parameters and return type, includes print statements with 'callback_context.agent_name', and creates an 'LlmAgent' instance with specific parameters\u2014all of which are not mentioned or supported by the retrieval context.\\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.50): The score is 0.50 because the output contains some relevant information about setting up an agent, but it includes multiple irrelevant details about imports and agent parameters that do not directly address registering a before_model callback, thus limiting the overall relevance.\\nMetric faithfulness failed: Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=32768, prompt_tokens=1238, total_tokens=34006, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\"})\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.6214)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 256\n        Settings.chunk_overlap = 64\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=256, chunk_overlap=64)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=4)\n        reranker = LLMRerank(top_n=4)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D2 (Score: 0.3000)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 128\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        try:\n            splitter = SemanticSplitterNodeParser()\n            nodes = splitter.get_nodes_from_documents(documents)\n        except Exception:\n            # Fallback to SentenceSplitter\n            splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=5)\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D3 (Score: 0.3500)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 128\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        try:\n            parser = SemanticSplitterNodeParser()\n            nodes = parser.get_nodes_from_documents(documents)\n        except Exception:\n            # Fallback to SentenceSplitter\n            splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n            nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=5)\n        reranker = LLMRerank(top_n=4)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D4 (Score: 0.1500)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\nfrom llama_index.core.tools import QueryEngineTool\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 256\n        Settings.chunk_overlap = 32\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=256, chunk_overlap=32)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        index = VectorStoreIndex(nodes)\n        tool1 = QueryEngineTool.from_defaults(\n            query_engine=index.as_query_engine(similarity_top_k=3),\n            description=\"General documentation queries.\",\n        )\n        tool2 = QueryEngineTool.from_defaults(\n            query_engine=index.as_query_engine(similarity_top_k=6),\n            description=\"Broader context queries.\",\n        )\n\n        self.query_engine = SubQuestionQueryEngine.from_defaults(\n            query_engine_tools=[tool1, tool2]\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D5 (Score: 0.6634)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 256\n        Settings.chunk_overlap = 64\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=256, chunk_overlap=64)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=4)\n        reranker = LLMRerank(top_n=4)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D6 (Score: 0.5760)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 400\n        Settings.chunk_overlap = 80\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=400, chunk_overlap=80)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=6)\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1500, Type: Exploratory)\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\nfrom llama_index.core.tools import QueryEngineTool\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 256\n        Settings.chunk_overlap = 32\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=256, chunk_overlap=32)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        index = VectorStoreIndex(nodes)\n        tool1 = QueryEngineTool.from_defaults(\n            query_engine=index.as_query_engine(similarity_top_k=3),\n            description=\"General documentation queries.\",\n        )\n        tool2 = QueryEngineTool.from_defaults(\n            query_engine=index.as_query_engine(similarity_top_k=6),\n            description=\"Broader context queries.\",\n        )\n\n        self.query_engine = SubQuestionQueryEngine.from_defaults(\n            query_engine_tools=[tool1, tool2]\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Modification: Change 1: Replace 57 lines with 58 lines\n\n# Current Program\n```python\nimport os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 256\n        Settings.chunk_overlap = 64\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=256, chunk_overlap=64)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=4)\n        reranker = LLMRerank(top_n=4)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity, framework_id\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.index = None\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 256\n        Settings.chunk_overlap = 64\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=256, chunk_overlap=64)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        self.index = VectorStoreIndex(nodes)\n        retriever = self.index.as_retriever(similarity_top_k=4)\n        reranker = LLMRerank(top_n=4)\n        self.query_engine = RetrieverQueryEngine(\n            retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n=======\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\nfrom llama_index.core.tools import QueryEngineTool\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 256\n        Settings.chunk_overlap = 32\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=256, chunk_overlap=32)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        index = VectorStoreIndex(nodes)\n        tool1 = QueryEngineTool.from_defaults(\n            query_engine=index.as_query_engine(similarity_top_k=3),\n            description=\"General documentation queries.\",\n        )\n        tool2 = QueryEngineTool.from_defaults(\n            query_engine=index.as_query_engine(similarity_top_k=6),\n            description=\"Broader context queries.\",\n        )\n\n        self.query_engine = SubQuestionQueryEngine.from_defaults(\n            query_engine_tools=[tool1, tool2]\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}