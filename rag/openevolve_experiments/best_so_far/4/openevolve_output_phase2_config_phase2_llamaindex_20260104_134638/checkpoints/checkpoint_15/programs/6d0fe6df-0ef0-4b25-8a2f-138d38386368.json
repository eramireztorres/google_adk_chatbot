{"id": "6d0fe6df-0ef0-4b25-8a2f-138d38386368", "code": "import os\nimport time\nfrom typing import Dict, Any\n\n# --- BOILERPLATE: DO NOT EVOLVE ---\n_rag_system_cache = None\n\ndef evaluate_rag(docs_path: str, query: str) -> Dict[str, Any]:\n    global _rag_system_cache\n    try:\n        if _rag_system_cache is None or _rag_system_cache.docs_dir != docs_path:\n            _rag_system_cache = RAGSystem(docs_path)\n        return _rag_system_cache.query(query)\n    except Exception as e:\n        return {\"answer\": f\"Error: {str(e)}\", \"contexts\": []}\n# --- END BOILERPLATE ---\n\n# EVOLVE-BLOCK-START\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.retrievers import QueryFusionRetriever, VectorIndexRetriever\nfrom llama_index.core.indices.keyword_table import KeywordTableIndex\n\nclass RAGSystem:\n    def __init__(self, docs_dir: str):\n        self.docs_dir = docs_dir\n        self.query_engine = None\n        self._initialized = False\n        self._initialize_system()\n\n    def _initialize_system(self):\n        if self._initialized:\n            return\n        Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n        Settings.chunk_size = 512\n        Settings.chunk_overlap = 192\n\n        try:\n            probe_vec = Settings.embed_model.get_text_embedding(\"probe\")\n            if not probe_vec or len(probe_vec) == 0:\n                raise ValueError(\"embedding probe returned empty vector\")\n        except Exception as e:\n            raise RuntimeError(f\"embedding_probe_failed: {e}\")\n\n        reader = SimpleDirectoryReader(\n            input_dir=self.docs_dir,\n            recursive=True,\n            required_exts=[\".md\", \".txt\"],\n        )\n        documents = reader.load_data()\n        splitter = SentenceSplitter(chunk_size=512, chunk_overlap=192)\n        nodes = splitter.get_nodes_from_documents(documents)\n\n        index = VectorStoreIndex(nodes)\n        keyword_index = KeywordTableIndex.from_documents(documents)\n        keyword_retriever = keyword_index.as_retriever(similarity_top_k=3)\n        vector_retriever = VectorIndexRetriever(index, similarity_top_k=3)\n        hybrid_retriever = QueryFusionRetriever(\n            retrievers=[vector_retriever, keyword_retriever],\n            similarity_top_k=4,\n            num_queries=2,\n            use_async=False,\n        )\n        reranker = LLMRerank(top_n=3)\n        self.query_engine = RetrieverQueryEngine(\n            hybrid_retriever,\n            node_postprocessors=[reranker],\n        )\n        self._initialized = True\n\n    def query(self, query_str: str) -> Dict[str, Any]:\n        response = self.query_engine.query(query_str)\n        contexts = [node.node.get_content() for node in response.source_nodes]\n        return {\"answer\": str(response), \"contexts\": contexts}\n# EVOLVE-BLOCK-END\n", "language": "python", "parent_id": "1644c5c7-7556-4a81-ac9b-63fd1251b232", "generation": 3, "timestamp": 1767534401.1341422, "iteration_found": 9, "metrics": {"error": 0.0, "timeout": true}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 55 lines with 58 lines", "parent_metrics": {"combined_score": 0.14999999999999997, "metrics": {"avg_score": 0.35, "time_penalty": 0.2, "avg_latency": 59.27836975455284, "framework_id": 0.0, "complexity": 38.6, "diversity": 0.0, "performance": 0.14999999999999997}, "framework_id": 0.0, "complexity": 38.6, "artifacts": {"critique": "Framework: Llama\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low answer_relevancy (0.00): The score is 0.00 because the output contained irrelevant statements about Python package installation issues, which are unrelated to the input requesting specific Python code definitions for tools and agents.\nQuery: 'Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.' -> Low contextual_recall (0.00): The score is 0.00 because the retrieval context is empty, providing no supporting information to attribute any sentence in the expected output to the context, leading to a lack of relevant connections.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low answer_relevancy (0.00): The score is 0.00 because the actual output did not include the requested Python example of a Function Tool with 'city' and 'unit' parameters, instead discussing unrelated package installation instructions.\nQuery: 'Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.' -> Low contextual_recall (0.00): The score is 0.00 because the retrieval context is empty, making it impossible to attribute any sentences from the expected output to the nodes in the context.\nQuery: 'Show the Python example that registers a before_model callback in an LlmAgent.' -> Low answer_relevancy (0.00): The score is 0.00 because the actual output contains irrelevant information about package installation and error handling, which do not address the specific request to show a Python example of registering a before_model callback in an LlmAgent.", "ingestion_guard": "query='Provide the Python code from the ADK quickstart that defines the get_current_time tool and the root_agent.': rag_system_cache is None\nquery='Give the Python example of a Function Tool with required parameters city and unit from the ADK function tools documentation.': rag_system_cache is None\nquery='Show the Python example that registers a before_model callback in an LlmAgent.': rag_system_cache is None\nquery='From the MCP tools documentation, provide the Python agent.py sample that connects to a local filesystem MCP server started via npx and exposes its tools through MCPToolset.': rag_system_cache is None\nquery='Show the Python snippet where an LlmAgent uses output_key to store last_greeting and a Runner executes it with InMemorySessionService.': rag_system_cache is None"}}, "island": 0}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}